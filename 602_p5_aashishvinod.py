# -*- coding: utf-8 -*-
"""602.p5.aashishvinod.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iEAdou8m_jgjsp3xnomkC2ld2x2T3LTC

**Define a problem or research question you will aim to address throughout the
course**


---



*   **Research Question:**
"How will climate change impact agricultural food production in American continent?"


*   **Why It’s Interesting/Important:**
I love food and enjoy exploring new dishes as much as I can. However, not everyone is as blessed & fortunate. This year, 42 countries rank as either Serious or Alarming on the Global Hunger Index. While hunger can result from various causes such as war, political conflicts, or poor resource management, climatic conditions are one of the most critical factors affecting agricultural production. Changes in temperature, precipitation patterns, and extreme weather events directly impact agricultural food production. Understanding and predicting these changes is essential for governments, farmers, and communities to adapt accordingly, improve resource allocation, and reduce food wastage, thus ensuring food security for the future.
*   **Specific Focus & Measurable Goal:**
Using historical climate data and agricultural production data, I will analyze correlations between climate variables (such as average temperature and precipitation) and crop yields. The goal is to develop a predictive model that estimates how future climate scenarios could affect food production levels.
This will allow me to answer:
"Given projected climate trends, how will the production of key crops like corn and wheat change in North America by 2040?" or "Determine the crop that would yield optimum production according the predicted climatic condition."

**Determine the population you wish to study**

---



*   **Population to Study:**
The population I wish to study is the agricultural sector in the American continent, which covers diverse climatic regions including the tropics, temperate zones, and arid areas. Within this broad sector, I will specifically focus on staple crops such as corn, wheat, and soybeans, as well as livestock production. These commodities play a key role in both domestic consumption and global exports, and they are highly sensitive to climate variables such as temperature, rainfall patterns, and extreme weather events.

**Identify variable(s) in the population sample that you will study**


---



*   **What are the independent variable(s)?** : station_id, city_name, date, avg_temp_c, min_temp_c, max_temp_c, precipitation_mm, snow_depth_mm, avg_wind_dir_deg, avg_wind_speed_kmh, peak_wind_gust_kmh, continent avg_sea_level_pres_hpa, sunshine_total_min, native_name, iso2, iso3, population, area, capital, capital_lat, capital_lng, region, station_id, city_name, state, iso2, iso3, latitude, longitude


*   **What is the dependent variable?** Production per year (Y1961	...	Y2020N,	Y2021,	Y2021F,	Y2021N,	Y2022,	Y2022F,	Y2022N,	Y2023,	Y2023F,	Y2023N)






*   **Explain what a confounding variable is** : A confounding variable is an external factor that influences both the independent and dependent variables, making it harder to determine the true relationship between them. That is, it can create a false impression of cause and effect if not properly accounted.

*   **Identify any potential confounding variables in your study** In this project, I identified season and country as potential confounders.


1.   **Season** influences both weather patterns (temperature, rainfall, sunshine) and agricultural yields, since different crops are planted and harvested in specific seasons.
2.   **Country** is also a confounder because farming practices, technology, irrigation systems, and policies differ between countries, while climate also varies geographically.



*  **How will you deal with them?** To address seasonality, I will focus on averaging weather data over the main growing months for each crop instead of relying on full calendar year averages. This makes it easier to capture the climate conditions that actually matter for crop development without needing highly detailed planting and harvest calendars. To account for country level differences, I will group countries into broader regional categories such as North America, Central America, and South America. This way, I can control for general differences in farming practices, technology, and policies across regions without requiring complex country-specific data.

**Formulate a hypothesis**

---



*   Due to global warming, there has been an increase in average temperatures across the globe. This affects weather patterns and has resulted in shifts in rainfall cycles. Therefore, I assume that regions with the greatest increase in temperature will be more favourable for the cultivation of crops that require warmer conditions. This analysis will help in predicting regions suitable for cultivating certain crops based on temperature trends, with the goal of obtaining maximum yield.

**Develop a detailed plan for data collection**

---



*   **How will you get the data?** <br>
For this study, I will collect data from reliable open source databases:

1.   **Climate Data:** Daily and historical climate records (temperature, precipitation, wind speed, sunshine hours) will be collected from "The Weather Dataset" (Kaggle). This dataset offers a comprehensive collection of Daily weather readings from major cities around the world. Some locations provide historical data tracing back to January 2, 1833, giving us a deep dive into long term weather patterns and their evolution.


2. **Agricultural Production Data:** Crop and livestock production data will be obtained from the Food and Agriculture Organization (FAO) database. Crop and livestock statistics are recorded for 278 products, covering the following categories:

- 2.1. CROPS PRIMARY: Cereals, Citrus Fruit, Fibre Crops, Fruit, Oil Crops, Oil Crops and Cakes in Oil Equivalent, Pulses, Roots and Tubers, Sugar Crops, Treenuts and Vegetables. Data are expressed in terms of area harvested, production quantity and yield. Cereals: Area and production data on cereals relate to crops harvested for dry grain only. Cereal crops harvested for hay or harvested green for food, feed or silage or used for grazing are therefore excluded.
- 2.2.  CROPS PROCESSED: Beer of barley; Cotton lint; Cottonseed; Margarine, short; Molasses; Oil, coconut (copra); Oil, cottonseed; Oil, groundnut; Oil, linseed; Oil, maize; Oil, olive, virgin; Oil, palm; Oil, palm kernel; Oil, rapeseed; Oil, safflower; Oil, sesame; Oil, soybean; Oil, sunflower; Palm kernels; Sugar Raw Centrifugal; Wine.
- 2.3.  LIVE ANIMALS: Animals live n.e.s.; Asses; Beehives; Buffaloes; Camelids, other; Camels; Cattle; Chickens; Ducks; Geese and guinea fowls; Goats; Horses; Mules; Pigeons, other birds; Pigs; Rabbits and hares; Rodents, other; Sheep; Turkeys.
- 2.4.  LIVESTOCK PRIMARY: Beeswax; Eggs (various types); Hides buffalo, fresh; Hides, cattle, fresh; Honey, natural; Meat (ass, bird nes, buffalo, camel, cattle, chicken, duck, game, goat, goose and guinea fowl, horse, mule, Meat nes, meat other camelids, Meat other rodents, pig, rabbit, sheep, turkey); Milk (buffalo, camel, cow, goat, sheep); Offals, nes; Silk-worm cocoons, reelable; Skins (goat, sheep); Snails, not sea; Wool, greasy.
- 2.5. LIVESTOCK PROCESSED: Butter (of milk from sheep, goat, buffalo, cow); Cheese (of milk from goat, buffalo, sheep, cow milk); Cheese of skimmed cow milk; Cream fresh; Ghee (cow and buffalo milk); Lard; Milk (dry buttermilk, skimmed condensed, skimmed cow, skimmed dried, skimmed evaporated, whole condensed, whole dried, whole evaporated); Silk raw; Tallow; Whey (condensed and dry); Yoghurt



*   **How will you ensure representativeness when using a sample?**<br>
To ensure representativeness, I will:

1.   Use a dataset that covers multiple countries across the American continent, capturing diverse climatic regions such as tropics, temperate zones, and arid areas.
2.   Include data spanning multiple decades to capture long term trends and avoid bias from short term anomalies.
3.   Select staple crops and livestock production because they represent a significant share of agricultural output and are sensitive to climate changes, ensuring meaningful analysis.
4. Use consistent measurement units and data quality checks to avoid inconsistencies.

*   **What method will you use to collect the data?**
1. Download structured datasets directly from trusted public repositories in formats such as CSV or parquet.
2. Use APIs, where available, to automate and update climate data collection efficiently.
3. Preprocess the data by cleaning missing or inconsistent values to ensure accuracy, harmonizing timeframes so that daily weather data aligns correctly with annual production data, and aggregating the information to appropriate spatial and temporal scales, such as yearly averages for each region.
4. Apply data merging and transformation techniques to combine climate and production datasets for thorough and meaningful analysis.

**Choose a data set**

---

For this project, I have selected a dataset that meets all the required criteria. The data is not pre cleaned, which provides a valuable opportunity to perform meaningful cleaning and preprocessing in the next phase. It is in a tabular CSV format, making it easy to work with and analyze. The dataset includes more than five variables, offering enough attributes for a thorough analysis, and contains over 1,000 samples to ensure reliable insights while keeping the file size well under 1GB. Most importantly, the dataset is publicly available and free to use, ensuring that it complies with permissions and ethical data use guidelines.<br>


*  **Explain why this dataset is interesting to you**<br>
These datasets really excite me because they give a chance to understand how climate change is affecting agricultural production something which is the need of the hour. By combining daily weather records with detailed agricultural production data, I can look at how changes in climate impact crop and livestock yields over time and across different regions. This isn’t just an academic project it could help make smarter decisions for the future of food and contribute to a more sustainable world.
Climate change is one of the biggest challenges we face today, affecting food security, livelihoods, and economies everywhere. Agriculture is mainly affected by this issue, it’s both vulnerable to changing weather and essential to life itself. These datasets let me measure and understand those changes in a meaningful way. They offer a rare mix of long term climate records and detailed agricultural production statistics, creating a unique opportunity to connect environmental shifts to real world impacts on food.
*   **What is in the dataset?**
1. **Weather Dataset:** Contains daily weather records for over 1,200 cities worldwide, including variables such as average, minimum, and maximum temperature, precipitation, snow depth, wind direction and speed, sea level pressure, and sunshine duration. It also includes city and country level geographic and demographic data.
2. **Production Dataset:** Contains agricultural statistics for 278 products, including cereals, fruits, fibre crops, pulses, oil crops, processed products, live animals, and primary and processed livestock products. Data include area harvested, production quantity, and yield for these products.
 * **Where is the dataset from?**
 1. **Weather Dataset:** Sourced from the Meteostat project, which collects weather data from official meteorological agencies and updates it weekly. The data is publicly available for research and analysis. (https://www.kaggle.com/datasets/guillemservera/global-daily-climate-data?resource=download)
 2. **Production Dataset:** Sourced from the Food and Agriculture Organization (FAO) of the United Nations, a reliable and globally recognized provider of agricultural statistics. The dataset is publicly available at:(https://www.fao.org/faostat/en/#data/QCL)

 *  **When is the dataset from?**
 1.  **Weather Dataset:** Includes historical records for many locations dating back to January 2, 1833. For this project, I will focus on data from 1961 to the present to align with agricultural production data.
 2.  **Production Dataset:** Covers agricultural production statistics from 1961 to the present, allowing long term trend analysis of crop and livestock output.
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

countries = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/DATA 602 Project/WEATHER/countries.csv")

cities = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/DATA 602 Project/WEATHER/cities.csv")

daily_weather = pd.read_parquet("/content/drive/MyDrive/Colab Notebooks/DATA 602 Project/WEATHER/daily_weather.parquet")

production= pd.read_csv("/content/drive/MyDrive/Colab Notebooks/DATA 602 Project/PRODUCTION/Production_Crops_Livestock_E_Americas_NOFLAG.csv")

daily_weather.head()

countries.head()

cities.head()

production.head()

"""# **PROJECT 2**

---

**Get some preliminary information about the dataset
 What is the shape of the data?**




*  What data types are in the data?
*  List all the columns/variables in the data
*  How many unique elements are in the data?
* Any descriptive statistics on the overall data?

---

# Dataset Overview

## 1. Countries Dataset
- **Shape:** 214 rows, 11 columns  
- **Columns:** country, native_name, iso2, iso3, population, area, capital, capital_lat, capital_lng, region, continent  
- **Data Types:** object for text fields, float64 for numeric fields  
- **Unique Values:** 214 countries, 213 native names, 212 ISO2 codes, 206 unique area values  
- **Descriptive Statistics:**  
  - **Population:** min = 30, max ≈ 1.37B, mean ≈ 33.65M  
  - **Area:** min = 2, max ≈ 17.12M sq.km, mean ≈ 653,760  
  - **Capital Latitude:** -54.28 to 78.22, **Longitude:** -176.17 to 179.12  

## 2. Cities Dataset
- **Shape:** 1,245 rows, 8 columns  
- **Columns:** station_id, city_name, country, state, iso2, iso3, latitude, longitude  
- **Data Types:** object for most columns, float64 for latitude and longitude  
- **Unique Values:** 1,227 station IDs, 1,234 city names, 216 countries  
- **Descriptive Statistics:**  
  - **Latitude:** -54.79 to 78.22, mean = 23.73  
  - **Longitude:** -176.17 to 179.22, mean = 20.78  

## 3. Daily Weather Dataset
- **Shape:** 27,635,763 rows, 14 columns  
- **Columns:** station_id, city_name, date, season, avg_temp_c, min_temp_c, max_temp_c, precipitation_mm, snow_depth_mm, avg_wind_dir_deg, avg_wind_speed_kmh, peak_wind_gust_kmh, avg_sea_level_pres_hpa, sunshine_total_min  
- **Data Types:** category for categorical fields, datetime64 for date, float64 for numeric weather metrics  
- **Unique Values:** 1,227 stations, 1,234 cities, 69,648 dates  
- **Descriptive Statistics:**  
  - **Avg Temp:** -70°C to 50.4°C  
  - **Min Temp:** -99°C to 64.2°C  
  - **Max Temp:** -99°C to 97°C  
  - **Precipitation:** 0 to 1,000 mm  
  - **Notes:** Many missing values for snow depth, wind gust, and sunshine duration  

## 4. Production Dataset
- **Shape:** 11,123 rows, 72 columns  
- **Columns:** Area Code, Area Code (M49), Area, Item Code, Item Code (CPC), Item, Element Code, Element, Unit, Y1961 to Y2023  
- **Data Types:** int64 for codes, object for names and units, float64 for yearly production values  
- **Unique Values:** 39 areas, 276 items, 8 elements  
- **Descriptive Statistics:**  
  - **Production Values:** 0 to over 782M  
  - **Mean Production per Year:** ~0.5–1M  
  - **Area Codes:** min = 8, max = 236, mean = 103.72
"""

# datasets
datasets= {
    "countries": countries,
    "cities": cities,
    "daily_weather": daily_weather,
    "production": production
}



for name, df in datasets.items() :
    print(f"\n ---Dataset: {name}---")

    #Shape of df
    print (f"Shape: {df.shape}")

    # Data types
    print("\nData Types:")
    print (df.dtypes)


    # Column
    print ("\nColumns:")
    print(df.columns.tolist())

    # No. of unique elements per column
    print("\nNumber of unique values per column:")
    print(df.nunique())


    #Descriptive statistics
    print ("\n Statistics:")
    print(df.describe(include='all'))

"""**3. Look for specific data needs for your project**
* ex. Do you have multiple datasets that need to be merged?
* What are they?
* How are you addressing them?
* If there are no specific needs for your project, state/explain that.



---
# Data Handling and Merging Process

For this project, I have worked with multiple datasets that needed to be combined to analyze agricultural production and weather data in the Americas. Specifically, I have used:

**Countries dataset** – This dataset provides information about each country, including continent, region, and geographic attributes. I used it to identify and filter countries in the Americas.

**Cities dataset** – It contains information about cities and weather stations, such as station_id and the corresponding country. I used this to link daily weather measurements to the correct country.

**Daily Weather dataset** – This dataset has daily records of temperature, precipitation, wind, and other weather variables for cities around the world.

**Production dataset** – This dataset includes annual agricultural production data per country and item, from 1961 onward.

---

## 1. Filtering the datasets for the Americas
I started by selecting countries in North, Central, and South America using the countries dataset. This allowed me to create a list of countries relevant to my study. I then filtered the cities dataset to retain only cities located in these countries, ensuring that all subsequent analyses would be geographically relevant. Similarly, I filtered the production dataset to keep only records corresponding to these countries, removing any unnecessary data that could introduce inconsistencies.

---

## 2. Preparing and aggregating weather data
The `daily_weather` dataset contains daily observations for each station, including metrics such as average, minimum, and maximum temperatures, precipitation, wind speed, wind gust, sunshine duration, and sea-level pressure. Because my analysis focuses on yearly agricultural production, I needed to aggregate daily weather data to yearly values.

**Step 2a: Convert dates and extract year**  
I converted the `date` column to datetime objects to ensure proper handling and extracted the year from each record. I filtered the dataset to include only years from 1961 onward to align with the production data.

**Step 2b: Ensure numeric consistency**  
Converted all weather-related columns to numeric types to handle non-numeric entries or inconsistent formatting.

**Step 2c: Aggregate by station and year**  
Grouped the data by `station_id` and `year` and calculated the mean for each weather metric. This step provided yearly averages for each station, smoothing out daily fluctuations and offering meaningful metrics for agricultural analysis.

**Step 2d: Merge with city information**  
Merged the aggregated station-level weather data with the cities dataset to assign each station to its respective country. This step ensured that each weather observation could be linked accurately to a country.

**Step 2e: Aggregate by country-year**  
I then aggregated station-level data to country-level yearly averages. This created a single row per country per year with all weather metrics, directly compatible with the production dataset.

---

## 3. Reshaping the production dataset
The `production` dataset was in a wide format, with each year as a separate column (e.g., Y1961, Y1962…). I reshaped it into a long format so that each row represents a unique combination of country, year, and agricultural item, with a single column for the production value. This transformation allowed for a seamless merge with the country-year weather dataset.

---

## 4. Handling missing data, outliers, and data consistency
I carefully addressed missing values, outliers, and inconsistencies before merging:

- For weather variables, I filled missing values using the mean for that country-year, and as a fallback, I used the overall country mean or overall mean of all countries.  
- For production values, I used forward and backward filling within each country-item combination to maintain continuity across years.  
- Identified outliers in both weather and production numeric columns using Z-scores (>3) and replaced them with NaN, followed by imputation using the same strategies above.  
- I standardized text columns, such as country names, item names, and element names, by stripping extra spaces and capitalizing consistently.

This ensured that both numeric and categorical data were clean, consistent, and reliable.

---

## 5. Merging datasets
Once all datasets were cleaned and aggregated, I merged the reshaped production data with the country-year weather data using `country_name` and `year` as keys. The resulting dataset contains:

- Country, year, and agricultural item  
- Production value  
- Yearly average weather metrics (temperature, precipitation, wind, sunshine, etc.)

This final dataset allows for analysis of how weather impacts agricultural production across multiple countries in the Americas over time.

---

## 6. Benefits of this approach
- Filtering to relevant countries removed unnecessary noise from unrelated regions.  
- Aggregating daily weather data to yearly averages made it compatible with production data and reduced volatility.  
- Reshaping the production dataset allowed a direct, clean merge with weather metrics.  
- Handling missing data, outliers, and text inconsistencies improved the overall quality, reliability, and usability of the dataset.

**4. Look for potential issues in the data**
* Are there duplicates? How many? Anything useful around unique values?
* Any missing or null values? What type of missing data? Where are they? Are
they expected?
* Any type inconsistency?
* Any problematic misspellings/mistypings?
* Any weird outliers?
  1. Show how you are detecting them! (ex. using Z-Score)
* What else?
* If there are no issues found:


  1.   Explain why in detail and show how you determined that.
  2.   Explain why in detail and show how you determined that.

---





## 1. Duplicates
I checked each dataset for duplicate rows using `df.duplicated().sum()`:

- **Countries**: 0 duplicates.  
- **Cities**: 0 duplicates.  
- **Daily Weather**: 21,280 duplicate rows were found. These duplicates were mostly historical daily records and were filtered out during the Americas-specific selection.  
- **Production**: 0 duplicates.  

**Conclusion:** Except for `daily_weather`, duplicates were not an issue. Duplicates in `daily_weather` were expected due to repeated observations, and I handled them by filtering to relevant stations and years.

---

## 2. Missing or Null Values
I explored missing values using `df.isnull().sum()`:

- **Countries**: Several missing values in columns like `native_name`, `iso2`, `population`, `area`, `capital`, `capital_lat`, `capital_lng`, `region`, and `continent`. These are minor and expected for small or less-documented countries.  
- **Cities**: Missing values in `city_name`, `state`, and `iso2`. Mostly expected, e.g., missing state information.  
- **Daily Weather**: Many missing values in weather columns like `avg_temp_c`, `min_temp_c`, `max_temp_c`, `precipitation_mm`, `snow_depth_mm`, `avg_wind_speed_kmh`, `peak_wind_gust_kmh`, `avg_sea_level_pres_hpa`, `sunshine_total_min`. This is expected for historical and partial weather station data.  
- **Production**: Missing values in year columns (Y1961–Y2023), which is normal for certain country-item combinations over the historical period.  

**Handling missing data:**  
- Weather variables: Filled missing values using the mean for that country-year, with fallback to country wide or overall mean.  
- Production values: Forward and backward filling within each country-item combination ensured continuity across years.

---

## 3. Type Inconsistencies
I inspected data types using `df.dtypes`:

- Some numeric columns were stored as `object` types due to formatting issues in raw data (e.g., production or weather metrics).  
- Converted all numeric columns (weather and production) to numeric types with `pd.to_numeric(errors='coerce')`.  

**Conclusion:** After conversion, all numeric columns were consistent, preventing errors during aggregation or analysis.

---

## 4. Problematic Misspellings or Typos
I checked categorical columns with fewer than 20 unique values:

- **Countries**: `continent` had a few unique values like `'Asia', 'Europe', 'North America'`. No unexpected typos were found.  
- **Cities and Production**: Columns like `Element` and `Unit` were consistent after stripping spaces and standardizing capitalization.  

**Conclusion:** Categorical data was cleaned and consistent.

---

## 5. Outliers
I used **Z-scores** to detect outliers:

- Checked all numeric columns across datasets.  
- Extreme values (|Z| > 3) were replaced with NaN and imputed using the same country-year mean for weather columns, and forward/backward filling for production values.  
- Examples include very large populations in countries like China or India and extreme weather values, which were handled correctly.

---

## 6. Additional Checks
- Checked for leading/trailing spaces in string columns; fixed with `.str.strip()`.  
- Ensured `date` column in `daily_weather` was converted to datetime.  
- Verified alignment of years (≥1961) between weather and production datasets.  

---

## 7. Summary and Why No Issues Remain
After performing these checks:

1. **Duplicates:** Only `daily_weather` had duplicates, which were filtered during Americas specific selection.  
2. **Missing Values:** Missing data was expected for certain variables; handled via imputation.  
3. **Type Consistency:** All numeric columns converted correctly; categorical columns standardized.  
4. **Outliers:** Extreme numeric values identified and imputed.  
5. **Formatting/Typo Issues:** Text columns standardized; no inconsistencies remain.  

**How I determined this:** I used pandas functions like `duplicated()`, `isnull().sum()`, `dtypes`, `.nunique()`, and Z-score analysis to systematically check each dataset.
"""

import pandas as pd
import numpy as np
from scipy import stats

# List of datasets
datasets = {
    "countries": countries,
    "cities": cities,
    "daily_weather": daily_weather,
    "production": production

}

for name, df in datasets.items():


    print(f"\n_____ Dataset: {name} _____")

    # 1. Checking for duplicates
    dup_count = df.duplicated().sum()

    print(f"Duplicate rows: {dup_count}")

    # If duplicates exist, showing first few
    if dup_count > 0:
        print("First few duplicate rows:")

        print(df[df.duplicated()].head())

    # 2. Missing/null values
    missing_count = df.isnull().sum()
    missing_count = missing_count[missing_count > 0]
    if len(missing_count) > 0:

        print("\nMissing values per column:")
        print(missing_count)
    else:
        print("\nNo missing values detected.")

    # 3. Data type inconsistencies
    print("\nData types:")
    print(df.dtypes)

    # Convert numeric columns with object type
    for col in df.select_dtypes(include='object'):
        try:
            df[col] = pd.to_numeric(df[col])
        except:
            continue

    # 4. Check categorical columns for small unique values (possible typos)
    print("\nCategorical columns with small number of unique values:")
    for col in df.select_dtypes(include='object').columns:

        if df[col].nunique() < 20:
            print(f"{col}: {df[col].unique()}")

    # 5. Detect numeric outliers using Z-score
    numeric_cols = df.select_dtypes(include=np.number).columns
    if len(numeric_cols) > 0:

        z_scores = np.abs(stats.zscore(df[numeric_cols].fillna(0)))
        outlier_rows = np.where(z_scores > 3)  # Z-score threshold = 3
        if len(outlier_rows[0]) > 0:
            print(f"\nPotential outliers detected in numeric columns (Z>3). Examples:")
            outlier_indices = np.unique(outlier_rows[0])[:5]
            print(df.iloc[outlier_indices])

        else:
            print("\nNo numeric outliers detected (Z>3).")

    # 6. Check for inconsistent text formatting
    print("\nCheck for leading/trailing spaces in string columns:")

    # Flag to track if any spaces are found

    spaces_found = False

    for col in df.select_dtypes(include='object').columns:
        if df[col].str.startswith(" ").any() or df[col].str.endswith(" ").any():
            print(f"{col} has leading/trailing spaces")
            spaces_found = True

    # If no spaces were found in any column, print message once
    if not spaces_found:


        print("No leading/trailing spaces found in any string column.")

"""**5. Any reorganization needed?**
* ex. Any columns that would be better understood if renamed (i.e. column “a”
renamed to “age”, “height” renamed to “height in meters”)

---


## 1. Renaming columns for clarity

- **Production dataset:**  
  - Renamed `Area` to `country_name`  
  - Renamed `Area Code` to `country_code`  
  This makes it clear that the column refers to the country, avoiding confusion with other codes or areas.

- **Year columns in production:**  
  - Original columns were named `Y1961`, `Y1962`, etc.  
  - Converted them into a single column `year` during reshaping (long format) for easier aggregation and merging with weather data.

- **Daily weather dataset:**  
  - Columns like `avg_temp_c`, `min_temp_c`, `max_temp_c`, `precipitation_mm`, `snow_depth_mm`, `avg_wind_speed_kmh`, `peak_wind_gust_kmh`, `avg_sea_level_pres_hpa`, `sunshine_total_min` were already descriptive.  
  - No renaming was necessary, but I ensured all column names were **consistent, lowercase, and readable**.

- **Other datasets (`countries` and `cities`):**  
  - Columns were mostly clear, but I standardized them by stripping spaces and using lowercase with underscores where necessary (e.g., `country_name`, `city_name`).

---

## 2. Restructuring data

- **Production data:**  
  - Transformed from **wide format** (one column per year) to **long format** (one row per country-year-item) to align with weather data.  
  - This made merging straightforward and analysis of trends over time easier.

- **Weather data:**  
  - Aggregated daily weather metrics to **yearly averages by country** to match the production dataset’s time scale.  
  - Ensured each weather column represents a single metric (temperature, precipitation, etc.) for clarity.

---
"""

# Renamed columns in production dataset
production.rename(columns={
    "Area": "country_name",
    "Area Code": "country_code"
}, inplace=True)

"""**6. Develop a detailed plan for how you will clean your data**
* Based on the potential issues you found, how will you address them?
* What techniques will you use and why?


---





## 1. Handling duplicates

- **Observations:**  
  - The `daily_weather` dataset contains 21,280 duplicate rows.  
  - Other datasets (`countries`, `cities`, `production`) do not have duplicates.  

- **Plan:**  
  - Remove duplicate rows in `daily_weather` using `drop_duplicates()`.  
  - Verify that each weather measurement for a given `station_id` and `date` remains unique after cleaning.  
  - Check that removing duplicates does not discard meaningful data.  

- **Reason:** Duplicates can bias aggregation, lead to inflated averages, and produce misleading results when merging datasets.

---

## 2. Handling missing values

- **Observations:**  
  - `countries` and `cities` have some missing values in `native_name`, `iso2`, `population`, `area`, and `state`.  
  - `daily_weather` has large amounts of missing data in almost all weather variables.  
  - `production` contains missing values across multiple year columns.  

- **Plan:**  
  - **Weather data:**  
    - Fill missing values using **country-year averages**.  
    - If an entire country-year is missing, fill with **country-wide mean**.  
    - As a final fallback, use the **overall dataset mean** for that variable.  
  - **Production data:**  
    - Use **forward-fill and backward-fill within each country-item combination** to ensure continuity across years.  
    - Inspect extreme gaps and cross-verify with known production trends for plausibility.  
  - **Countries and cities datasets:**  
    - Fill critical columns like `iso2`, `population`, and `area` using external reference data if missing.  

- **Reason:** Handling missing values carefully prevents bias in aggregation and ensures consistent merging of weather and production data.

---

## 3. Correcting data type inconsistencies

- **Observations:**  
  - Some columns that should be numeric, such as production values and weather metrics, are stored as objects.  
  - The `date` column in `daily_weather` requires datetime formatting.  

- **Plan:**  
  - Convert numeric columns to the correct type using `pd.to_numeric(errors='coerce')`.  
  - Convert the `date` column in `daily_weather` to `datetime` using `pd.to_datetime(errors='coerce')`.  
  - Validate that numeric columns contain only valid numeric entries after conversion.  

- **Reason:** Correct data types are essential for aggregation, calculations, statistical analysis, and outlier detection.

---

## 4. Standardizing text and categorical fields

- **Observations:**  
  - String columns such as `country_name`, `Item`, and `Element` have inconsistent capitalization and may have leading or trailing spaces.  

- **Plan:**  
  - Strip leading and trailing spaces using `.str.strip()`.  
  - Standardize capitalization with `.str.title()` for readability.  
  - Verify uniqueness of country and item names after standardization to avoid mismatches during merging.  

- **Reason:** This ensures consistent grouping and avoids errors when merging datasets or performing aggregations.

---

## 5. Detecting and handling outliers

- **Observations:**  
  - Numeric columns, including `population`, `area`, weather metrics, and production values, contain extreme values.  

- **Plan:**  
  - Use the **Z-score method** (`Z > 3`) to detect outliers.  
  - For **weather data**, replace extreme outliers with `NaN` and impute using **country-year mean**.  
  - For **production data**, replace extreme outliers with `NaN` and impute using **forward-fill and backward-fill within each country-item**.  
  - Re-check for outliers after imputation to ensure no extreme values remain.  

- **Reason:** Outliers can distort averages, trends, and correlations. Proper handling ensures more robust and reliable results.

---

## 6. Reshaping datasets

- **Observations:**  
  - `production` is in a wide format, with a separate column for each year (e.g., Y1961, Y1962…).  
  - Weather data is aggregated by station and year.  

- **Plan:**  
  - Convert the `production` dataset to **long format** with columns: `country_name`, `year`, `Item`, `value`.  
  - Aggregate weather data to **station-year**, then **country-year** averages.  
  - Merge production and weather data on `country_name` and `year`.  

- **Reason:** Reshaping ensures compatibility between production and weather datasets, allowing a single row to contain all relevant metrics for analysis.

---

## 7. Reorganizing columns and renaming

- **Observations:**  
  - Columns in `production` such as `Area` and `Area Code` are unclear.  

- **Plan:**  
  - Rename columns for clarity:  
    - `Area` → `country_name`  
    - `Area Code` → `country_code`  
  - Ensure consistent column names across datasets to simplify merging.  

- **Reason:** Clear, descriptive column names improve readability and reduce confusion during analysis.

---

## 8. Final validation and quality checks

- **Plan:**  
  - Verify that there are no remaining duplicates.  
  - Confirm that all numeric columns have valid, consistent data types.  
  - Check for any remaining missing values and impute if necessary.  
  - Validate the merged dataset for consistency (e.g., check ranges for weather metrics and production values).  
  - Generate summary statistics to ensure realistic distributions.  

- **Reason:** Final validation ensures that the cleaned dataset is reliable, accurate, and ready for exploratory analysis or modeling.




"""

#Drop Duplicates

daily_weather.drop_duplicates(inplace=True)

production.drop_duplicates(inplace=True)

import pandas as pd
import numpy as np

# Step 1: Filter countries to Americas
americas = countries[countries['continent'].isin(['North America', 'Central America', 'South America'])]

americas_countries = americas['country'].tolist()

# Step 2: Filter cities and daily_weather

cities_americas = cities[cities['country'].isin(americas_countries)]
daily_weather_americas = daily_weather[daily_weather['station_id'].isin(cities_americas['station_id'])]

# Step 3: convert date and extract year

daily_weather_americas = daily_weather_americas.copy()  # avoid SettingWithCopyWarning
daily_weather_americas['date'] = pd.to_datetime(daily_weather_americas['date'], errors='coerce')
daily_weather_americas = daily_weather_americas[daily_weather_americas['date'].dt.year >= 1961]
daily_weather_americas['year'] = daily_weather_americas['date'].dt.year



# Step 4: Ensure numeric columns are numeric

agg_cols = ['avg_temp_c', 'min_temp_c', 'max_temp_c', 'precipitation_mm',
            'snow_depth_mm', 'avg_wind_speed_kmh', 'peak_wind_gust_kmh',
            'avg_sea_level_pres_hpa', 'sunshine_total_min']

for col in agg_cols:
    daily_weather_americas[col] = pd.to_numeric(daily_weather_americas[col], errors='coerce')

# Step 5: Aggregate weather by station_id and year

weather_agg = daily_weather_americas.groupby(['station_id', 'year'])[agg_cols].mean().reset_index()

# Step 6: Merge with city info to get country

weather_agg = weather_agg.merge(cities_americas[['station_id', 'country']], on='station_id', how='left')

# Step 7: Aggregate by country-year
weather_country_year = weather_agg.groupby(['country', 'year'])[agg_cols].mean().reset_index()

# Step 8: Merge with production data (long format)
year_cols = [col for col in production.columns if col.startswith('Y')]
production_long = production.melt(
    id_vars=['country_name', 'country_code', 'Item', 'Item Code', 'Element', 'Element Code', 'Unit'],
    value_vars=year_cols,
    var_name='year',
    value_name='value'
)
production_long['year'] = production_long['year'].str.replace('Y', '').astype(int)


# Filter production to Americas

production_long = production_long[production_long['country_name'].isin(americas_countries)]

# Final merge

final_df = production_long.merge(weather_country_year, left_on=['country_name','year'], right_on=['country','year'], how='left')
final_df.drop(columns=['country'], inplace=True)

print("Final merged dataframe shape:", final_df.shape)
final_df.head()

import pandas as pd
import numpy as np
from scipy import stats




#  Ensure numeric columns are numeric

numeric_cols = ['avg_temp_c', 'min_temp_c', 'max_temp_c', 'precipitation_mm',
                'snow_depth_mm', 'avg_wind_speed_kmh', 'peak_wind_gust_kmh',
                'avg_sea_level_pres_hpa', 'sunshine_total_min', 'value']

for col in numeric_cols:
    final_df[col] = pd.to_numeric(final_df[col], errors='coerce')


#  Handle missing values


# Fill weather missing values with country-year mean
weather_cols = numeric_cols[:-1]  # exclude 'value' (production)
for col in weather_cols:
    final_df[col] = final_df.groupby(['country_name', 'year'])[col].transform(
        lambda x: x.fillna(x.mean())
    )

# Fill production missing values with previous year's value per country-item
final_df.sort_values(['country_name', 'Item', 'year'], inplace=True)
final_df['value'] = final_df.groupby(['country_name', 'Item'])['value'].transform(
    lambda x: x.fillna(method='ffill').fillna(method='bfill')
)


#  Handle outliers using Z-score

for col in numeric_cols:
    col_zscore = np.abs(stats.zscore(final_df[col].fillna(0)))
    outliers = col_zscore > 3
    # Replace extreme outliers with NaN and handle with imputation
    final_df.loc[outliers, col] = np.nan
    # Impute again with country-year mean
    if col != 'value':  # weather columns
        final_df[col] = final_df.groupby(['country_name', 'year'])[col].transform(
            lambda x: x.fillna(x.mean())
        )
    else:  # production values
        final_df[col] = final_df.groupby(['country_name', 'Item'])[col].transform(
            lambda x: x.fillna(method='ffill').fillna(method='bfill')
        )


#  Standardize text columns

text_cols = ['country_name', 'Item', 'Element']
for col in text_cols:
    final_df[col] = final_df[col].str.strip().str.title()

#  Fill weather columns with country-wide mean (ignoring year) as a fallback
for col in weather_cols:
    final_df[col] = final_df.groupby('country_name')[col].transform(
        lambda x: x.fillna(x.mean())
    )

#  For any remaining NaNs (e.g., entire country-year missing), fill with overall mean

for col in weather_cols:
    final_df[col].fillna(final_df[col].mean(), inplace=True)

#  Production values: still forward/backward fill
final_df['value'] = final_df.groupby(['country_name', 'Item'])['value'].transform(
    lambda x: x.ffill().bfill()
)

final_df = final_df.drop(columns=['sunshine_total_min'])

# Final check

print("Cleaned dataframe shape:", final_df.shape)
print(final_df.head())
print("\nMissing values after cleaning:\n", final_df.isnull().sum())

"""**8. Save your cleaned data to a new file.**


---



"""

#saving final_df to the drive
final_df.to_csv('/content/drive/MyDrive/Colab Notebooks/DATA 602 Project/final_cleaned_data.csv', index=False)

"""**9. Import the cleaned data fil**

---


"""

import pandas as pd

# Import CSV
cleaned_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DATA 602 Project/final_cleaned_data.csv')

#check
print("Imported cleaned dataframe shape:", cleaned_df.shape)

"""**10. Display**
* the first 5 data entries from the cleaned data file
* the last 5 data entries from the cleaned data file

---


"""

cleaned_df.head()

cleaned_df.tail()

"""## **PROJECT 3**

---


"""

# Import essential libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Display settings
pd.set_option('display.max_columns', None)
sns.set(style="whitegrid", palette="muted", font_scale=1.1)

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/DATA 602 Project/final_cleaned_data.csv")
df.head()

"""**2. Capture initial thoughts**
* Do you think you have the right data? Why or why not?
* What are your initial questions before knowing much about the data?

---





* Yes, I believe this dataset is well-suited for my analysis. It includes all the key variables I need to explore my research question, such as crop yields, country names, years of observation, and relevant environmental factors like temperature and rainfall. The dataset comes from a reputable and publicly accessible source, which gives me confidence in its accuracy and reliability. At this stage, it seems comprehensive enough to provide meaningful insights into trends and patterns in crop production across different regions and time periods.
* Before diving deeper into the data, I have a few questions that I hope this dataset can help answer:

  * Which countries consistently have the highest crop yields, and are there noticeable trends over the years?

  * How do environmental factors such as temperature and rainfall impact crop yields? Are there any strong correlations or patterns?

  * Are there missing values or unusual outliers in the dataset that could affect the results? How should they be handled to ensure accurate analysis?

  * Are there any seasonal or periodic trends in crop yields that might be hidden when looking at the raw data?

  These questions will guide my exploration as I begin analyzing the dataset and creating visualizations to better understand the relationships between these variables.
"""

# Basic info
print("Shape of dataset:", df.shape)
df.info()

# Display column names
list(df.columns)

"""**3. Explore characteristics of the data**
* What is the shape of your data?
* What does each record/row in the dataset represent?
* What variables/columns do you have?
* Are there any duplicates?
  1. How do you know?
  2. For any duplicates not already addressed in Pre-Processing:
    * How will you handle them? Handle them.

---

## Step 3: Explore Characteristics of the Data

**Shape of the dataset:**  
The dataset contains **564,291 rows and 17 columns**, which indicates that there is substantial amount of data to work with. Each row represents a specific measurement or observation for a particular crop, country, and year, along with associated environmental variables.  

**What each record/row represents:**  
Each row in the dataset corresponds to a single observation for a specific crop (or agricultural item) in a specific country during a specific year. For that observation, the dataset records the measured value (e.g., yield or production) along with several environmental factors such as temperature, precipitation, wind speed, and sea level pressure. Essentially, each row captures both **agricultural output and the surrounding environmental context**.  

**Variables/columns:**  
The dataset contains 17 columns with the following information:  

1. **country_name** – Name of the country (categorical, object type)  
2. **country_code** – Numeric code representing the country (int64)  
3. **Item** – Name of the crop or agricultural item (object)  
4. **Item Code** – Numeric code representing the crop/item (int64)  
5. **Element** – Type of measurement (e.g., production, yield) (object)  
6. **Element Code** – Numeric code for the measurement type (int64)  
7. **Unit** – Unit of the measurement (object)  
8. **year** – Year of the observation (int64)  
9. **value** – Observed measurement (float64; contains some missing values)  
10. **avg_temp_c** – Average temperature in °C (float64)  
11. **min_temp_c** – Minimum temperature in °C (float64)  
12. **max_temp_c** – Maximum temperature in °C (float64)  
13. **precipitation_mm** – Precipitation in millimeters (float64)  
14. **snow_depth_mm** – Snow depth in millimeters (float64)  
15. **avg_wind_speed_kmh** – Average wind speed in km/h (float64)  
16. **peak_wind_gust_kmh** – Peak wind gust in km/h (float64)  
17. **avg_sea_level_pres_hpa** – Average sea level pressure in hPa (float64)  

**Are there any duplicates?**  
I checked for duplicate rows and found that **there are no duplicates** in this dataset. This means each observation is unique, and no further cleaning for duplicates is necessary.  

**Handling duplicates (if any were found):**  
Since there are no duplicate rows, there is nothing to handle in this step. If duplicates had existed, I would have removed them using `drop_duplicates()` to ensure that repeated observations do not skew the analysis.  

"""

# Summary statistics
df.describe()

# Missing values
df.isnull().sum()

# Check duplicates
duplicates = df.duplicated().sum()
print("Number of duplicate rows:", duplicates)

"""4. Any additional transformations/manipulations you think you might need?
* ex. mapping categoricals to numbers, different units, different date format,
string formatting, logarithmic transformation, etc

---

## Step 4: Additional Transformations / Manipulations

While the dataset is mostly clean, there are a few transformations and manipulations that could help improve analysis and visualization:

1. **Mapping categorical variables to numbers (if needed):**  
   - Columns like `country_name`, `Item`, and `Element` are currently strings (object type).  
   - For certain analyses like machine learning models, these could be encoded as numerical categories using `LabelEncoder` or one-hot encoding.  

2. **Handling missing values in `value`:**  
   - The `value` column has **560,637 non-null entries** out of 564,291, which means there are some missing observations.  
   - Options for handling missing values:  
     - **Remove missing rows**: Simple approach if missing values are few.  
     - **Imputation**: Fill missing values using the mean, median, or interpolation based on other years/countries.  

3. **Date / Year format:**  
   - The `year` column is already numeric, which is good for plotting trends.  
   - If needed, we could convert it to a `datetime` type for time-series plots.  

4. **Unit consistency check:**  
   - Most units appear consistent (e.g., temperature in °C, precipitation in mm).  
   - No conversions are required unless we want alternative units for specific visualizations.  

5. **Possible logarithmic transformation:**  
   - For variables with highly skewed distributions, such as `value` (crop yield or production), applying a log transformation could normalize the distribution and make patterns easier to visualize.  

6. **String formatting:**  
   - Column names are mostly clean, but we could standardize them (e.g., lowercase, replace spaces with underscores) for easier coding:  
     ```python
     df.columns = df.columns.str.lower().str.replace(' ', '_')
     ```

These transformations will help ensure consistency in analysis and make visualizations and computations more straightforward.  

"""

# Remove duplicates
df = df.drop_duplicates()

# Handle missing values
df = df.fillna(df.median(numeric_only=True))

"""6. Explore every variable in the dataset
* What is the variable’s datatype?
* What units are the variable measured in, and how do you know the units?
* What does the variable represent?
* Any transformations/manipulations needed to better analyze the variable?
  *  Is this different from what you initially thought in step 5?
* Any missing data or extreme outliers?
  * How do you know if there are or not?
  * For any not already addressed in Pre-Processing:
  * How will you handle them? Handle them.
* What are the descriptive/summary statistics for the variable?
* What is the distribution of the variable’s values?
* What are the frequencies of values?
* Be sure to visualize!
  * Explain what you are seeing (or not seeing) in the visuals

---

# Exploratory Data Analysis (EDA) Summary

---





###  country_name
- **Datatype:** object (categorical/text)  
- **Units:** N/A – these are just country names  
- **What it represents:** Which country the agricultural observation is from  
- **Transformations needed:** None for now, though we might encode it numerically if we build machine learning models later  
- **Does this match initial expectations?** Yes, exactly what I expected  
- **Missing data / outliers:** No missing values (0%), and outliers don't really apply to country names  
- **Descriptive stats:** We have 30 unique countries in the dataset. Mexico appears most frequently with 31,437 records  
- **Distribution / frequencies:** The distribution is quite uneven – Mexico dominates the dataset, while some smaller countries have far fewer observations  
- **Visualization insight:** A count plot clearly shows Mexico has the most records, with other countries trailing behind.

---

### country_code
- **Datatype:** int64 (numeric, discrete)  
- **Units:** N/A – just a numeric identifier for each country  
- **What it represents:** A unique code assigned to each country (like a country ID)  
- **Transformations needed:** None necessary  
- **Missing / outliers:** No missing values, no outliers detected  
- **Descriptive stats:** Mean of 100.2, ranging from 8 to 234  
- **Distribution:** The codes are fairly spread out across the range. A histogram shows some clustering depending on how the codes were assigned  

---

### Item
- **Datatype:** object (categorical/text)  
- **Units:** N/A – product/crop names  
- **What it represents:** The type of agricultural product being measured (e.g., wheat, corn, eggs)  
- **Transformations needed:** Might group less common items together or encode for modeling  
- **Missing / outliers:** No missing values, no outliers (it's categorical)  
- **Descriptive stats:** 276 unique items! The most common is "Hen Eggs In Shell, Fresh" with 9,450 entries  
- **Distribution / frequencies:** Highly skewed – some products appear very frequently while others are rare  
- **Visualization insight:** A horizontal bar plot works best here to avoid overlapping labels. We can see that egg production dominates, with many other products having much smaller representation  

---

### Item Code
- **Datatype:** int64 (numeric, discrete)  
- **Units:** N/A – corresponds to each Item  
- **What it represents:** A unique numeric identifier for each agricultural product  
- **Transformations needed:** None  
- **Missing / outliers:** No missing values, but 4,347 potential outliers detected  
- **Descriptive stats:** Mean of 893.2, ranging from 15 to 17,530 (quite a wide range!)  
- **Distribution:** Right-skewed, meaning most codes cluster toward lower values with a long tail of higher codes  

---

###  Element
- **Datatype:** object (categorical)  
- **Units:** N/A – describes the type of measurement  
- **What it represents:** What aspect of agriculture is being measured (e.g., Production quantity, Area harvested, Yield)  
- **Transformations needed:** None  
- **Missing / outliers:** No missing values, no outliers  
- **Descriptive stats:** 8 unique measurement types, with "Production" being the most common (220,626 observations)  
- **Distribution:** Heavily skewed toward Production measurements  
- **Visualization insight:** A bar chart shows Production dominates, which makes sense since that's our primary focus  

---

###  Element Code
- **Datatype:** int64 (numeric, discrete)  
- **Units:** N/A – corresponds to each Element type  
- **What it represents:** Numeric code for the measurement type  
- **Missing / outliers:** No missing values, no outliers  
- **Descriptive stats:** Mean of 5,407.8, ranging from 5,111 to 5,513  

---

###  Unit
- **Datatype:** object (categorical)  
- **Units:** This column tells us the unit of measurement (e.g., tonnes "t", kilograms "kg", hectares "ha")  
- **What it represents:** The measurement unit for the value column  
- **Missing / outliers:** No missing values  
- **Descriptive stats:** 10 unique units, with "t" (tonnes) being most common (218,547 observations)  
- **Visualization insight:** Most data is measured in tonnes, which is standard for agricultural production  

---

###  year
- **Datatype:** int64 (numeric, discrete)  
- **Units:** Calendar year  
- **What it represents:** The year when the observation was recorded  
- **Missing / outliers:** No missing values, no outliers  
- **Descriptive stats:** Ranges from 1961 to 2023, with a mean of 1992 (right in the middle!)  
- **Distribution:** Fairly uniform across the decades – we have good temporal coverage  
- **Visualization insight:** A histogram shows relatively even distribution across years, giving us a solid time series to work with  

---

###  value
- **Datatype:** float64 (numeric, continuous)  
- **Units:** Depends on the Unit column (usually tonnes for production)  
- **What it represents:** The actual measurement – how much was produced, area harvested, or yield achieved  
- **Transformations needed:** Definitely consider a log transformation because of the extreme right skew  
- **Missing / outliers:** Only 0% missing (all filled now!), but 99,584 potential outliers detected  
- **Descriptive stats:** Mean of 376,620 but median of only 5,341.5 – this huge difference screams "skewed data!" The range goes from 0 to 28,388,030  
- **Distribution:** Extremely right-skewed. Most values are small (thousands), but a few are massive (millions)  
- **How I'm handling it:** The missing values were already addressed in preprocessing. For outliers, I'll keep them since they represent legitimate large-scale production (like major grain harvests), but I'll analyze them separately  
- **Visualization insight:** A histogram shows most data clustered at low values with a long tail. A log-scale plot makes the distribution much easier to see  

---

###  avg_temp_c, min_temp_c, max_temp_c
- **Datatype:** float64 (numeric, continuous)  
- **Units:** Degrees Celsius (°C)  
- **What they represent:** Average, minimum, and maximum temperatures for the location and time period  
- **Missing / outliers:** No missing values. Some extreme outliers exist (very cold or hot temperatures), but they're likely real measurements from extreme climates  
- **Descriptive stats:**  
  - avg_temp_c: Mean 21.3°C (nice and temperate!)  
  - min_temp_c: Mean 16.0°C, goes as low as -7.6°C  
  - max_temp_c: Mean 26.4°C, reaches up to 36.3°C  
- **Distribution:** Mostly normal distributions with some negative outliers (cold regions) and positive outliers (hot regions)  
- **Visualization insight:** Temperature distributions look fairly bell-shaped, which is expected for climate data  

---

###  precipitation_mm
- **Datatype:** float64 (numeric, continuous)  
- **Units:** Millimeters (mm)  
- **What it represents:** Amount of rainfall  
- **Missing / outliers:** No missing values, 16,810 potential outliers (heavy rainfall events)  
- **Descriptive stats:** Mean of 5.0 mm, ranging from 0 to 49.6 mm  
- **Distribution:** Right-skewed – most days have moderate rainfall, but some have extreme downpours  
- **Visualization insight:** Most precipitation values cluster below 10mm, with occasional heavy rainfall spikes  

---

###  snow_depth_mm
- **Datatype:** float64 (numeric, continuous)  
- **Units:** Millimeters (mm)  
- **What it represents:** Depth of snow accumulation  
- **Missing / outliers:** No missing values, 65,979 potential outliers  
- **Descriptive stats:** Mean of 67.3 mm. Interestingly, the median equals the 25th and 75th percentiles (67.3mm), suggesting many constant/imputed values  
- **Distribution:** Right-skewed with many zeros (no snow in tropical regions) and some very high values  
- **Visualization insight:** Bimodal distribution – lots of zeros (warm countries) and a separate cluster where snow is present  

---

###  avg_wind_speed_kmh, peak_wind_gust_kmh
- **Datatype:** float64 (numeric, continuous)  
- **Units:** Kilometers per hour (km/h)  
- **What they represent:** Average wind speed and peak wind gusts  
- **Missing / outliers:**  
  - avg_wind_speed_kmh: No missing, 15,825 outliers  
  - peak_wind_gust_kmh: No missing, 137,718 outliers (!!)  
- **Descriptive stats:**  
  - avg_wind_speed_kmh: Mean 11.8 km/h  
  - peak_wind_gust_kmh: Mean 36.9 km/h, but median also 36.9 with identical quartiles (suggests constant value for many rows)  
- **Distribution:** peak_wind_gust_kmh shows a suspiciously flat distribution – likely missing data that was filled with a constant  
- **Visualization insight:** Average wind speed looks normal, but peak gust has data quality issues  

---

###  avg_sea_level_pres_hpa
- **Datatype:** float64 (numeric, continuous)  
- **Units:** Hectopascals (hPa)  
- **What it represents:** Average atmospheric pressure at sea level  
- **Missing / outliers:** No missing values, 2,529 outliers  
- **Descriptive stats:** Mean of 1,013.9 hPa (very close to standard atmospheric pressure!), ranging from 1,006.9 to 1,022.3 hPa  
- **Distribution:** Tightly clustered around the mean with small deviations – this is expected for atmospheric pressure  
- **Visualization insight:** Nearly normal distribution centered around 1,014 hPa  

---

##  Summary Observations

**Missing Data:**  
- Only the `value` column had any missing data, and that was less than 1% (already handled in preprocessing)

**Outliers:**  
- Several variables have outliers: `value`, `Item Code`, temperature variables, precipitation, snow depth, and wind measurements
- Most outliers are legitimate extreme values (like massive production outputs or extreme weather) rather than errors

**Highly Skewed Variables:**  
- `value` and `Item Code` are extremely right-skewed – should consider log transformations for modeling
- `precipitation_mm` and `snow_depth_mm` are also right-skewed

**Categorical Variables:**  
- `Item` and `country_name` show significant imbalance – some categories dominate while others are rare


**Data Quality Notes:**  
- `peak_wind_gust_kmh` shows signs of imputation (constant values for many rows)
- `snow_depth_mm` also has patterns suggesting imputed values

"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Explore every variable
for col in df.columns:
    print(f"\n{'='*50}\nVariable: {col}")

    # Data type
    print(f"Datatype: {df[col].dtype}")

    # Basic info: count, unique values, top value, frequency
    print("Summary Info:")
    print(df[col].describe(include='all'))

    # Missing data
    missing_count = df[col].isnull().sum()
    print(f"Missing values: {missing_count} ({missing_count/len(df)*100:.2f}%)")

    # Check extreme outliers for numeric columns using IQR
    if np.issubdtype(df[col].dtype, np.number):
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]
        print(f"Number of potential outliers: {outliers.shape[0]}")

    # Visualization
    if np.issubdtype(df[col].dtype, np.number):
        plt.figure(figsize=(12,5))
        sns.histplot(df[col], kde=True, bins=30)
        plt.title(f"Distribution of {col}", fontsize=14, pad=15)
        plt.xlabel(col, fontsize=12, labelpad=10)
        plt.ylabel("Frequency", fontsize=12, labelpad=10)
        plt.tight_layout()
        plt.show()
    else:
        # Dynamically set figure height based on number of unique categories
        plt.figure(figsize=(12, max(6, len(df[col].unique()) * 0.3)))
        sns.countplot(y=col, data=df, order=df[col].value_counts().index)
        plt.title(f"Value Counts of {col}", fontsize=14, pad=15)
        plt.xlabel("Frequency", fontsize=12, labelpad=10)
        plt.ylabel(col, fontsize=12, labelpad=10)
        plt.yticks(fontsize=10)
        plt.tight_layout()
        plt.show()

"""6. Explore relationships between variables
* Which ones appear independent? Which ones appear dependent and on what?
* Explore possible:
  * Event frequencies
  * Periodicity/seasonality
  * Correlations between variables
  * Pairwise relationships
  * Anything else that could be interesting/useful in understanding your
data
* Be sure to visualize!
* Be sure you explain in detail what you are finding/seeing (or not) in the data.
* Optional: Extra Credit (up to 15 points): Simpson’s Paradox
  * [Up to 5 points each] If your dataset has variables with subgroups (ex.
married vs. single, age groups, gender, illness severity groups, etc), do
the same plots for each individual subgroup and compare them to the
plot with the entire variable sample.
* Are there any interesting patterns or possible evidence of
Simpson’s Paradox? Explain.

---
## Step 6: Explore Relationships Between Variables

In this step, I examined how different variables in the dataset relate to each other. Understanding these relationships is crucial for identifying which climate factors actually influence agricultural production and for detecting any patterns or trends over time.

---

###  **Correlation Matrix Analysis**

The correlation heatmap reveals the strength and direction of linear relationships between all numeric variables.

**Strong Positive Correlations (>0.90):**
- **Temperature variables are highly correlated with each other:**
  - `avg_temp_c` ↔ `min_temp_c`: 0.98
  - `avg_temp_c` ↔ `max_temp_c`: 0.97
  - `min_temp_c` ↔ `max_temp_c`: 0.94

This makes perfect sense! When average temperatures are high, both minimum and maximum temperatures tend to be high as well. These three variables essentially measure the same underlying phenomenon (temperature), just from different angles. **This is important for modeling** – we might only need to use one temperature variable to avoid multicollinearity.

**Moderate Positive Correlations (0.30-0.40):**
- **Temperature ↔ Precipitation:** Around 0.36
  - Warmer regions tend to have slightly more rainfall, which aligns with tropical climate patterns where both temperature and precipitation are high

**Moderate Negative Correlations (-0.40 to -0.48):**
- **Temperature ↔ Wind Speed:** -0.44 (avg_temp vs avg_wind_speed)
  - Cooler regions tend to be windier, possibly because they're at higher latitudes or elevations
- **Temperature ↔ Sea Level Pressure:** -0.41 to -0.43
  - Warmer areas have lower atmospheric pressure (hot air rises), which is a well-known meteorological principle
- **Temperature ↔ Snow Depth:** -0.16 to -0.19
  - Obvious relationship – colder places have more snow!

**Weak or No Correlation:**
- **Production value (`value`) shows very weak correlations with most variables** (all <0.10)
  - This is actually surprising and concerning! It suggests that simple linear relationships between individual climate variables and production don't exist
  - This doesn't mean climate doesn't matter – it likely means the relationship is more complex (non-linear, interactive, or confounded by other factors like crop type and country)

**Independent vs Dependent Variables:**
- **Independent (don't depend on production):** All climate variables (temperature, precipitation, wind, pressure) – these are determined by geography and atmospheric conditions
- **Dependent (our outcome of interest):** Production `value` – this is what we're trying to predict
- **Confounders:** Country, crop type, and year act as confounding variables that influence both climate exposure and agricultural practices

---

###  **Pairwise Relationships**

This pairplot shows scatter plots between five key variables, giving us a more detailed view than the correlation matrix.

**Key Observations:**

1. **Temperature variables form tight linear patterns:**
   - The diagonal relationships between avg_temp, min_temp, and max_temp show clear linear progressions
   - Very little scatter around the trend line, confirming their extremely high correlation

2. **Production value shows distinct banding patterns:**
   - When plotted against temperature, `value` shows horizontal bands or clusters
   - These bands likely represent different crop types or countries – each has its own typical production range
   - **This explains the low correlation!** The relationship between temperature and production differs by crop/country

3. **Precipitation shows a clustered distribution:**
   - Most observations have low to moderate precipitation (0-20mm)
   - A few extreme precipitation events exist (up to 50mm)
   - Against temperature: no clear linear trend, but higher temperatures do allow for higher precipitation in some cases (tropical regions)

4. **Value distribution is extremely skewed:**
   - The histograms on the diagonal show `value` is heavily right-skewed
   - Most production values are small, with a long tail of very large values
   - This confirms the need for log transformation for any modeling

**What's NOT appearing in the relationships:**
- No clear linear relationship between any single climate variable and production value
- This suggests we need to look at interactions, non-linear effects, or control for confounders

---

###  **Event Frequencies: Production Over Time**

This line chart shows total production value by country from 1961 to 2023.

**Temporal Trends:**

1. **Overall upward trend across decades:**
   - Most countries show increasing production over the 60+ year period
   - This aligns with technological improvements, better farming practices, and possibly climate change effects

2. **Major producers dominate:**
   - Two lines stand out at the top (pink and orange) – these are likely Brazil and the United States, the largest agricultural producers in the Americas
   - These countries produce 4-5× more than mid-tier countries

3. **Variability increases over time:**
   - More fluctuation in recent decades compared to the 1960s-1970s
   - Could indicate increased climate variability or economic factors affecting production

4. **Some countries show decline or stagnation:**
   - A few lines remain flat or decline slightly over time
   - This could be due to land use changes, economic shifts, or climate challenges

**What this tells us about periodicity:**
- **No obvious seasonal pattern** (which makes sense since this is annual data)
- **Long-term secular trend**: Clear increase over decades
- **Year-to-year volatility**: Production fluctuates, possibly due to weather variability, pests, or economic factors

---

###  **Distribution by Country**

The boxplot reveals enormous differences in production across countries.

**Key Findings:**

1. **Extreme outliers dominate:**
   - Brazil, Canada, Mexico, and the United States show production values reaching 2.5-2.8×10⁷
   - These outliers are so extreme they compress the boxes into thin lines at the bottom

2. **Most production is actually quite small:**
   - The boxes (representing 50% of the data) are barely visible because they're so close to zero
   - Median production for most countries is far below the outliers

3. **High between-country variability:**
   - Some countries (likely smaller or less agriculturally intensive) have consistently low production
   - Major agricultural producers have both high medians and extreme outliers

4. **Country is clearly a major factor:**
   - The huge differences between countries show that simply looking at climate won't predict production
   - We must control for country-level differences (technology, land area, farming intensity, policies)

**Insight:** This visualization confirms that **country is a critical confounding variable** that must be addressed in any analysis.

---

###  **Average Value per Decade**

This bar chart shows a clear upward trend in agricultural production over time.

**Trend Analysis:**

1. **Consistent growth every decade:**
   - 1960s: ~300,000 average value
   - 2020s: ~450,000 average value
   - That's a 50% increase over 60 years!

2. **Acceleration in recent decades:**
   - Growth rate appears faster from 2000 onwards
   - Could be due to technological advances, expansion of agricultural land, or climate factors

3. **What's driving this increase?**
   - **Technology**: Better seeds, fertilizers, irrigation, machinery
   - **Climate change**: Some regions may benefit from warming (longer growing seasons, CO₂ fertilization)
   - **Land use**: More land converted to agriculture
   - **Population-driven demand**: More mouths to feed drives intensification

**Implications for our research question:**
- If production is increasing while temperatures are also increasing (due to climate change), we need to be careful about causation
- The trend could be masking negative climate impacts in some regions while showing positive impacts in others

---

###  **Production vs Temperature**

This scatterplot directly addresses our research question: Does temperature affect production?

**What I'm Seeing:**

1. **Vertical banding at specific temperatures:**
   - Data clusters at specific temperature values (around 0°C, 12°C, 15°C, 25°C)
   - This suggests observations are grouped by location/country (each has a characteristic temperature)

2. **No clear linear relationship:**
   - Production values are spread across the full range at every temperature level
   - High production occurs at both cold (-5°C) and hot (30°C) temperatures

3. **Some temperature ranges have more data:**
   - Most observations fall between 10-25°C (temperate to warm climates)
   - Fewer observations in very cold (<0°C) or very hot (>28°C) regions

4. **Production ranges are constant across temperatures:**
   - At every temperature, we see production from near-zero to 2.5×10⁷
   - This constant spread suggests temperature alone isn't determining production capacity

**What this DOESN'T show:**
- No obvious positive or negative trend
- No evidence that warmer temperatures systematically increase or decrease production

**Why might this be?**
- **Different crops thrive at different temperatures:** Wheat prefers cooler climates, while corn and soybeans prefer warmer ones
- **Countries confound the relationship:** USA (temperate) and Brazil (tropical) both have high production despite different climates
- **Non-linear effects:** Perhaps there's an optimal temperature range, and both too cold and too hot are bad

---

###  **Wind Speed vs Precipitation**

This explores the relationship between two climate variables.

**Observations:**

1. **Weak positive relationship:**
   - Slight upward trend – areas with more wind tend to have slightly more precipitation
   - But the relationship is very noisy with lots of scatter

2. **Most data clusters at low values:**
   - The majority of observations have wind speeds of 5-15 km/h and precipitation of 0-10mm
   - This is typical weather for agricultural regions (not too extreme)

3. **Outliers exist:**
   - A few observations show very high precipitation (up to 50mm) or high wind speeds (up to 25 km/h)
   - These might represent storm events or particularly windy/rainy locations

4. **Horizontal banding:**
   - Some rows of points at specific precipitation values suggest possible data artifacts or measurement conventions

**Interpretation:**
- Wind and precipitation are both weather phenomena that can co-occur (stormy conditions bring both)
- But the relationship is weak – you can have rain without much wind and vice versa
- Neither variable shows a strong relationship with production (from earlier analyses)

---

###  **Country-Element Heatmap**

This heatmap shows average values for different agricultural measurements across countries.

**Major Insights:**

1. **Production and Stocks dominate:**
   - Darker blue colors in the "Production" and "Stocks" columns indicate these have the highest average values
   - Mexico, Colombia, and USA show particularly high production values

2. **Canada shows unique pattern:**
   - High values for "Laying" (egg production) and "Animals" measurements
   - Suggests Canada specializes in livestock rather than crops

3. **Many countries have missing measurements:**
   - Light yellow cells indicate very low or zero values
   - Not all countries measure all elements (or some elements don't apply)

4. **Yield vs Production patterns differ:**
   - "Yield" column (far right) shows different intensity pattern than "Production"
   - High production doesn't always mean high yield – could indicate large area rather than efficiency

5. **Regional patterns:**
   - South American countries (Colombia, Paraguay) show different patterns than North American ones
   - Could reflect different agricultural systems, climates, or specializations

**What this reveals about relationships:**
- Different countries specialize in different types of agricultural production
- Production values aren't comparable across countries without considering what's being measured
- Element type is another important confounder in our analysis

---

##  **Summary of Relationships**

### **Variables that Appear Independent:**
1. **Climate variables from production outcomes:**
   - Temperature, precipitation, wind speed don't show strong linear relationships with production value
   - They're determined by geography, not agricultural activity

2. **Different crop types:**
   - Each crop appears to have its own climate requirements
   - They respond independently to the same climate conditions

### **Variables that Appear Dependent:**

1. **Production value depends on:**
   - **Country** (MAJOR factor – accounts for technology, land area, policies)
   - **Crop/Item type** (different crops have different yields)
   - **Year/Time** (secular trend shows steady increase)
   - **Climate variables** (but in complex, non-linear ways)

2. **Temperature variables depend on each other:**
   - Min, max, and average temperatures are nearly perfectly correlated
   - They're all measuring the same underlying temperature regime

3. **Wind and pressure depend on temperature:**
   - Moderate negative correlations suggest atmospheric dynamics

### **Event Frequencies:**
- Production has increased steadily over 60+ years
- Year-to-year fluctuations exist but are relatively small compared to the long-term trend
- No obvious periodic/seasonal patterns (since data is annual)

### **Correlations:**
- **Strong:** Temperature variables with each other (0.94-0.98)
- **Moderate:** Temperature with precipitation (0.36), wind (-0.44), pressure (-0.41)
- **Weak/None:** Climate variables with production value (<0.10)

### **Key Takeaway for the Research Question:**

The lack of simple linear relationships between climate and production doesn't mean climate doesn't matter! Instead, it tells us:

1. **The relationship is complex** – likely involves interactions between variables, non-linear effects, and thresholds
2. **Confounders are critical** – we MUST control for country and crop type to see climate effects
3. **Different crops respond differently** – what's good for wheat might be bad for soybeans
4. **We need more sophisticated analysis** – regression modeling with interactions, or separate analyses by crop type

"""

# Step 6: Explore Relationships Between Variables

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set plot style
sns.set(style="whitegrid", palette="muted", font_scale=1.1)

#  Correlation heatmap for numeric variables
numeric_cols = df.select_dtypes(include=[np.number]).columns

plt.figure(figsize=(12,8))
corr_matrix = df[numeric_cols].corr()
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Matrix of Numeric Variables")
plt.tight_layout()
plt.show()
print("\n\n")  # Adds space after plot

#  Pairwise relationships for key numeric variables
pair_cols = ["avg_temp_c", "min_temp_c", "max_temp_c", "value", "precipitation_mm"]
sns.pairplot(df[pair_cols], kind="scatter", plot_kws={'alpha':0.3})
plt.suptitle("Pairwise Relationships Between Key Variables", y=1.02)
plt.show()
print("\n\n")  # Adds space after plot

#  Event frequencies: sum of production value by country over years
production_df = df[df["Element"]=="Production"].groupby(["country_name","year"])["value"].sum().reset_index()

plt.figure(figsize=(14,6))
sns.lineplot(data=production_df, x="year", y="value", hue="country_name", legend=False)
plt.title("Total Production Value by Country Over Years")
plt.xlabel("Year")
plt.ylabel("Production Value")
plt.tight_layout()
plt.show()
print("\n\n")  # Adds space after plot

#  Boxplots for categorical vs numeric relationships
plt.figure(figsize=(12,6))
sns.boxplot(data=df[df["Element"]=="Production"], x="country_name", y="value")
plt.title("Distribution of Production Value by Country")
plt.xlabel("Country")
plt.ylabel("Production Value")
plt.xticks(rotation=75)
plt.tight_layout()
plt.show()
print("\n\n")  # Adds space after plot

#  Average value per decade
df['decade'] = (df['year']//10)*10
decade_avg = df.groupby('decade')['value'].mean().reset_index()

plt.figure(figsize=(10,5))
sns.barplot(x='decade', y='value', data=decade_avg)
plt.title("Average Value per Decade")
plt.xlabel("Decade")
plt.ylabel("Average Value")
plt.tight_layout()
plt.show()
print("\n\n")  # Adds space after plot

#  Scatterplot for temperature vs production value
temp_df = df[df["Element"]=="Production"]
plt.figure(figsize=(10,6))
sns.scatterplot(data=temp_df, x="avg_temp_c", y="value", alpha=0.3)
plt.title("Production Value vs Average Temperature")
plt.xlabel("Average Temperature (°C)")
plt.ylabel("Production Value")
plt.tight_layout()
plt.show()
print("\n\n")  # Adds space after plot

#  Wind vs precipitation correlation
plt.figure(figsize=(8,5))
sns.scatterplot(data=df, x="avg_wind_speed_kmh", y="precipitation_mm", alpha=0.3)
plt.title("Average Wind Speed vs Precipitation")
plt.xlabel("Average Wind Speed (km/h)")
plt.ylabel("Precipitation (mm)")
plt.tight_layout()
plt.show()
print("\n\n")  # Adds space after plot

#  Pairwise heatmap for categorical-numeric aggregation
country_element_avg = df.groupby(["country_name", "Element"])["value"].mean().unstack()
plt.figure(figsize=(12,6))
sns.heatmap(country_element_avg, cmap="YlGnBu", annot=False)
plt.title("Average Value per Country and Element")
plt.xlabel("Element")
plt.ylabel("Country")
plt.tight_layout()
plt.show()
print("\n\n")  # Adds space after plot

"""**Simpson’s Paradox**
* [Up to 5 points each] If your dataset has variables with subgroups (ex.
married vs. single, age groups, gender, illness severity groups, etc), do
the same plots for each individual subgroup and compare them to the
plot with the entire variable sample.
  * Are there any interesting patterns or possible evidence of
Simpson’s Paradox? Explain

---
## Optional Extra Credit: Simpson's Paradox Analysis

### **Evidence of Simpson's Paradox: YES! Strong and Widespread**

I examined whether the relationship between average temperature and agricultural production differs when we look at the entire dataset versus individual countries. The results reveal a **example of Simpson's Paradox** that has major implications for understanding climate impacts on agriculture.

---

### **What is Simpson's Paradox and Why Does It Matter Here?**

Simpson's Paradox occurs when a trend observed in aggregated data **reverses or disappears** when the data is separated into subgroups. In this analysis:

- **Subgroup variable:** Country (30 countries in the Americas)
- **Independent variable:** Average Temperature (°C)
- **Dependent variable:** Production Value

**Why this matters for my research:** If we ignore country-level differences and just look at the aggregate relationship between temperature and production, we might draw completely wrong conclusions about how climate affects agriculture. Different countries have different agricultural systems, technologies, and baseline climates, making them respond differently to temperature.

---

### **The Paradox Revealed: Aggregate vs. Individual Countries**

#### **Overall Trend (Black Line in Visualization):**
- **Slope: -24,254.90**
- **Direction: NEGATIVE**
- **Interpretation:** When we lump all countries together, there appears to be a *slight negative* relationship between temperature and production. This suggests that warmer temperatures are associated with lower production across the Americas.

#### **Individual Country Trends (Colored Points and Lines):**
- **20 out of 30 countries show POSITIVE slopes** (opposite of the overall trend!)
- **These countries are marked in RED** in the visualization
- **10 countries show negative slopes** (consistent with overall trend, shown in BLUE)

**This is Simpson's Paradox in action:** The aggregate negative trend completely masks the fact that **most individual countries** show positive relationships between temperature and production!

---

### **Countries with Strongest Simpson's Paradox**

Here are the countries where the paradox is most pronounced (ranked by magnitude of difference from overall trend):

**Top 5 Countries with Opposite Trends:**

1. **Guatemala**
   - Within-country slope: +180,524.96
   - Difference from overall: 204,779.86
   - As temperature increased over time in Guatemala, production increased dramatically

2. **Argentina**
   - Within-country slope: +95,891.01
   - Difference from overall: 120,145.91
   - Shows strong positive temperature-production relationship

3. **Peru**
   - Within-country slope: +93,976.75
   - Difference from overall: 118,231.66
   - Production growth coincided with warming

4. **Mexico**
   - Within-country slope: +90,255.65
   - Difference from overall: 114,510.55
   - One of the largest producers, shows clear positive trend

5. **Canada**
   - Within-country slope: +56,960.42
   - Difference from overall: 81,215.32
   - Cold climate country benefiting from warming

**All 20 paradox countries:** Guatemala, Argentina, Peru, Mexico, Canada, Chile, Colombia, Honduras, United States, Nicaragua, El Salvador, Costa Rica, Belize, Cuba, Uruguay, Ecuador, Saint Kitts and Nevis, Puerto Rico, Haiti, and Grenada

---

### **Interesting Patterns Observed**

#### **1. Geographic Pattern in the Paradox**

Looking at the visualization, I notice distinct clustering:

- **Coldest regions (−5°C to 5°C):**
  - Countries like Canada and Chile (red points on the left)
  - These cold-climate countries show **positive slopes**
  - Interpretation: Warming may be *beneficial* by extending growing seasons

- **Temperate regions (10°C to 20°C):**
  - Dense red cluster showing many countries with positive trends
  - Includes major producers like USA, Mexico, Argentina
  - These countries increased production as temperatures rose slightly

- **Warmest regions (20°C to 30°C):**
  - Mix of red and blue points
  - Some tropical countries show positive trends, others negative
  - Suggests complex, non-linear relationships at high temperatures

#### **2. The "Vertical Banding" Pattern**

The visualization shows **distinct vertical bands** of points at specific temperatures:
- Each country has a relatively **constant average temperature**
- Within-country temperature variation is minimal (just a few degrees over 60 years)
- This means the "slopes" we're detecting are really **temporal trends** – how production changed over time as temperatures slowly increased

#### **3. Production Scale Varies Enormously**

- Production values range from near-zero to 2.8×10⁷
- Countries cluster at different production levels (their characteristic baselines)
- **This is why the aggregate trend is negative:** Countries with cooler climates (like USA, Canada, Argentina) happen to be major producers, while some warmer tropical countries have lower production. When aggregated, this creates a spurious negative correlation.

---

### **Why Does This Paradox Occur? (The Confounding Story)**

The Simpson's Paradox arises because **country is a major confounding variable:**

**Three factors creating the paradox:**

1. **Different baseline climates:**
   - Cold countries: Canada (5°C), Argentina (10°C)
   - Temperate countries: USA (15°C), Mexico (20°C)
   - Tropical countries: Colombia (25°C), Ecuador (27°C)

2. **Different production capacities:**
   - Large temperate countries (USA, Canada, Argentina, Brazil) have **massive agricultural sectors** with high production
   - Smaller tropical countries have **lower absolute production**
   - When we aggregate, the high-production cold/temperate countries dominate, creating a false negative correlation

3. **Different temporal trends:**
   - **Most countries increased production over 1961-2023** (technology, better practices)
   - This increase happened **while temperatures were rising** (climate change)
   - Within each country: warming coincided with production growth (positive slope)
   - Across countries: cold countries with high production vs. warm countries with lower production (negative aggregate slope)

**The paradox formula:**
```
Aggregate trend = Weighted average of individual trends + Between-group differences

Negative overall slope = (Mostly positive within-country slopes) + (Cold countries produce more than warm countries)
```

---

### **Implications for My Research Question**

**"How will climate change impact agricultural food production in the American continent?"**

This Simpson's Paradox analysis reveals three critical insights:

#### **1. You Cannot Use Aggregate Analysis**
If I had only looked at the overall negative slope (-24,254), I might conclude: *"Warmer temperatures reduce production – climate change is bad for agriculture."*

**But this would be WRONG!** The majority of countries (20/30) actually show positive relationships. The negative aggregate trend is a statistical artifact caused by mixing countries with fundamentally different characteristics.

#### **2. Climate Effects Are Country-Specific**
The paradox shows that **there is no single answer** to how temperature affects production. Instead:
- **Cold-climate countries (Canada, Chile):** Warming appears beneficial (longer growing seasons, less frost)
- **Temperate countries (USA, Argentina, Mexico):** Production increased alongside warming (though causation unclear)
- **Some tropical countries:** May already be at temperature limits

#### **3. Context Matters More Than Raw Temperature**
The relationship between temperature and production depends on:
- **Starting temperature** (optimal ranges differ by crop)
- **Agricultural technology and infrastructure**
- **Water availability**
- **Crop types grown**
- **Economic and policy factors**

Temperature alone cannot predict agricultural outcomes – we must consider the **full context**.

---

### **Statistical Interpretation**

**What the numbers mean:**

- **Overall slope of -24,254:** For every 1°C increase in temperature across all countries, production decreases by ~24,254 units on average
  - But this is **misleading** due to confounding!

- **Guatemala's slope of +180,524:** Within Guatemala, as temperature increased over time, production increased by ~180,525 units per °C
  - This is **6-7 times larger** than the overall slope, but in the **opposite direction**!

- **The paradox magnitude (difference from overall):**
  - Guatemala differs by 204,780 units – the strongest paradox
  - Even small countries like Grenada (+88) still show opposite trends
  - **67% of countries (20/30)** exhibit the paradox

---

### **Visualization Insights**

From the Simpson's Paradox plot, I can see:

1. **Red points dominate the middle temperature ranges (10-25°C)**
   - This is where most agricultural activity happens
   - These countries show positive trends despite the overall negative aggregate

2. **The black overall regression line is nearly flat (slight negative)**
   - Crosses the data cloud horizontally
   - Shows weak aggregate relationship

3. **Individual country regression lines (small red/blue lines) have varied slopes**
   - Red lines generally point upward (positive slopes)
   - Blue lines point downward (negative slopes)
   - No single pattern fits all countries

4. **The blue cluster around 25°C**
   - Appears to be Brazil (based on temperature and high production)
   - One of the few major producers with a negative trend
   - Its large sample size pulls the overall line down

---

### **Limitations and Caveats**

**Important considerations for interpreting these results:**

1. **Within-country temperature variation is minimal:**
   - Each country's average temperature only varies by a few degrees over 62 years
   - We're detecting **temporal correlations** (production changes over time) more than true **temperature effects**
   - Confounding by technology improvements, land use changes, and other time-varying factors

2. **Correlation ≠ Causation:**
   - Just because production increased while temperature increased doesn't mean warming *caused* the increase
   - Could be due to: better seeds, more fertilizer, irrigation expansion, farm subsidies, etc.

3. **Ecological fallacy:**
   - The aggregate relationship tells us about differences *between* countries
   - Individual country relationships tell us about changes *within* countries over time
   - Neither directly answers: "What would happen if we artificially raised temperature?"

4. **Missing the crop-level story:**
   - Different crops respond differently to temperature
   - A country-level analysis might show positive trends because farmers *switched* to heat-tolerant crops
   - Need crop-specific analyses to fully understand climate impacts

---

### **Conclusion: Yes, Strong Evidence of Simpson's Paradox**

**Summary of findings:**

 **Simpson's Paradox is present and widespread:**
- Overall trend: Negative slope (-24,254)
- Individual country trends: 67% show opposite (positive) slopes
- Magnitude of paradox: Very large (up to 204,780 units difference)

 **The paradox teaches us important lessons:**
- **Country is a critical confounder** that must be controlled for in any climate-production analysis
- **Aggregate analyses can be deeply misleading** when subgroups have different baseline characteristics
- **Context matters:** Climate impacts depend on starting conditions, technologies, and local factors

 **Implications for future analysis:**
- Must use **multivariate regression** controlling for country, year, and crop type
- Should conduct **stratified analyses** (separate models for different regions or crops)
- Need to distinguish **between-country** effects (different climates) from **within-country** effects (temporal changes)

**Final thought:** This Simpson's Paradox analysis has fundamentally changed how I need to approach my research question. Rather than asking "Does temperature affect production?" I should ask: "How do temperature effects differ by country, crop type, and baseline climate?" The paradox reveals that there's no single answer – climate impacts are **heterogeneous and context-dependent**.



"""

# Step 7c: Detect and summarize Simpson's Paradox with automatic report

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import linregress

sns.set(style="whitegrid", palette="muted", font_scale=1.1)

subgroup_col = "country_name"   # Subgroup variable
main_col = "avg_temp_c"
value_col = "value"

# Filter for Production only
df_prod = df[df["Element"]=="Production"]

#  Overall trend
overall_slope, overall_intercept, r_value, p_value, std_err = linregress(
    df_prod[main_col], df_prod[value_col]
)
print(f"Overall slope: {overall_slope:.5f}\n")

#  Subgroup trends & detect Simpson's Paradox
simpsons_paradox_subgroups = []
subgroup_slopes = {}

for subgroup in df_prod[subgroup_col].unique():
    subset = df_prod[df_prod[subgroup_col]==subgroup]
    if len(subset) < 10:  # skip very small subgroups
        continue
    slope, intercept, r_value, p_value, std_err = linregress(
        subset[main_col], subset[value_col]
    )
    subgroup_slopes[subgroup] = slope
    # Check if slope has opposite sign to overall trend
    if overall_slope * slope < 0:
        simpsons_paradox_subgroups.append(subgroup)

#  Sort subgroups by magnitude of paradox (difference in slope)
slope_diff = {sg: abs(overall_slope - s) for sg, s in subgroup_slopes.items() if sg in simpsons_paradox_subgroups}
slope_diff_sorted = dict(sorted(slope_diff.items(), key=lambda item: item[1], reverse=True))

print("Subgroups with strongest Simpson's Paradox (opposite slope):")
for sg, diff in slope_diff_sorted.items():
    print(f"- {sg}: slope = {subgroup_slopes[sg]:.5f}, diff from overall = {diff:.5f}")

#  Visualization
plt.figure(figsize=(14,7))
for subgroup, slope in subgroup_slopes.items():
    subset = df_prod[df_prod[subgroup_col]==subgroup]
    color = 'red' if subgroup in simpsons_paradox_subgroups else 'blue'
    plt.scatter(subset[main_col], subset[value_col], alpha=0.3, label=subgroup if subgroup in simpsons_paradox_subgroups else "", c=color)

    # Regression line for subgroup
    x_vals = np.array([subset[main_col].min(), subset[main_col].max()])
    y_vals = slope * x_vals + linregress(subset[main_col], subset[value_col]).intercept
    plt.plot(x_vals, y_vals, color=color, linewidth=1)

# Overall regression line
x_vals = np.array([df_prod[main_col].min(), df_prod[main_col].max()])
y_vals = overall_slope * x_vals + overall_intercept
plt.plot(x_vals, y_vals, color='black', linewidth=2, label='Overall Trend')

plt.xlabel("Average Temperature (°C)")
plt.ylabel("Production Value")
plt.title("Simpson's Paradox Detection: Production vs Avg Temp")
plt.legend(loc='upper left', bbox_to_anchor=(1,1), title="Red = Opposite Trend")
plt.tight_layout()
plt.show()

"""## Optional Extra Credit: Simpson's Paradox Analysis by Crop Type

### **Evidence of Simpson's Paradox: YES, but Limited**

I conducted a Simpson's Paradox analysis using **crop type (Item)** as the subgroup variable to determine whether the relationship between temperature and agricultural production differs when examining individual crops versus all crops combined. The analysis focused on the top 15 crops by frequency in the dataset.

---

### **What Did I Investigate?**

- **Subgroup variable:** Crop Type (15 different agricultural products)
- **Independent variable:** Average Temperature (°C)
- **Dependent variable:** Production Value
- **Sample size:** 30,240 observations across 15 crops

**The types analyzed include:**
1. Hen Eggs In Shell, Fresh (3,780 obs)
2. Fruit Primary (1,890 obs)
3. Fat Of Pigs (1,890 obs)
4. Tomatoes (1,890 obs)
5. Various livestock products and primary crops

---

### **The Paradox Revealed: Aggregate vs. Individual Crops**

#### **Overall Trend (All Crops Combined):**
- **Slope: -31,912.60**
- **Direction: NEGATIVE**
- **R²: 0.0066** (very weak relationship)
- **P-value: 3.87×10⁻⁴⁵** (highly significant despite weak correlation)
- **Interpretation:** When all crops are lumped together, there appears to be a negative relationship between temperature and production—suggesting warmer temperatures are associated with lower production values.

#### **Individual Crop Trends:**
- **14 out of 15 crops (93.3%)** show negative slopes consistent with the overall trend
- **Only 1 crop (6.7%)** shows Simpson's Paradox: **Sugar Crops Primary**

---

### **The Paradox Crop: Sugar Crops Primary**

**Sugar Crops Primary is the standout:**
- **Slope: +88,721.69** (POSITIVE, opposite of overall trend!)
- **Difference from overall: 120,634.29** (massive reversal)
- **R²: 0.0176** (still weak but stronger than overall)
- **P-value: 7.04×10⁻⁹** (highly significant)
- **Mean temperature: 21.61°C**
- **Average production: 3,304,859**

**What this means:** While the aggregate data suggests production decreases with temperature, sugar crops actually show the *opposite* pattern—production **increases** with temperature. This is a classic example of Simpson's Paradox where the aggregate relationship masks a completely different relationship within a specific subgroup.

---

### **Interesting Patterns Observed**

#### **1. Weak Overall Relationships Despite Statistical Significance**

Looking at the **heatmap (Image 2)** and **statistical results**, I notice:
- **Overall R² is only 0.0066** (less than 1% of variance explained!)
- Individual crop R² values range from 0.01 to 0.06
- Despite weak correlations, all relationships are statistically significant (p < 0.001)

**Why?** With 30,240 observations, even tiny relationships become statistically significant. The practical significance is questionable.

#### **2. Most Crops Show Consistent Negative Trends**

From the **bar chart (Image 3)**, I can see:
- **Oil crops show the strongest negative relationships:**
  - Oilcrops, Oil Equivalent: -98,073 slope
  - Oilcrops, Cake Equivalent: -83,952 slope
  - Roots And Tubers: -92,489 slope

- **Fruits and vegetables also show negative trends:**
  - Fruit Primary: -74,240
  - Vegetables Primary: -65,851
  - Tomatoes: -51,701

- **Livestock products show smaller negative effects:**
  - Sheep fat, meat, and offal products have slopes near zero
  - This suggests animal products are less temperature-sensitive than crops

#### **3. The Sugar Crop Exception Makes Biological Sense**

The **individual crop plot (Image 1, top-left)** shows Sugar Crops Primary in red with a positive upward trend. This makes sense because:
- **Sugar cane is a tropical/subtropical crop** that thrives in warm climates (20-30°C)
- Higher temperatures (within limits) promote photosynthesis and sugar accumulation
- The crop literally needs heat to produce high yields
- This is consistent with where sugar cane is grown (Brazil, tropical Americas)

Looking at the scatter plot, sugar crop production peaks in the 20-30°C range, exactly where you'd expect for this heat-loving crop.

#### **4. Vertical Banding Pattern Persists**

In the **main paradox detection plot (Image 4)**, I still see:
- **Distinct vertical bands** at specific temperatures
- This confirms that crops are grown in specific climate zones
- Different crops cluster at different temperature ranges
- Limited temperature variation within each crop type

**Temperature clustering by crop:**
- Cooler crops (eggs, some vegetables): cluster around 10-20°C
- Warmer crops (sugar, tropical fruits): cluster around 20-30°C

#### **5. Production Scale Varies Enormously**

The plots show production values ranging from near-zero to 2.8×10⁷, creating:
- **Extreme compression** of the data at the bottom of plots
- **Outliers dominating** the visual space
- Most crops cluster at low production values with a few massive outliers

This extreme skew explains why:
- R² values are so low
- Relationships are hard to see visually
- Log transformation would help (but wasn't applied here)

---

### **Why Is Simpson's Paradox So Limited Here?**

Unlike the country analysis (where 67% showed paradox), only **6.7% of crops** show Simpson's Paradox. Why?

#### **Biological Constraints Are Real:**
- Most crops genuinely do worse in extreme heat
- Heat stress, water loss, and disrupted pollination are real problems
- Only specialized tropical crops (like sugar cane) benefit from high temperatures

#### **Crop Selection Reflects Climate:**
- Farmers don't grow heat-sensitive crops in hot regions
- The data reflects *adapted* agricultural systems
- We're not seeing "what if we moved wheat to the tropics?"—we're seeing where wheat is actually grown

#### **Different Mechanism Than Country Paradox:**
- **Country paradox** was about confounding (rich temperate countries vs. poor tropical countries)
- **Crop paradox** is about genuine biological differences in temperature requirements
- Sugar cane's positive slope is *real*, not a statistical artifact

---

### **Visual Evidence Analysis**

#### **From Individual Crop Plots:**
- **Sugar Crops (top-left, red):** Clear positive trend cutting through the data cloud
- **All other crops (blue):** Negative trends, though very weak
- The **dashed overall line** is nearly flat, showing the aggregate relationship is weak
- Individual crop lines deviate from overall, but mostly in the same negative direction

#### **From Bar Chart:**
- **Sugar Crops Primary stands alone in red** on the right side (positive)
- All 14 other crops cluster on the left (negative slopes)
- The **dashed overall trend line** is slightly negative, pulled down by the majority
- The dramatic visual separation shows Sugar Crops is a true outlier

#### **From Main Paradox Plot:**
- **Red scatter (Sugar Crops)** concentrates in the warmer temperature range (15-30°C)
- **Red regression line** has clear positive slope going upward-right
- **Blue lines** (all other crops) trend downward
- **Black dashed overall line** is nearly horizontal with slight negative slope
- Shows how one outlier crop can create a paradox

---

### **Interpretation: What Does This Mean for My Research Question?**

**"How will climate change impact agricultural food production in the American continent?"**

This analysis reveals **critical nuance**:

#### **1. Not All Crops Respond the Same Way**
- **Temperature-sensitive crops** (vegetables, fruits, oilcrops) show production declines with warming
- **Heat-loving crops** (sugar cane) actually benefit from warmer temperatures
- **Livestock products** show minimal temperature effects

**Implication:** Climate change impacts will be **crop-specific**. Winners and losers depend on what's being grown.

#### **2. The Weak Overall Relationship Is Misleading**
- An R² of 0.0066 means temperature alone explains almost nothing
- Other factors dominate: **rainfall, soil, technology, management practices**
- Temperature is *one factor among many*, not the primary driver

**Implication:** Focusing only on temperature misses the bigger picture. Water availability, extreme events, and adaptive capacity matter more.

#### **3. Adaptation Opportunities Exist**
- If warming occurs, farmers could:
  - Shift to more heat-tolerant crops (expand sugar cane, tropical fruits)
  - Move temperature-sensitive crops to cooler regions or higher elevations
  - Develop heat-resistant varieties through breeding

**Implication:** Agricultural systems can adapt, but this requires planning, investment, and potentially painful transitions for farmers.

#### **4. Geographic Considerations Are Still Critical**
- Sugar cane's positive slope doesn't mean "warming is good everywhere"
- It means "in places where sugar cane thrives (tropics), warmer helps"
- In already-hot regions, even sugar might hit thermal limits

**Implication:** Must still control for country/region to understand local impacts.

---

### **Limitations of This Analysis**

**Important caveats to consider:**

1. **Very weak relationships overall** (R² < 0.02 for all crops)
   - Temperature variation within each crop is minimal
   - We're detecting temporal correlations more than temperature effects

2. **Confounding by time:**
   - Production increased over time (technology, breeding)
   - Temperature also increased over time (climate change)
   - Hard to separate the two effects

3. **Missing nonlinear effects:**
   - These are linear regressions, but temperature effects might be nonlinear
   - There may be optimal ranges with dropoffs at extremes
   - A quadratic model might show different patterns

4. **Aggregated crop categories:**
   - "Fruit Primary" and "Vegetables Primary" lump many species together
   - Individual fruits might show different patterns
   - More granular analysis could reveal more paradoxes

5. **Still not addressing confounders:**
   - Haven't controlled for country, region, or precipitation
   - These factors likely explain more variance than temperature

---

### **Comparison to Country-Based Analysis**

| Aspect | Country Analysis | Crop Type Analysis |
|--------|-----------------|-------------------|
| Subgroups | 30 countries | 15 crops |
| Paradox prevalence | 67% (20/30) | 6.7% (1/15) |
| Overall slope | -24,255 | -31,913 |
| Strongest paradox | Guatemala (+180,525) | Sugar Crops (+88,722) |
| Mechanism | Confounding (rich cold countries vs poor warm ones) | Biological (crop-specific temperature requirements) |
| Implication | Must control for country | Different crops need different climates |

**Key difference:** The country paradox was primarily a **statistical artifact** from confounding. The crop paradox reflects **real biological differences** in crop requirements.

---

### **Conclusion: Limited but Meaningful Paradox**

**Summary of findings:**

 **Simpson's Paradox detected:** Yes, but only in 1 out of 15 crops (6.7%)

 **The paradox is meaningful:** Sugar Crops' positive slope reflects genuine biological reality, not a statistical quirk

 **The paradox teaches important lessons:**
- Aggregate relationships mask important crop-specific patterns
- Not all agricultural products respond to temperature the same way
- Climate change will create winners (tropical crops) and losers (temperate crops)
- Adaptation strategies must be crop-specific and region-specific



**Final thought:** While Simpson's Paradox is less prevalent when comparing crops than when comparing countries, its presence for sugar crops reveals an important truth: **climate change impacts on agriculture are not uniform**. Some crops and regions will benefit while others suffer. Understanding these differences—rather than relying on aggregate trends—is essential for developing effective adaptation strategies and ensuring food security in a changing climate.


"""

# SIMPSON'S PARADOX ANALYSIS: BY CROP TYPE (ITEM)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import linregress

# Set style for better-looking plots
sns.set(style="whitegrid", palette="muted", font_scale=1.1)


print("SIMPSON'S PARADOX ANALYSIS: PRODUCTION VS TEMPERATURE BY CROP TYPE")

print()


# STEP 1: PREPARE THE DATA


# Filter for Production only
df_prod = df[df["Element"] == "Production"].copy()

# Get top 15 crops by frequency (to keep visualization manageable)
print(" Identifying top crops by frequency...")
top_crops = df_prod['Item'].value_counts().head(15).index.tolist()

print(f"\nTop 15 crops selected:")
for i, crop in enumerate(top_crops, 1):
    count = len(df_prod[df_prod['Item'] == crop])
    print(f"  {i:2d}. {crop:40s} ({count:,} observations)")

# Filter dataset to only include top crops
df_crops = df_prod[df_prod['Item'].isin(top_crops)].copy()

print(f"\nFiltered dataset: {len(df_crops):,} observations")
print()

# STEP 2: CALCULATE OVERALL TREND (ALL CROPS COMBINED)



print("OVERALL TREND ANALYSIS (All Crops Combined)")


main_col = "avg_temp_c"
value_col = "value"

# Calculate overall regression
overall_slope, overall_intercept, overall_r, overall_p, overall_stderr = linregress(
    df_crops[main_col], df_crops[value_col]
)

print(f"\n Overall Regression Results:")
print(f"   Slope:              {overall_slope:,.2f}")
print(f"   Intercept:          {overall_intercept:,.2f}")
print(f"   R-squared:          {overall_r**2:.4f}")
print(f"   P-value:            {overall_p:.4e}")
print(f"   Relationship:       {'Significant' if overall_p < 0.05 else 'Not significant'} at α=0.05")

if overall_slope > 0:
    print(f"   Direction:          POSITIVE (production increases with temperature)")
elif overall_slope < 0:
    print(f"   Direction:          NEGATIVE (production decreases with temperature)")
else:
    print(f"   Direction:          NEUTRAL (no relationship)")

print()


# STEP 3: CALCULATE INDIVIDUAL CROP TRENDS



print("INDIVIDUAL CROP TREND ANALYSIS")

print()

crop_slopes = {}
crop_stats = {}
simpsons_paradox_crops = []

for crop in top_crops:
    subset = df_crops[df_crops['Item'] == crop]

    # Skip if too few observations
    if len(subset) < 30:
        print(f" Skipping {crop}: only {len(subset)} observations")
        continue

    # Calculate regression for this crop
    slope, intercept, r_value, p_value, std_err = linregress(
        subset[main_col], subset[value_col]
    )

    # Store results
    crop_slopes[crop] = slope
    crop_stats[crop] = {
        'slope': slope,
        'intercept': intercept,
        'r_squared': r_value**2,
        'p_value': p_value,
        'n_obs': len(subset),
        'mean_temp': subset[main_col].mean(),
        'mean_production': subset[value_col].mean()
    }

    # Check for Simpson's Paradox (opposite sign from overall trend)
    if overall_slope * slope < 0:
        simpsons_paradox_crops.append(crop)


# STEP 4: DISPLAY DETAILED RESULTS

print(f" Successfully analyzed {len(crop_slopes)} crops\n")

# Sort crops by slope magnitude to show strongest opposite trends
slope_diff = {
    crop: abs(overall_slope - slope)
    for crop, slope in crop_slopes.items()
    if crop in simpsons_paradox_crops
}
slope_diff_sorted = dict(sorted(slope_diff.items(), key=lambda item: item[1], reverse=True))


print(f"SIMPSON'S PARADOX DETECTION RESULTS")

print()
print(f" Paradox Summary:")
print(f"   Overall slope:                 {overall_slope:,.2f}")
print(f"   Crops showing OPPOSITE trend:  {len(simpsons_paradox_crops)} out of {len(crop_slopes)}")
print(f"   Paradox prevalence:            {len(simpsons_paradox_crops)/len(crop_slopes)*100:.1f}%")
print()

if len(simpsons_paradox_crops) > 0:
    print(" CROPS WITH STRONGEST SIMPSON'S PARADOX (opposite slope):")

    print(f"{'Rank':<6} {'Crop':<40} {'Slope':>15} {'Diff from Overall':>20}")


    for rank, (crop, diff) in enumerate(slope_diff_sorted.items(), 1):
        slope = crop_slopes[crop]
        print(f"{rank:<6} {crop:<40} {slope:>15,.2f} {diff:>20,.2f}")

    print()

    # Detailed statistics for paradox crops
    print("\n DETAILED STATISTICS FOR PARADOX CROPS:")

    for crop in slope_diff_sorted.keys():
        stats = crop_stats[crop]
        print(f"\n{crop}:")
        print(f"   Slope:              {stats['slope']:>15,.2f}")
        print(f"   R²:                 {stats['r_squared']:>15.4f}")
        print(f"   P-value:            {stats['p_value']:>15.4e}")
        print(f"   Significance:       {'Yes' if stats['p_value'] < 0.05 else 'No'} (α=0.05)")
        print(f"   Observations:       {stats['n_obs']:>15,}")
        print(f"   Avg Temperature:    {stats['mean_temp']:>15.2f}°C")
        print(f"   Avg Production:     {stats['mean_production']:>15,.0f}")
else:
    print(" No Simpson's Paradox detected - all crops show consistent direction with overall trend")

print()

# Also show crops consistent with overall trend
consistent_crops = [crop for crop in crop_slopes.keys() if crop not in simpsons_paradox_crops]
if len(consistent_crops) > 0:

    print(f"CROPS CONSISTENT WITH OVERALL TREND ({len(consistent_crops)} crops):")

    print(f"{'Crop':<40} {'Slope':>15} {'R²':>10} {'P-value':>12}")

    for crop in consistent_crops:
        stats = crop_stats[crop]
        print(f"{crop:<40} {stats['slope']:>15,.2f} {stats['r_squared']:>10.4f} {stats['p_value']:>12.4e}")

print()
print()


# STEP 5: VISUALIZATION 1 - MAIN SIMPSON'S PARADOX PLOT



print("GENERATING VISUALIZATIONS...")

print()

plt.figure(figsize=(16, 9))

# Create color palette
paradox_colors = sns.color_palette("Reds_d", n_colors=len(simpsons_paradox_crops))
consistent_colors = sns.color_palette("Blues_d", n_colors=len(consistent_crops))

# Plot each crop
crop_color_map = {}
legend_elements = []

# Plot paradox crops (RED)
for i, crop in enumerate(simpsons_paradox_crops):
    subset = df_crops[df_crops['Item'] == crop]
    color = paradox_colors[i]
    crop_color_map[crop] = color

    # Scatter plot
    plt.scatter(subset[main_col], subset[value_col],
               alpha=0.4, s=20, color=color, edgecolors='none')

    # Regression line
    stats = crop_stats[crop]
    x_min, x_max = subset[main_col].min(), subset[main_col].max()
    x_vals = np.array([x_min, x_max])
    y_vals = stats['slope'] * x_vals + stats['intercept']
    line, = plt.plot(x_vals, y_vals, color=color, linewidth=2.5, alpha=0.8)

    # Add to legend (only for paradox crops)
    legend_elements.append((line, crop))

# Plot consistent crops (BLUE) - without labels to reduce clutter
for i, crop in enumerate(consistent_crops):
    subset = df_crops[df_crops['Item'] == crop]
    color = consistent_colors[i] if i < len(consistent_colors) else 'blue'
    crop_color_map[crop] = color

    # Scatter plot
    plt.scatter(subset[main_col], subset[value_col],
               alpha=0.3, s=15, color=color, edgecolors='none')

    # Regression line
    stats = crop_stats[crop]
    x_min, x_max = subset[main_col].min(), subset[main_col].max()
    x_vals = np.array([x_min, x_max])
    y_vals = stats['slope'] * x_vals + stats['intercept']
    plt.plot(x_vals, y_vals, color=color, linewidth=1.5, alpha=0.6)

# Plot overall regression line (THICK BLACK)
x_min, x_max = df_crops[main_col].min(), df_crops[main_col].max()
x_vals = np.array([x_min, x_max])
y_vals = overall_slope * x_vals + overall_intercept
overall_line, = plt.plot(x_vals, y_vals, color='black', linewidth=4,
                          label=f'Overall Trend (slope={overall_slope:,.0f})',
                          linestyle='--', alpha=0.9)

# Formatting
plt.xlabel("Average Temperature (°C)", fontsize=14, fontweight='bold')
plt.ylabel("Production Value", fontsize=14, fontweight='bold')
plt.title("Simpson's Paradox Detection: Production vs Temperature by Crop Type\n" +
          f"Red = Opposite Trend ({len(simpsons_paradox_crops)} crops) | Blue = Consistent Trend ({len(consistent_crops)} crops)",
          fontsize=16, fontweight='bold', pad=20)

# Create legend
legend_handles = [overall_line] + [elem[0] for elem in legend_elements]
legend_labels = [overall_line.get_label()] + [elem[1] for elem in legend_elements]

plt.legend(legend_handles, legend_labels,
          loc='upper left', bbox_to_anchor=(1.01, 1),
          title="Red = Paradox Crops",
          fontsize=10, title_fontsize=11, frameon=True, shadow=True)

plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(" Visualization 1 complete: Main Simpson's Paradox plot")
print()


# STEP 6: VISUALIZATION 2 - SLOPE COMPARISON BAR CHART


plt.figure(figsize=(14, 8))

# Prepare data for bar chart
crops_sorted = sorted(crop_slopes.keys(), key=lambda x: crop_slopes[x], reverse=True)
slopes_sorted = [crop_slopes[crop] for crop in crops_sorted]
colors_sorted = ['red' if crop in simpsons_paradox_crops else 'blue' for crop in crops_sorted]

# Create bar chart
bars = plt.barh(range(len(crops_sorted)), slopes_sorted, color=colors_sorted, alpha=0.7, edgecolor='black')

# Add overall slope line
plt.axvline(x=overall_slope, color='black', linewidth=3, linestyle='--',
           label=f'Overall Slope ({overall_slope:,.0f})', alpha=0.8)

# Formatting
plt.yticks(range(len(crops_sorted)), crops_sorted, fontsize=11)
plt.xlabel("Slope (Change in Production per °C)", fontsize=13, fontweight='bold')
plt.ylabel("Crop Type", fontsize=13, fontweight='bold')
plt.title("Individual Crop Slopes vs Overall Trend\n" +
         f"Simpson's Paradox: {len(simpsons_paradox_crops)}/{len(crop_slopes)} crops show opposite trend",
         fontsize=15, fontweight='bold', pad=15)

# Add value labels on bars
for i, (crop, slope) in enumerate(zip(crops_sorted, slopes_sorted)):
    if slope > 0:
        ha = 'left'
        x_pos = slope + abs(max(slopes_sorted) - min(slopes_sorted)) * 0.02
    else:
        ha = 'right'
        x_pos = slope - abs(max(slopes_sorted) - min(slopes_sorted)) * 0.02

    plt.text(x_pos, i, f'{slope:,.0f}', va='center', ha=ha, fontsize=9, fontweight='bold')

# Legend
from matplotlib.patches import Patch
legend_elements = [
    Patch(facecolor='red', alpha=0.7, label=f'Paradox Crops (n={len(simpsons_paradox_crops)})'),
    Patch(facecolor='blue', alpha=0.7, label=f'Consistent Crops (n={len(consistent_crops)})'),
    plt.Line2D([0], [0], color='black', linewidth=3, linestyle='--', label='Overall Trend')
]
plt.legend(handles=legend_elements, loc='lower right', fontsize=11, frameon=True, shadow=True)

plt.grid(True, axis='x', alpha=0.3)
plt.tight_layout()
plt.show()

print(" Visualization 2 complete: Slope comparison bar chart")
print()


# STEP 7: VISUALIZATION 3 - HEATMAP OF CROP STATISTICS


plt.figure(figsize=(12, 8))

# Prepare data for heatmap
heatmap_data = []
heatmap_crops = []

for crop in crops_sorted:
    stats = crop_stats[crop]
    heatmap_data.append([
        stats['slope'] / 1000,  # Scale down for better visualization
        stats['r_squared'],
        -np.log10(stats['p_value']) if stats['p_value'] > 0 else 10,  # -log10(p) for significance
        stats['mean_temp'],
        stats['mean_production'] / 10000  # Scale down
    ])
    heatmap_crops.append(crop)

heatmap_df = pd.DataFrame(
    heatmap_data,
    index=heatmap_crops,
    columns=['Slope\n(×1000)', 'R²', '-log10(p)\n(Significance)',
             'Mean Temp\n(°C)', 'Mean Prod\n(×10k)']
)

# Create heatmap
sns.heatmap(heatmap_df, annot=True, fmt='.2f', cmap='RdYlGn', center=0,
           cbar_kws={'label': 'Standardized Value'}, linewidths=0.5, linecolor='gray')

plt.title("Crop Statistics Heatmap\nHigher values = Greener | Lower values = Redder",
         fontsize=14, fontweight='bold', pad=15)
plt.ylabel("Crop Type", fontsize=12, fontweight='bold')
plt.xlabel("Statistic", fontsize=12, fontweight='bold')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

print(" Visualization 3 complete: Crop statistics heatmap")
print()


# STEP 8: VISUALIZATION 4 - SCATTER PLOT WITH TREND ANNOTATIONS


fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.flatten()

# Plot top 6 paradox crops individually (if we have them)
crops_to_plot = simpsons_paradox_crops[:6] if len(simpsons_paradox_crops) >= 6 else simpsons_paradox_crops + consistent_crops[:6-len(simpsons_paradox_crops)]

for idx, crop in enumerate(crops_to_plot):
    ax = axes[idx]
    subset = df_crops[df_crops['Item'] == crop]
    stats = crop_stats[crop]

    # Scatter plot
    color = 'red' if crop in simpsons_paradox_crops else 'blue'
    ax.scatter(subset[main_col], subset[value_col], alpha=0.5, s=30, color=color)

    # Regression line
    x_min, x_max = subset[main_col].min(), subset[main_col].max()
    x_vals = np.linspace(x_min, x_max, 100)
    y_vals = stats['slope'] * x_vals + stats['intercept']
    ax.plot(x_vals, y_vals, color='darkred' if crop in simpsons_paradox_crops else 'darkblue',
           linewidth=3, label=f"Slope={stats['slope']:,.0f}")

    # Overall trend line for comparison
    y_overall = overall_slope * x_vals + overall_intercept
    ax.plot(x_vals, y_overall, color='black', linewidth=2, linestyle='--',
           alpha=0.5, label=f"Overall={overall_slope:,.0f}")

    # Formatting - FIXED LINE
    ax.set_xlabel("Avg Temperature (°C)", fontsize=10)
    ax.set_ylabel("Production Value", fontsize=10)

    paradox_label = 'PARADOX' if crop in simpsons_paradox_crops else 'CONSISTENT'
    p_label = '<0.001' if stats['p_value'] < 0.001 else f"={stats['p_value']:.3f}"
    title_color = 'darkred' if crop in simpsons_paradox_crops else 'darkblue'

    ax.set_title(f"{crop}\n{paradox_label} | R²={stats['r_squared']:.3f} | p{p_label}",
                fontsize=11, fontweight='bold', color=title_color)
    ax.legend(loc='best', fontsize=9)
    ax.grid(True, alpha=0.3)

# Remove extra subplots if we have fewer than 6 crops
for idx in range(len(crops_to_plot), 6):
    fig.delaxes(axes[idx])

plt.suptitle("Individual Crop Analysis: Temperature vs Production\n" +
            "Red = Simpson's Paradox | Blue = Consistent with Overall Trend",
            fontsize=16, fontweight='bold', y=0.995)
plt.tight_layout()
plt.show()

print(" Visualization 4 complete: Individual crop detailed plots")
print()


# STEP 9: SUMMARY STATISTICS TABLE



print("FINAL SUMMARY")

print()

summary_stats = {
    'Total crops analyzed': len(crop_slopes),
    'Crops showing paradox': len(simpsons_paradox_crops),
    'Crops consistent with overall': len(consistent_crops),
    'Paradox prevalence (%)': len(simpsons_paradox_crops)/len(crop_slopes)*100,
    'Overall slope': overall_slope,
    'Overall R²': overall_r**2,
    'Overall p-value': overall_p,
    'Strongest paradox crop': list(slope_diff_sorted.keys())[0] if slope_diff_sorted else 'None',
    'Strongest paradox magnitude': list(slope_diff_sorted.values())[0] if slope_diff_sorted else 0
}

for key, value in summary_stats.items():
    if isinstance(value, float):
        if 'p-value' in key:
            print(f"{key:.<45} {value:.4e}")
        elif '%' in key:
            print(f"{key:.<45} {value:.1f}%")
        else:
            print(f"{key:.<45} {value:,.2f}")
    elif isinstance(value, int):
        print(f"{key:.<45} {value:,}")
    else:
        print(f"{key:.<45} {value}")

print()
print("ANALYSIS COMPLETE!")

"""## Optional Extra Credit: Simpson's Paradox Analysis by Region

### **Evidence of Simpson's Paradox: YES, Strong and Widespread**

I conducted a Simpson's Paradox analysis using **geographic region** as the subgroup variable to examine whether the relationship between temperature and agricultural production differs across the major regions of the Americas. This analysis revealed a **dramatic example of Simpson's Paradox** with profound implications for understanding climate impacts on agriculture.

---

### **What Did I Investigate?**

- **Subgroup variable:** Geographic Region (4 major regions in the Americas)
- **Independent variable:** Average Temperature (°C)
- **Dependent variable:** Production Value
- **Sample size:** 220,626 observations across 30 countries grouped into 4 regions

**The regions analyzed:**
1. **Caribbean** (10 countries, 57,771 observations)
2. **Central America** (6 countries, 41,832 observations)
3. **North America** (3 countries, 32,130 observations)
4. **South America** (11 countries, 88,893 observations)

---

### **The Paradox Revealed: A Stunning Reversal**

#### **Overall Trend (All Regions Combined):**
- **Slope: -24,254.90**
- **Direction: NEGATIVE**
- **R²: 0.0069** (very weak relationship)
- **P-value: ~0** (highly significant)
- **Interpretation:** When aggregating all regions together, there appears to be a negative relationship—warmer temperatures are associated with lower agricultural production across the Americas.

#### **Individual Region Trends:**
**THREE out of FOUR regions (75%) show the OPPOSITE pattern!**

**Regions showing Simpson's Paradox (positive slopes):**

1. **Central America**
   - Slope: +38,430.13
   - Difference from overall: 62,685.04 (strongest paradox!)
   - R²: 0.0018, p < 0.001
   - Mean temperature: 26.78°C (warmest region)

2. **South America**
   - Slope: +12,610.99
   - Difference from overall: 36,865.89
   - R²: 0.0008, p < 0.001
   - Mean temperature: 21.01°C

3. **Caribbean**
   - Slope: +5,517.65
   - Difference from overall: 29,772.55
   - R²: 0.0064, p < 0.001
   - Mean temperature: 21.80°C

**Only ONE region consistent with overall trend:**

4. **North America**
   - Slope: -17,816.25 (negative, like overall)
   - R²: 0.0010, p < 0.001
   - Mean temperature: 13.67°C (coldest region)

---

### **Interesting Patterns Observed**

#### **1. The Temperature Gradient Tells a Story**

From the **Temperature Distribution violin plot (Image 1)**, I observe fascinating regional climate patterns:

**North America (coldest):**
- **Trimodal distribution** with peaks around 3°C, 12°C, and 22°C
- Wide temperature range (-5°C to 23°C)
- Reflects three distinct climate zones: Canada (cold), northern USA (temperate), Mexico (warm)
- Shows **negative slope**: production decreases with warming

**South America (most variable):**
- **Widest temperature spread** (11°C to 30°C)
- Multiple peaks showing diverse climates from Patagonia to Amazon
- Includes cold Chile/Argentina, temperate Uruguay, tropical Brazil
- Shows **positive slope**: production increases with warming

**Caribbean (concentrated warm):**
- **Narrow distribution** clustered around 27°C
- Small peak around 12°C (likely mountain regions)
- Very consistent tropical climate
- Shows **positive slope** despite limited temperature variation

**Central America (uniformly tropical):**
- **Extremely tight distribution** around 27°C
- Most homogeneous climate of all regions
- Shows **strongest positive slope** (+38,430!)

**Key insight:** Regions with warmer base climates show positive temperature-production relationships, while the cold region shows negative relationships.

---

#### **2. The Geographic Pattern of the Paradox**

From the **main paradox plot** and **bar chart**, a clear geographic gradient emerges:

**Temperature zones and their responses:**
- **Cold zone (North America, 13.67°C avg):** NEGATIVE slope (-17,816)
  - Suggests these temperate/cold agricultural systems may be near optimal and warming hurts
  
- **Warm zone (Caribbean/South America, ~21-22°C):** POSITIVE slopes (+5,518 to +12,611)
  - Moderate warming appears beneficial in these regions
  
- **Hot zone (Central America, 26.78°C):** STRONGEST positive slope (+38,430)
  - Even in the hottest region, warming correlates with increased production!

**This creates a paradoxical geographic pattern:**
- Aggregate data dominated by North America's negative trend
- But 75% of regions (representing diverse tropical/subtropical agriculture) show opposite positive trends


---

#### **3. Visual Evidence from Individual Region Plots (Image 2)**

**Caribbean (top-left, purple):**
- Vertical clustering at ~27°C with slight positive upward trend
- Most data compressed at low production values
- A few extreme outliers reaching 2×10⁷
- Positive slope despite very narrow temperature range

**Central America (top-middle, orange):**
- VERY tight clustering at 26-28°C
- Horizontal spread minimal, mostly at one temperature
- Clear positive slope visible even in this constrained range
- Some high-value outliers at 2.5×10⁷

**North America (top-right, red):**
- Dramatic vertical banding at specific temperatures (cold Canada, temperate USA, warm Mexico)
- Shows clear negative slope across the full range
- Massive data density around 15°C (USA)
- Only region consistent with overall trend

**South America (bottom-left, blue):**
- Widest temperature spread (10-30°C)
- Three distinct vertical bands reflecting different countries
- Positive slope visible across diverse climates
- High production values throughout temperature range

---

#### **4. The Heatmap Reveals Critical Context (Image 3)**

**Statistical significance (-log10(p) column):**
- **Caribbean: 81.88** (dark green = extremely significant!)
- Central America: 17.56
- South America: 15.66
- North America: 7.76

Despite low R² values, the Caribbean's paradox is the most statistically robust due to large sample size.

**Temperature variability (Temp StdDev column):**
- **Caribbean: 9.19°C** (highest variability despite tropical location)
- North America: 6.62°C
- South America: 4.77°C
- **Central America: 1.04°C** (extremely homogeneous!)

This explains why Central America shows strong positive slope despite narrow range—the temporal warming trend over 60+ years drives the correlation.

**Production scale (Mean Prod column):**
- **North America: 1,383,000** (by far the largest, shown in dark green)
- South America: 506,066
- Caribbean: 91,250
- Central America: 154,039

**THIS IS THE KEY TO THE PARADOX:** North America's massive production scale dominates the aggregate statistics, pulling the overall slope negative even though it represents only 1 of 4 regions!

---

### **Why Does This Paradox Occur? (The Confounding Story)**

The Simpson's Paradox arises from **three interacting factors:**

#### **1. Different Baseline Climates**
- **Cold baseline (North America):** Already near optimal for temperate crops (wheat, corn), warming moves away from optimum
- **Warm baselines (others):** Room for benefit from additional warming, or temporal correlation with agricultural development

#### **2. Massive Production Differences**
North America produces **2.5× more** than the next largest region (South America). When we aggregate:
- North America's negative trend has **disproportionate weight**
- Three regions with positive trends get **statistically overwhelmed**
- The aggregate tells North America's story, not the Americas' story

**The math:**
```
Overall slope ≈ (North America weight × -17,816) + (Others weight × positive values)
            ≈ Large negative contribution + Smaller positive contribution
            = Net negative (-24,255)
```

#### **3. Confounding by Agricultural Development**
Looking at the temporal dimension:
- **1961-2023:** Production increased across most regions (technology, better practices)
- **1961-2023:** Temperatures increased globally (climate change)
- **Within warm regions:** Production growth coincided with warming → positive slopes
- **Within cold region:** Different dynamics (possibly hit temperature limits) → negative slope

---

### **Interpretation: What Does This Mean for My Research Question?**

**"How will climate change impact agricultural food production in the American continent?"**

This regional paradox analysis reveals that **the answer depends entirely on WHERE you look:**

#### **1. No Single Answer Exists**
If I relied only on the aggregate analysis (slope = -24,255), I would conclude:
> "Climate warming will reduce agricultural production across the Americas."

**But this is WRONG for 75% of regions!** The paradox shows:
- **Central America:** Strong positive relationship (+38,430)
- **South America:** Positive relationship (+12,611)
- **Caribbean:** Positive relationship (+5,518)
- **North America:** Negative relationship (-17,816)

#### **2. The Cold-Warm Divide**
The paradox reveals a **fundamental climatic divide:**

**Temperate/Cold regions (North America):**
- Negative temperature effects
- May be near optimal temperatures for current crops
- Warming could reduce yields or require crop shifts
- Represents high-production industrial agriculture

**Tropical/Subtropical regions (Central/South America, Caribbean):**
- Positive temperature correlations
- Either benefit from warming or show spurious correlation with development
- Represent diverse agricultural systems, often less industrialized
- More vulnerable to other climate factors (droughts, hurricanes)

#### **3. Scale Matters More Than Geography**
The paradox teaches a sobering lesson about **power dynamics in agriculture:**
- North America's agricultural juggernaut (USA, Canada) **dominates** aggregate statistics
- Smaller producers' experiences get **statistically erased**
- Policy makers relying on aggregate data miss regional realities

**Implication:** Climate adaptation strategies MUST be region-specific, not continent-wide.

#### **4. Correlation Still Doesn't Equal Causation**
Even with the paradox identified, interpretation is complex:

**Positive slopes in warm regions could mean:**
-  Warming genuinely helps within tropical agricultural systems
-  Temporal correlation: both production and temperature increased over time
-  Adaptation: farmers successfully adapted to gradual warming
-  May not predict future: hitting thermal limits soon

**Negative slope in cold region could mean:**
-  Temperate crops hitting upper temperature thresholds
-  Extreme weather events (not captured in average temperature)
-  Could reverse if farmers shift to heat-tolerant crops

---

### **Comparison: Region vs. Crop Type Analyses**

| Aspect | Crop Type Analysis | Region Analysis |
|--------|-------------------|-----------------|
| Subgroups | 15 crops | 4 regions |
| Paradox prevalence | 6.7% (1/15) | **75.0% (3/4)** |
| Overall slope | -31,913 | -24,255 |
| Strongest paradox | Sugar Crops (+88,722) | Central America (+38,430) |
| Mechanism | Biological (crop requirements) | **Geographic + scale (production concentration)** |
| R² values | 0.01-0.06 | 0.001-0.006 (even weaker!) |
| Key finding | Most crops hurt by heat, except sugar | Most regions show positive, but North America dominates aggregate |

**The region analysis reveals MORE dramatic paradox:**
- Higher paradox prevalence (75% vs. 7%)
- Affects majority of geographic area
- Shows importance of production scale in masking patterns

---

### **Limitations and Caveats**

#### **1. Extremely Weak Relationships (R² < 0.01)**
- Temperature explains less than 1% of production variance in ALL regions
- These are essentially **noise** masquerading as patterns
- Statistical significance ≠ practical significance with 220k observations

**Interpretation:** Temperature alone is NOT a meaningful predictor of production. Other factors (rainfall, technology, economics, policies) matter far more.

#### **2. Temporal Confounding Remains Unresolved**
- Still haven't separated "warming effect" from "time trend effect"
- 1961-2023: Technology improved everywhere + temperatures rose everywhere
- Can't distinguish causation from correlation

**What we're really seeing:** How production changed over 62 years while temperatures happened to increase, not necessarily how temperature changes CAUSED production changes.

#### **3. Within-Region Temperature Variation is Minimal**
From the violin plots:
- Each region occupies a specific temperature niche
- Limited overlap between regions
- We're comparing BETWEEN regions more than analyzing WITHIN regions

**The slopes reflect:**
- Temporal trends within each region (production over time)
- NOT spatial gradients (what would happen if we moved farms to different temperatures)

#### **4. Aggregate Regions Hide Country-Level Diversity**
- "South America" includes cold Chile and tropical Brazil
- "Caribbean" spans microstates and diverse islands
- Country-level paradoxes may exist within regional paradoxes!

---

### **Visual Evidence Summary**

**From Bar Chart:**
The stark visual contrast is immediate:
- **Central America's orange bar** extends far right (massive positive)
- **North America's red bar** extends left (negative)
- **Three hatched bars** (paradox regions) all point right
- **One solid bar** (consistent) points left
- Black dashed line (overall) slightly negative, positioned between extremes

**Visual metaphor:** The aggregate is the "compromise" between one large negative force and three smaller positive forces.

**From Main Paradox Plot:**
- **Color clustering by temperature:** North America (red) dominates cold zones, others fill warm zones
- **Solid vs. dashed lines:** Three regions show positive slopes (solid, paradox), one shows negative (dashed, consistent)
- **Vertical banding:** Each region occupies distinct temperature niches
- **Overall black dashed line:** Nearly horizontal, slightly negative, cuts through the middle

**Key visual insight:** The paradox is GEOGRAPHIC—it's not scattered individuals, but entire regions showing opposite patterns.

---

### **Conclusion: Yes, Profound Simpson's Paradox**

**Summary of findings:**

 **Simpson's Paradox strongly present:** 75% of regions (3/4) show opposite trends from aggregate

 **The paradox is geographically structured:**
- Cold region: Negative relationship
- Warm regions: ALL show positive relationships
- Creates coherent geographic pattern

 **The mechanism is clear:**
- **Production scale confounding:** North America's massive output dominates aggregate statistics
- **Different baseline climates:** Cold vs. warm regions respond differently
- **Temporal correlation:** Production growth coincides with warming over 60 years

 **The paradox has major implications:**
- **Cannot use aggregate data** to predict regional impacts
- **Climate adaptation must be region-specific**
- **Small producers' experiences hidden** by large producers' statistics
- **Geographic equity issue:** Tropical regions' positive experiences erased in aggregate

 **But interpretation requires extreme caution:**
- **R² < 0.01 everywhere:** Temperature explains almost nothing
- **Correlation ≠ causation:** Temporal confounding unresolved
- **Within-region variation minimal:** Detecting time trends, not temperature effects
- **May not predict future:** Past patterns may not continue under accelerating change

---

### **Final Insight: The Geographic Justice Dimension**

This Simpson's Paradox reveals something profound about **whose story gets told:**

**Aggregate statistics privilege the powerful:**
- North America (USA + Canada) = 15% of observations but determines aggregate trend
- Central + South America + Caribbean = 85% of observations but voices suppressed
- The "Americas' story" is really "North America's story"

**Climate change impacts are:**
- **Experienced differently** by different regions
- **Reported differently** depending on who does the counting
- **Understood differently** if we look at aggregate vs. disaggregate data

**For food security planning:** We MUST analyze regions separately. Policies based on aggregate trends will fail most of the continent while potentially helping only the already-powerful North American agricultural sector.






"""

# SIMPSON'S PARADOX ANALYSIS: BY REGION


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import linregress

# Set style for better-looking plots
sns.set(style="whitegrid", palette="muted", font_scale=1.1)

print("SIMPSON'S PARADOX ANALYSIS: PRODUCTION VS TEMPERATURE BY REGION")
print()


# STEP 1: PREPARE THE DATA & CREATE REGIONAL GROUPINGS


# Filter for Production only
df_prod = df[df["Element"] == "Production"].copy()

print("Creating regional groupings...")
print()

# Define region mapping for all countries in the Americas
region_mapping = {
    # North America
    'Canada': 'North America',
    'United States Of America': 'North America',
    'Mexico': 'North America',

    # Central America
    'Guatemala': 'Central America',
    'Belize': 'Central America',
    'El Salvador': 'Central America',
    'Honduras': 'Central America',
    'Nicaragua': 'Central America',
    'Costa Rica': 'Central America',
    'Panama': 'Central America',

    # Caribbean
    'Cuba': 'Caribbean',
    'Haiti': 'Caribbean',
    'Dominican Republic': 'Caribbean',
    'Jamaica': 'Caribbean',
    'Trinidad And Tobago': 'Caribbean',
    'Puerto Rico': 'Caribbean',
    'Bahamas': 'Caribbean',
    'Barbados': 'Caribbean',
    'Saint Lucia': 'Caribbean',
    'Grenada': 'Caribbean',
    'Saint Vincent And The Grenadines': 'Caribbean',
    'Antigua And Barbuda': 'Caribbean',
    'Dominica': 'Caribbean',
    'Saint Kitts And Nevis': 'Caribbean',

    # South America
    'Colombia': 'South America',
    'Venezuela': 'South America',
    'Guyana': 'South America',
    'Suriname': 'South America',
    'French Guiana': 'South America',
    'Ecuador': 'South America',
    'Peru': 'South America',
    'Brazil': 'South America',
    'Bolivia': 'South America',
    'Paraguay': 'South America',
    'Chile': 'South America',
    'Argentina': 'South America',
    'Uruguay': 'South America'
}

# Apply region mapping
df_prod['region'] = df_prod['country_name'].map(region_mapping)

# Check for unmapped countries
unmapped = df_prod[df_prod['region'].isna()]['country_name'].unique()
if len(unmapped) > 0:
    print(f"Warning: {len(unmapped)} countries not mapped to regions:")
    for country in unmapped:
        print(f"   - {country}")
    print()

# Remove rows without region mapping
df_regions = df_prod[df_prod['region'].notna()].copy()

# Display region statistics
print("Regional Distribution:")

region_counts = df_regions['region'].value_counts().sort_index()
for region in region_counts.index:
    count = region_counts[region]
    n_countries = len(df_regions[df_regions['region']==region]['country_name'].unique())
    print(f"   {region:<20s} {count:>8,} observations  ({n_countries:>2} countries)")

print(f"\n   {'TOTAL':<20s} {len(df_regions):>8,} observations")
print()


# STEP 2: CALCULATE OVERALL TREND (ALL REGIONS COMBINED)


print("OVERALL TREND ANALYSIS (All Regions Combined)")
print()

main_col = "avg_temp_c"
value_col = "value"

# Calculate overall regression
overall_slope, overall_intercept, overall_r, overall_p, overall_stderr = linregress(
    df_regions[main_col], df_regions[value_col]
)

print(f"\nOverall Regression Results:")
print(f"   Slope:              {overall_slope:,.2f}")
print(f"   Intercept:          {overall_intercept:,.2f}")
print(f"   R-squared:          {overall_r**2:.4f}")
print(f"   P-value:            {overall_p:.4e}")
print(f"   Relationship:       {'Significant' if overall_p < 0.05 else 'Not significant'} at α=0.05")

if overall_slope > 0:
    print(f"   Direction:          POSITIVE (production increases with temperature)")
elif overall_slope < 0:
    print(f"   Direction:          NEGATIVE (production decreases with temperature)")
else:
    print(f"   Direction:          NEUTRAL (no relationship)")

print()


# STEP 3: CALCULATE INDIVIDUAL REGION TRENDS


print("INDIVIDUAL REGION TREND ANALYSIS")
print()

region_slopes = {}
region_stats = {}
simpsons_paradox_regions = []

regions = df_regions['region'].unique()

for region in sorted(regions):
    subset = df_regions[df_regions['region'] == region]

    print(f"Analyzing {region}...")
    print(f"   Sample size: {len(subset):,} observations")

    # Skip if too few observations
    if len(subset) < 30:
        print(f"   Skipping: only {len(subset)} observations")
        print()
        continue

    # Calculate regression for this region
    slope, intercept, r_value, p_value, std_err = linregress(
        subset[main_col], subset[value_col]
    )

    # Store results
    region_slopes[region] = slope
    region_stats[region] = {
        'slope': slope,
        'intercept': intercept,
        'r_squared': r_value**2,
        'p_value': p_value,
        'n_obs': len(subset),
        'n_countries': len(subset['country_name'].unique()),
        'mean_temp': subset[main_col].mean(),
        'std_temp': subset[main_col].std(),
        'mean_production': subset[value_col].mean(),
        'median_production': subset[value_col].median()
    }

    print(f"   Slope: {slope:,.2f}")
    print(f"   R²: {r_value**2:.4f}")
    print(f"   P-value: {p_value:.4e}")
    print(f"   Mean temperature: {subset[main_col].mean():.2f}°C")
    print()

    # Check for Simpson's Paradox (opposite sign from overall trend)
    if overall_slope * slope < 0:
        simpsons_paradox_regions.append(region)

# STEP 4: DISPLAY DETAILED RESULTS


print(f"Successfully analyzed {len(region_slopes)} regions\n")

# Sort regions by slope magnitude to show strongest opposite trends
slope_diff = {
    region: abs(overall_slope - slope)
    for region, slope in region_slopes.items()
    if region in simpsons_paradox_regions
}
slope_diff_sorted = dict(sorted(slope_diff.items(), key=lambda item: item[1], reverse=True))

print(f"SIMPSON'S PARADOX DETECTION RESULTS")
print()
print(f"Paradox Summary:")
print(f"   Overall slope:                 {overall_slope:,.2f}")
print(f"   Regions showing OPPOSITE trend: {len(simpsons_paradox_regions)} out of {len(region_slopes)}")
print(f"   Paradox prevalence:            {len(simpsons_paradox_regions)/len(region_slopes)*100:.1f}%")
print()

if len(simpsons_paradox_regions) > 0:
    print("REGIONS WITH SIMPSON'S PARADOX (opposite slope):")

    print(f"{'Rank':<6} {'Region':<25} {'Slope':>15} {'Diff from Overall':>20}")


    for rank, (region, diff) in enumerate(slope_diff_sorted.items(), 1):
        slope = region_slopes[region]
        print(f"{rank:<6} {region:<25} {slope:>15,.2f} {diff:>20,.2f}")

    print()

    # Detailed statistics for paradox regions
    print("\nDETAILED STATISTICS FOR PARADOX REGIONS:")

    for region in slope_diff_sorted.keys():
        stats = region_stats[region]
        print(f"\n{region}:")
        print(f"   Slope:              {stats['slope']:>15,.2f}")
        print(f"   R²:                 {stats['r_squared']:>15.4f}")
        print(f"   P-value:            {stats['p_value']:>15.4e}")
        print(f"   Significance:       {'Yes' if stats['p_value'] < 0.05 else 'No'} (α=0.05)")
        print(f"   Observations:       {stats['n_obs']:>15,}")
        print(f"   Countries:          {stats['n_countries']:>15}")
        print(f"   Avg Temperature:    {stats['mean_temp']:>15.2f}°C (±{stats['std_temp']:.2f})")
        print(f"   Avg Production:     {stats['mean_production']:>15,.0f}")
        print(f"   Median Production:  {stats['median_production']:>15,.0f}")
else:
    print("No Simpson's Paradox detected - all regions show consistent direction with overall trend")

print()

# Also show regions consistent with overall trend
consistent_regions = [region for region in region_slopes.keys() if region not in simpsons_paradox_regions]
if len(consistent_regions) > 0:
    print(f"REGIONS CONSISTENT WITH OVERALL TREND ({len(consistent_regions)} regions):")

    print(f"{'Region':<25} {'Slope':>15} {'R²':>10} {'P-value':>12} {'Countries':>10}")

    for region in sorted(consistent_regions):
        stats = region_stats[region]
        print(f"{region:<25} {stats['slope']:>15,.2f} {stats['r_squared']:>10.4f} {stats['p_value']:>12.4e} {stats['n_countries']:>10}")

print()
print()


# STEP 5: VISUALIZATION 1 - MAIN SIMPSON'S PARADOX PLOT


print("GENERATING VISUALIZATIONS...")
print()

plt.figure(figsize=(16, 9))

# Create color palette - use distinct colors for each region
region_colors = {
    'North America': '#e74c3c',      # Red
    'Central America': '#f39c12',     # Orange
    'Caribbean': '#9b59b6',           # Purple
    'South America': '#3498db'        # Blue
}

# If we have more regions, add more colors
all_regions = sorted(df_regions['region'].unique())
if len(all_regions) > 4:
    extra_colors = sns.color_palette("husl", len(all_regions) - 4)
    for i, region in enumerate(all_regions):
        if region not in region_colors:
            region_colors[region] = extra_colors[i]

legend_elements = []

# Plot each region
for region in sorted(regions):
    if region not in region_slopes:
        continue

    subset = df_regions[df_regions['region'] == region]
    stats = region_stats[region]

    # Determine if this region shows paradox
    is_paradox = region in simpsons_paradox_regions
    base_color = region_colors.get(region, '#95a5a6')

    # Make paradox regions more prominent
    alpha = 0.5 if is_paradox else 0.3
    size = 25 if is_paradox else 15
    edge = 'darkred' if is_paradox else 'none'
    edgewidth = 0.5 if is_paradox else 0

    # Scatter plot
    plt.scatter(subset[main_col], subset[value_col],
               alpha=alpha, s=size, color=base_color,
               edgecolors=edge, linewidths=edgewidth,
               label=f"{region}{' (PARADOX)' if is_paradox else ''}")

    # Regression line
    x_min, x_max = subset[main_col].min(), subset[main_col].max()
    x_vals = np.array([x_min, x_max])
    y_vals = stats['slope'] * x_vals + stats['intercept']

    linestyle = '-' if is_paradox else '--'
    linewidth = 3 if is_paradox else 2

    plt.plot(x_vals, y_vals, color=base_color, linewidth=linewidth,
            linestyle=linestyle, alpha=0.9)

# Plot overall regression line (THICK BLACK)
x_min, x_max = df_regions[main_col].min(), df_regions[main_col].max()
x_vals = np.array([x_min, x_max])
y_vals = overall_slope * x_vals + overall_intercept
plt.plot(x_vals, y_vals, color='black', linewidth=4,
         label=f'Overall Trend (slope={overall_slope:,.0f})',
         linestyle='--', alpha=0.9)

# Formatting
plt.xlabel("Average Temperature (°C)", fontsize=14, fontweight='bold')
plt.ylabel("Production Value", fontsize=14, fontweight='bold')
plt.title("Simpson's Paradox Detection: Production vs Temperature by Region\n" +
          f"Paradox Regions: {len(simpsons_paradox_regions)}/{len(region_slopes)} | " +
          f"Solid lines = Paradox | Dashed lines = Consistent",
          fontsize=16, fontweight='bold', pad=20)

plt.legend(loc='upper left', bbox_to_anchor=(1.01, 1),
          fontsize=11, frameon=True, shadow=True)

plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("Visualization 1 complete: Main Simpson's Paradox plot")
print()


# STEP 6: VISUALIZATION 2 - SLOPE COMPARISON BAR CHART


plt.figure(figsize=(12, 7))

# Prepare data for bar chart
regions_sorted = sorted(region_slopes.keys(), key=lambda x: region_slopes[x], reverse=True)
slopes_sorted = [region_slopes[region] for region in regions_sorted]
colors_sorted = [region_colors.get(region, '#95a5a6') for region in regions_sorted]

# Adjust alpha based on paradox status
alphas = [0.9 if region in simpsons_paradox_regions else 0.6 for region in regions_sorted]

# Create bar chart
bars = plt.barh(range(len(regions_sorted)), slopes_sorted,
               color=colors_sorted, alpha=0.8, edgecolor='black', linewidth=1.5)

# Highlight paradox regions with pattern
for i, region in enumerate(regions_sorted):
    if region in simpsons_paradox_regions:
        bars[i].set_hatch('///')

# Add overall slope line
plt.axvline(x=overall_slope, color='black', linewidth=3, linestyle='--',
           label=f'Overall Slope ({overall_slope:,.0f})', alpha=0.8, zorder=10)

# Add zero line
plt.axvline(x=0, color='gray', linewidth=1, linestyle=':', alpha=0.5)

# Formatting
plt.yticks(range(len(regions_sorted)), regions_sorted, fontsize=12, fontweight='bold')
plt.xlabel("Slope (Change in Production per °C)", fontsize=13, fontweight='bold')
plt.ylabel("Region", fontsize=13, fontweight='bold')
plt.title("Individual Region Slopes vs Overall Trend\n" +
         f"Simpson's Paradox: {len(simpsons_paradox_regions)}/{len(region_slopes)} regions show opposite trend",
         fontsize=15, fontweight='bold', pad=15)

# Add value labels on bars
for i, (region, slope) in enumerate(zip(regions_sorted, slopes_sorted)):
    if slope > 0:
        ha = 'left'
        x_pos = slope + abs(max(slopes_sorted) - min(slopes_sorted)) * 0.03
    else:
        ha = 'right'
        x_pos = slope - abs(max(slopes_sorted) - min(slopes_sorted)) * 0.03

    plt.text(x_pos, i, f'{slope:,.0f}', va='center', ha=ha,
            fontsize=10, fontweight='bold')

# Legend
from matplotlib.patches import Patch
legend_elements = [
    Patch(facecolor='gray', alpha=0.8, hatch='///',
          label=f'Paradox Regions (n={len(simpsons_paradox_regions)})'),
    Patch(facecolor='gray', alpha=0.6,
          label=f'Consistent Regions (n={len(consistent_regions)})'),
    plt.Line2D([0], [0], color='black', linewidth=3, linestyle='--', label='Overall Trend')
]
plt.legend(handles=legend_elements, loc='lower right', fontsize=11, frameon=True, shadow=True)

plt.grid(True, axis='x', alpha=0.3)
plt.tight_layout()
plt.show()

print("Visualization 2 complete: Slope comparison bar chart")
print()


# STEP 7: VISUALIZATION 3 - HEATMAP OF REGION STATISTICS


plt.figure(figsize=(14, 7))

# Prepare data for heatmap
heatmap_data = []
heatmap_regions = []

for region in regions_sorted:
    stats = region_stats[region]
    heatmap_data.append([
        stats['slope'] / 1000,  # Scale down for better visualization
        stats['r_squared'],
        -np.log10(stats['p_value']) if stats['p_value'] > 0 else 10,
        stats['mean_temp'],
        stats['std_temp'],
        stats['n_countries'],
        stats['mean_production'] / 10000  # Scale down
    ])
    heatmap_regions.append(region)

heatmap_df = pd.DataFrame(
    heatmap_data,
    index=heatmap_regions,
    columns=['Slope\n(×1000)', 'R²', '-log10(p)\nSignificance',
             'Mean Temp\n(°C)', 'Temp StdDev\n(°C)',
             'N Countries', 'Mean Prod\n(×10k)']
)

# Create heatmap
sns.heatmap(heatmap_df, annot=True, fmt='.2f', cmap='RdYlGn', center=0,
           cbar_kws={'label': 'Standardized Value'}, linewidths=1, linecolor='white')

plt.title("Regional Statistics Heatmap\nHigher values = Greener | Lower values = Redder",
         fontsize=14, fontweight='bold', pad=15)
plt.ylabel("Region", fontsize=12, fontweight='bold')
plt.xlabel("Statistic", fontsize=12, fontweight='bold')
plt.yticks(rotation=0, fontweight='bold')
plt.tight_layout()
plt.show()

print("Visualization 3 complete: Regional statistics heatmap")
print()


# STEP 8: VISUALIZATION 4 - INDIVIDUAL REGION SCATTER PLOTS


n_regions = len(region_slopes)
n_cols = 3 if n_regions >= 3 else n_regions
n_rows = (n_regions + n_cols - 1) // n_cols

fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5*n_rows))
if n_regions == 1:
    axes = [axes]
else:
    axes = axes.flatten() if n_regions > 1 else [axes]

for idx, region in enumerate(sorted(region_slopes.keys())):
    ax = axes[idx]
    subset = df_regions[df_regions['region'] == region]
    stats = region_stats[region]

    # Scatter plot
    is_paradox = region in simpsons_paradox_regions
    color = region_colors.get(region, '#95a5a6')

    ax.scatter(subset[main_col], subset[value_col], alpha=0.5, s=30, color=color)

    # Regression line
    x_min, x_max = subset[main_col].min(), subset[main_col].max()
    x_vals = np.linspace(x_min, x_max, 100)
    y_vals = stats['slope'] * x_vals + stats['intercept']
    ax.plot(x_vals, y_vals, color='darkred' if is_paradox else color,
           linewidth=3, label=f"Slope={stats['slope']:,.0f}")

    # Overall trend line for comparison
    y_overall = overall_slope * x_vals + overall_intercept
    ax.plot(x_vals, y_overall, color='black', linewidth=2, linestyle='--',
           alpha=0.5, label=f"Overall={overall_slope:,.0f}")

    # Formatting
    ax.set_xlabel("Avg Temperature (°C)", fontsize=10)
    ax.set_ylabel("Production Value", fontsize=10)

    paradox_label = 'PARADOX' if is_paradox else 'CONSISTENT'
    p_label = '<0.001' if stats['p_value'] < 0.001 else f"={stats['p_value']:.3f}"
    title_color = 'darkred' if is_paradox else 'darkblue'

    ax.set_title(f"{region} ({stats['n_countries']} countries)\n" +
                f"{paradox_label} | R²={stats['r_squared']:.3f} | p{p_label}",
                fontsize=11, fontweight='bold', color=title_color)
    ax.legend(loc='best', fontsize=9)
    ax.grid(True, alpha=0.3)

# Remove extra subplots
for idx in range(len(region_slopes), len(axes)):
    fig.delaxes(axes[idx])

plt.suptitle("Individual Region Analysis: Temperature vs Production\n" +
            "Paradox vs Consistent with Overall Trend",
            fontsize=16, fontweight='bold', y=0.995)
plt.tight_layout()
plt.show()

print("Visualization 4 complete: Individual region detailed plots")
print()


# STEP 9: VISUALIZATION 5 - TEMPERATURE DISTRIBUTION BY REGION


plt.figure(figsize=(14, 7))

# Create violin plot for temperature distribution
region_order = sorted(df_regions['region'].unique())
colors_list = [region_colors.get(r, '#95a5a6') for r in region_order]

violin_parts = plt.violinplot([df_regions[df_regions['region']==r][main_col].values
                               for r in region_order],
                              positions=range(len(region_order)),
                              widths=0.7,
                              showmeans=True,
                              showmedians=True)

# Color the violins
for i, pc in enumerate(violin_parts['bodies']):
    pc.set_facecolor(colors_list[i])
    pc.set_alpha(0.7)

# Formatting
plt.xticks(range(len(region_order)), region_order, rotation=15, ha='right', fontweight='bold')
plt.xlabel("Region", fontsize=13, fontweight='bold')
plt.ylabel("Average Temperature (°C)", fontsize=13, fontweight='bold')
plt.title("Temperature Distribution by Region\n" +
         "Violin width shows data density | White dot = median | Black bar = mean",
         fontsize=15, fontweight='bold', pad=15)
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()

print("Visualization 5 complete: Temperature distribution by region")
print()

# STEP 10: SUMMARY STATISTICS TABLE


print("FINAL SUMMARY")
print()

summary_stats = {
    'Total regions analyzed': len(region_slopes),
    'Regions showing paradox': len(simpsons_paradox_regions),
    'Regions consistent with overall': len(consistent_regions),
    'Paradox prevalence (%)': len(simpsons_paradox_regions)/len(region_slopes)*100 if len(region_slopes) > 0 else 0,
    'Overall slope': overall_slope,
    'Overall R²': overall_r**2,
    'Overall p-value': overall_p,
    'Total observations': len(df_regions),
    'Total countries': len(df_regions['country_name'].unique()),
    'Strongest paradox region': list(slope_diff_sorted.keys())[0] if slope_diff_sorted else 'None',
    'Strongest paradox magnitude': list(slope_diff_sorted.values())[0] if slope_diff_sorted else 0
}

for key, value in summary_stats.items():
    if isinstance(value, float):
        if 'p-value' in key:
            print(f"{key:.<45} {value:.4e}")
        elif '%' in key:
            print(f"{key:.<45} {value:.1f}%")
        else:
            print(f"{key:.<45} {value:,.2f}")
    elif isinstance(value, int):
        print(f"{key:.<45} {value:,}")
    else:
        print(f"{key:.<45} {value}")

print()

# Additional regional comparison table
print("REGIONAL COMPARISON TABLE")
print()
print(f"{'Region':<20} {'Countries':>10} {'Observations':>12} {'Mean Temp':>12} {'Slope':>15} {'Paradox?':>10}")

for region in sorted(region_slopes.keys()):
    stats = region_stats[region]
    is_paradox = 'YES' if region in simpsons_paradox_regions else 'No'
    print(f"{region:<20} {stats['n_countries']:>10} {stats['n_obs']:>12,} "
          f"{stats['mean_temp']:>12.2f}°C {stats['slope']:>15,.0f} {is_paradox:>10}")

print()
print("ANALYSIS COMPLETE!")

"""**7. Do you trust that this data is a reasonable dataset to use?**
* Why or why not?
* If not, how will you handle it?

---

### 7. Do you trust that this data is a reasonable dataset to use?

Yes, I believe the dataset I used is generally **trustworthy and reasonable** for analyzing how climate change impacts agricultural production across countries in the Americas. It combines several sources of data, including daily weather information and agricultural production records, which together give me a strong foundation to study the relationship between climate variables and crop yields.

---

#### **Why I trust the dataset**

I used multiple files in this project — `daily_weather.parquet`, `Production_Crops_Livestock_E_Americas_NOFLAG.csv`, `countries.csv`, and `cities.csv`. Each of these sources contains important and reliable information:

- The **daily weather data** provides detailed measurements of temperature and precipitation, which are essential for climate analysis.  
- The **production data** gives insight into how crops and livestock outputs change across different countries and years.  
- The **countries and cities data** helped me merge geographic information so that I could connect climate data to production data accurately.

After loading and combining everything, I created a cleaned dataset called **`cleaned_df`**, which I used for the final analysis. I carefully inspected the dataset for duplicates, missing values, and inconsistent formats. This cleaning process gave me confidence that the data is structured properly and ready for analysis.

I also trust this dataset because it represents **multiple countries and years**, which allows me to identify patterns and trends rather than relying on data from a single region or time period. The variables included are also directly relevant to my research question — they help me understand how climate factors such as temperature and rainfall might affect agricultural productivity.

---

#### **Concerns and limitations**

While the dataset is useful and well-structured, it does have a few limitations that I needed to be careful about:

- Some **missing values** exist in both the weather and production data. Not every country reports consistently every year.  
- The **time scale** of the data differs — weather data is daily, but production data is annual. This means I had to aggregate weather data into annual averages to align the time periods.  
- There might be **outliers or unusual spikes** in production or temperature data due to events like droughts, floods, or data reporting errors.  
- The dataset focuses mostly on **climatic and production factors**, but not on other things that might influence agriculture (like soil fertility, farming technology, or economic conditions).

---

#### **How I handled these issues**

To make the dataset more reliable, I took several steps:
- I **cleaned and merged** all data sources carefully and checked for missing or duplicated entries.  
- I used **aggregation and normalization techniques** to ensure that weather data matched production data in terms of time scale.  
- I identified and handled **outliers** to prevent them from skewing results.  
- I made sure to **document my preprocessing steps** clearly so the analysis remains transparent and reproducible.

---

#### **Conclusion**

Overall, I trust that my dataset is **reasonable and suitable** for this project. It provides enough detail, coverage, and reliability to explore the effects of climate change on agriculture in the Americas. While it isn’t perfect and has some limitations, I handled those issues through data cleaning, aggregation, and careful analysis. As a result, I’m confident that my findings are based on solid and trustworthy data.


---

8. Wrap up
* Provide an overview of what you learned through the Data Exploration process.
  * Did this affect your hypothesis?
  * Why or why not?
* Summarize any key findings, insights, or things you’ll want to explore more.

---

### 8. Wrap Up

Through this data exploration process, I learned a lot about how **climate variables such as temperature and precipitation directly relate to agricultural productivity** across different countries in the Americas. By cleaning, merging, and analyzing the datasets, I was able to better understand the relationships between environmental patterns and food production trends over time.

---

#### **What I learned from the Data Exploration process**

While exploring the data, I noticed several interesting trends. For example, **increases in average temperature and irregular rainfall patterns** often aligned with **drops in crop production** in certain countries. This suggested that climate variability truly does play a major role in agricultural outcomes. On the other hand, some regions appeared to be less affected or even showed increased yields — possibly due to adaptation strategies, improved irrigation, or more resilient crops.

I also learned how important it is to **clean and align data carefully**. The raw datasets had different formats, missing entries, and time scale mismatches (daily weather data vs. annual production data). Working through these challenges taught me how to merge large and complex datasets properly, check for outliers, and make sure variables are on compatible scales before drawing conclusions.

---

#### **Did this affect my hypothesis?**

Originally, my hypothesis was that **rising temperatures and changes in precipitation patterns would negatively impact agricultural production**. After exploring the data, I still believe this hypothesis holds true overall — the trends in the data support the idea that climate change has a measurable effect on crop yields.  

However, my analysis also revealed that **the impact isn’t uniform across all countries or crops**. Some areas seem to adapt better to climate shifts, possibly due to geography, local policies, or access to modern agricultural techniques. This finding made me refine my hypothesis: instead of a simple “climate change decreases production” statement, I now see it as **“climate change affects production unevenly, depending on how resilient and adaptable each region is.”**

---

#### **Key findings and insights**

- Temperature and rainfall fluctuations are closely linked to changes in agricultural output.  
- There is strong **geographical variation** — some countries show resilience, while others show steep declines.  
- Data integration from multiple sources provided richer insights but required careful cleaning and preprocessing.  
- Outliers and missing data can significantly affect interpretations if not handled properly.  
- The process deepened my understanding of **how environmental data can be used to support sustainable agricultural planning**.

---

#### **Next steps and further exploration**

Moving forward, I want to:

- Explore **machine learning models** to predict future crop yields under different climate scenarios.  


---

In summary, this exploration process not only validated my initial hypothesis but also helped me see the complexity of the relationship between climate change and food security. It reinforced the idea that data-driven analysis is essential for understanding and addressing real-world environmental challenges.

# **PROJECT 4**

---

**2. What kind of ML task is presented by your hypothesis, and what type of learning is it?**
* Ex. Classification, Regression, Clustering, etc
* Ex. Supervised Learning, Reinforcement Learning, etc
* Tammy recommendation: unless you are experienced or have ample extra time and motivation to dive deep on your own, stick to supervised learning techniques for this course.
* Describe what you intend to do at a high level.

## Type of ML Task: Regression
My project presents a **regression** task because I am predicting continuous numerical value:
agricultural production quantity. The target variable `value` represents production outputs that can take any positive real number, making this inherently a regression problem rather than classification or clustering.

## Type of Learning : Supervised Learning

This is a supervised learning problem because:



1.   **Labeled Training Data**: I have historical data where both the input features and the output labels are known for the years 1961- 2023.
2.  **Learn from Examples:** The algorithm will learn patterns from past climate-production relationships to make predictions about future or unknown scenarios.

This contrasts with:
* Unsupervised Learning (no labels, just finding patterns)
* Reinforcement Learning (learning through trial-and-error with rewards)

Following Professor's recommendation, I am sticking with supervised learning techniques, which are well-suited for this agricultural prediction problem.

## High- Level Description of Intent

**What I Will Do:**
I intend to build a supervised regression model that predicts agricultural production values based on climate and contextual features. Specifically: <br>
**Input Features:**
* Climate Variables: average temperature, precipitation, wind speed, atmospheric pressure
* Temporal information: year
* Geographic Context: Country
* Agricultural Context: crop/item type, measurement type
* Engineered features: temperature-precipitation interactions, squared temperature terms

**Target Variable:**
* Log-transformed production value( using `log(1+value)` to handle the extreme right skew I discovered in EDA)

**Objective:** Predict how much of a given agricultural product will be produced in a specific country and year, given the climate conditions and context.

**Why This Approach:**
Based on my exploratory analysis, I discovered that:

1. **Simple Linear relationship don't exist:** Individual climate variable show very weak correlations(< 0.10) with production

2. **Complex interactions matter:** The Simpson's Paradox analysis revealed that relationships differ dramatically by country and crop type.

3. **Context is critical:** Temperature effects depend on baseline climate, crop type, and agricultural infrastructure

Therefore, I need a model that can:
* Capture **non-linear** relationships (e.g., optimal temperature ranges)
* Handle interaction effects (e.g., temperature * precipitation)
* Account for **confounding variables** (country, crop type)
* Work with **heterogeneous data**(different countries respond differently)

## The Modelling Strategy:
**Step 1: Feature Engineering**
* Remove multicollinear features (keep only avg_temp_c, drop min/max)
* Log-transform the target to handle skewness
* Encode categorical variables (country, crop, element)
* Create interactions and polynomial features
* Scale features if needed (depending on algorithm choice)

**Step 2: Model Training**
* Split data : 80% training, 20% testing
* Train a regression algorithm that handles non-linearity and interactions
* Validate using cross-validation with training set
* Tune hyperparameters to optimize performance

**Step 3: Predictions & Evaluation**
* Make predictions on held-out test set
* Inverse-transform predictions back to original scale
* Calculate regression metrics
* Analyse features importance to understand what drives production

**Step 4: Interpretation**
* Use model insights to answer the research question:"How will climate change impact agricultural production?"
* Generate predictions for future climate scenarios (e.g., +2 C warming by 2040)
* Identify crops and regions most vulnerable to climate change
* Provide actionable insights for adaptation strategies

## Expected Outcome:
The trained model will enable me to:
1. **Quantify climate impacts:** Estimate how much production changes per degree of warming
2. **Make regional predictions:** Account for country-specific responses discovered in Simpson's Paradox.
3. **Identify vulnerable Systems:** Find which crop-country combinations are most at risk
4. **Support decision-making:** Provide data-driven insights for agricultural planning and climate adaptation

##Connecting to original Hypothesis:
My original hypothesis suggested that "regions with the greatest increase in temperature will be more favourable for crops requiring warmer conditions."

However, my EDA and Simpsons's Paradox analysis revealed this is

* **oversimplified**. The reality is more nuanced:
* **Cold regions**(Canada, Chile) may benefit from moderate warning
* **Temperate regions** (USA, Argentina) show mixed effects
* **Tropical regions** may already be near thermal limits
* **Crop-specific responses** vary dramatically (sugar can thrives with heat, while most crops suffer)

Therefore, my ML model will capture these complex, **context-dependent relationships** rather than assuming a simple universal pattern. This is why a sophisticated regression approach with interaction terms and non-linear modelling is necessary

---

**3. What features will you use?**
* 📝Note: Depending on your algorithm, this may need to be done as part of or after the next step or later in this writeup
  * That is ok - just make sure that it makes sense/is clear. The key is to be sure to explain what you are doing and why (all of the below steps)
* Show/explain how you are deciding, what you are choosing, and why
  *  Ex. Feature Engineering techniques
  *  Ex. Removing columns you no longer need due to application of a transformation in earlier steps
* Were you able to reduce the dimensionality of your dataset? If so, by how much? If not, why not?
* What is the dimensionality of the resulting dataset you plan to use with your algorithm?
* What assumptions can be reasonably made about the resulting dataset? Why?
  * Ex. Normality, independence, etc
* What are the names of the selected features, and for each one, are they discrete/categorical or continuous?
* Optional: Extra Credit (up to 10 points):
  * Try out a couple of different techniques and compare them.
  * Which one gave you the highest reduction in dimensions (number of features)?
  * What do you think might be the reason for the difference? (Or if there was no difference, why might that be?)
  * What is the benefi t of reducing dimensionality?

## Feature Selection Decision process
My cleaned dataset contains 17 columns.

##Decision Framework:
I will select features based on:
1. Correlation analysis from my EDA
2. Multicollinearity detection (VIF scores, correlation matrix)
3. Data quality issues identified during exploration
4. Domain knowledge about agriculture and climate
5. Simpson's Paradox insights about important confounders

## Features to REMOVE and WHY
1. Redundant Code Columns (3 features removes) <br>
Removing: `country_code`, `Item Code`, `Element code` <br>
**Reason:** These are just numeric identifiers that duplicate information already contained in their corresponding name columns (`country_name`, `Item`, `Element`). The name columns are more interpretable and contain the same information. Including both would add unnecessary dimensionality without improving model performance.<br>
**Example:** `country_code=32` is less informative than `country_name="Argentina"`

2. Multicollinear Temperature Features(2 features removed) <br>
**Removing:** `min_temp_c`, `max_temp_c` <br>
**Keeping:** `avg_temp-c` <br>
**Reason:** My correlation matrix showed extreme multicollinearity
**Why this matters:** Multicollinearity causes:
* Unstable coefficient estimates
* Inflated standard errors
* Difficulty interpreting individual feature importance
* Redundant information <br>

**Choice rationale:** I keep `avg_temp_c` because:
* It's the most stable measure
* Most relevant for crop growth
* Provides complete information when min/max vary predictably around it.

3. Low-Quality Data Features (2 features removed) <br>
**Removing:** `snow_depth_mm`, `peak_wind_gust_kmh` <br>
**Reasons:** <br>
`snow_depth_mm`: <br>
* My descriptive statistics showed 65,979 outliers (11.7% of data)
* This indicates imputed/ filled data rather than real measurements
* Most Americas countries are tropical/subtropical with no snow: this feature adds noise<br>


`peak_wind_gust_kmh`:
* 137,718 outliers (24.4 % of data)
* Less relevant to agricultural production than sustained wind speed

4. Unit Column (1 feature removed) <br>
**Removing:** `Unit`<br>
**Reason:** This column indicates measurement units (tonnes, hectares, kg/ha, etc.) but:
* It's already encoded in the Element column.
* Different units make the target variable non-comparable
* I'll handle this by modelling Element types seperately or using Element as a categorical feature
* Including Unit adds redundancy without predictive value.

## **Features to KEEP and why**
**Climate Features (Continuous Variables)**
1. `avg_temp_c`
* **Type**: Continuous
* **Reason**: Primary climate variable of interest
* **Range**: -4.7 C to 30.0 C
* **Quality**: No missing values, 5289 outliers (0.9%)
* **Importance:** Temperature directly affects photosynthesis, growing season length, pest populations

2. `precipitation_mm`
* **Type**: Continuous
* **Reason**: Critical for crop growth; moderate correlation with temperature (0.36)
* **Range**: 0 to 49.6 mm
* **Quality**: No missing values, 16810 outliers(3%) - represents real extreme rainfall events
* **Importance:** Water availability is often MORE limiting than temperature

3. `avg_wind_speed_kmh`
* **Type**: Continuous
* **Reason**:Affects evapotranspiration, pollination, physical crop damage
* **Range**: 0.8 to 27.2 km/h
* **Quality**: 15825 outliers (2.8%)
* **Negative correlation with temperature (-0.44):** Provides complementary information

4. `avg_sea_level_pres_hpa`
* **Type**: Continuous
* **Reason**: Indicator of weather systems; correlates with temperature and precipitation patterns
* **Range**: 1006.9 to 10223.3 hPa (very stable, as expected)
* **Quality**: Only 2529 outliers (0.4%) - excellent quality
* **Importance:** Stable atmospheric conditions favor consistent crop growth

<br>

**Contectual Features(Categorical Variables)** <br>

5. `country_name`
* **Type**: Categorical (30 Unique Values)
* **Reason**: **CRITICAL CONFOUNDER** identified in Simpson's Paradox analysis
* **Why essential:**
  * 67% of countries show opposite temperature-production relationship from aggregate
  * Captures agricultural infrastructure, technology, policies, farming practices
  * Geographic / climatic baseline differences
* **Encoding Needed:** Label Encoding or One-Hot Encoding


6. `item`<br>

* **Type**: Categorical (276 Unique Values)
* **Reason**: **CRITICAL**- different crops respond differently to climate
* Examples from my analysis:
  * Sugar cane: positive temperature response (+88722 slope)
  * Most crops: negative temperature response
  * Livestock products: minimal temperature sensitivity
* **Encoding needed**: Label Encoding (high cardinality makes One-Hot impractical)

7. `Element`
* **Type**: Categorical (8 Unique Values)
* **Reason**: Specifies what's being measured
* **Why important:** Different measurements have different scales and meanings
* **Most Common:** Production (39% of data)
* **Encoding:** One-Hot Encoding (low cardinality - Only 8 values)

**Temporal Feature**

8. `year`
* **Type:** Continuous
* **Reason:** Captures technological improvements, agricultural practices evolution over time
* **Evidence from EDA**:
  * Clear upward trend: produciton increased ~50% from 1960s to 2020s
  * Acceleration in recent decades
* Important note: This is NOT just a time trend - it captures non-climate factors like:
  * Better seeds, fertilizers, machinery
  * Improved irrigation and farming techniques
  * Policy changes and subsidies
  * Land use expansion


##Feature Engineering : Creating New Features
1. **Temperatue Non-Linearity**: `temp_squared`
"""

# df['temp_squared'] = df['avg_temp_c']**2

"""* **Type:** Continuous (engineered)
* **Reason:** Capture optimal temperature ranges and non-linear effects
* **Domain Knowledge:**
  * Crops have optimal temperature ranges
  * Too cold = slow growth
  * Optimal = maximum growth
  * Too hot = heat stress, reduced yields
  * This creates a parabolic (quadratic ) relationship


2. **Climate Interaction:** `temp_precip_interaction`

"""

# df['temp_precip_interaction'] = df['avg_temp_c'] * df['precipitation_mm']

"""* **Type:** Continuous (engineered)
* **Reason:** Temperature and precipitation effects are not independent
* **Domain Knowledge:**
  * High temperature + low precipitation = drought stress (very bad)
  * High temperature + high precipitation = humid tropics (good for some crops)
  * Low temperature + high precipitation = fungal diseases, waterlogging
  * Low temperature + low precipitation = semi-arid cold (marginal agriculture)
* **Why this matters:**
  * The same temperature has different effects depending on water availability
  * Interaction terms let the model learn these combined effects

3. **Regional Grouping: `region`**

"""

# # Based on my geographic analysis


# region_mapping = {

#     'North America': ['Canada', 'United States Of America', 'Mexico'],
#     'Central America': ['Guatemala', 'Belize', 'El Salvador', 'Honduras',
#                         'Nicaragua', 'Costa Rica', 'Panama'],
#     'Caribbean': ['Cuba', 'Haiti', 'Dominican Republic', 'Jamaica',
#                   'Trinidad And Tobago', 'Puerto Rico', 'Bahamas', 'Barbados',
#                   'Saint Lucia', 'Grenada', 'Saint Vincent And The Grenadines',
#                   'Antigua And Barbuda', 'Dominica', 'Saint Kitts And Nevis'],


#     'South America': ['Colombia', 'Venezuela', 'Ecuador', 'Peru', 'Brazil',
#                       'Bolivia', 'Paraguay', 'Chile', 'Argentina', 'Uruguay',
#                       'Guyana', 'Suriname', 'French Guiana']
# }



# # Create reverse mapping: country -> region

# country_to_region = {}
# for region, countries in region_mapping.items():
#     for country in countries:

#         country_to_region[country] = region

# # Now map it
# df['region'] = df['country_name'].map(country_to_region)

# # Check if any countries weren't mapped

# unmapped = df[df['region'].isna()]['country_name'].unique()

# if len(unmapped) > 0:

#     print(f"Warning: {len(unmapped)} countries not mapped to regions:")


#     print(unmapped)

"""* **Type:** Categorical (4 values)
* **Reason:** My regional Simpson's Paradox analysis showed 75% paradox prevalence
* **Findings:**
  * Central America: +38,430 slope (strongest positive)
  * South America: +12,611 slope
  * Carribean : +5,518 slope
  * North America: -17,816 slope (only negative)
* **Benefit:** Reduces dimensionality from 30 countries to 4 regions while preserving climate response patterns
* **Decision:** I'll keep `country_name` for now (more granular) but may test `region` as alternative.

## Target Variable Transformation
**Log Transformation of `value`**

"""

# df['log_value'] = np.log1p(df['value'])  # log(1 + value) to handle zeros

"""**Why this is ESSENTIAL:**
1. Extreme right skew identified in EDA:
  * Mean: 376.620
  * Median : 5.341 (70* smaller)
  * Max: 28,388,030 (531 * the mean!)
  * 99,584 outliers (17.6% of data)
2. Problems with untransformed data:
  * Model will be dominated by extreme values
  * Predictions will have high error for small productions
  * Residuals will be heteroscedastic (non-constant variance)
  * Violates assumptions of many algorithms
3. Benefits of log transformation:
  * Makes distribution more symmetric (closer to normal)
  * Stabilizes variances across production scales
  * Model predicts percentage changes rather than absolute changes (more meaningful)
  * Outliers have less extreme influence
  * Interpretability: coefficients represents multiplicative effects
4. Post-processing needed:
  * Predictions will be in log-scale
  * Must inverse transform: `predicted_value = np.exam(log_predicted_value)`

  <br>

## Encoding Categorical Variables

**Strategy:**
  For tree-based algorithm (Random Foest , XGBoost):
* Use label Encoding for all categorical features
* Trees can handle ordinal encodings naturally
* Doesn't create high dimensionality


"""

# from sklearn.preprocessing import LabelEncoder

# le_country = LabelEncoder()
# le_item = LabelEncoder()

# df['country_encoded'] = le_country.fit_transform(df['country_name'])
# df['item_encoded'] = le_item.fit_transform(df['Item'])

"""**For Element (Only 8 categories):**
* Use One-Hot Encoding (creates 7 dummy variables, drop first to avoid multicollinearity)
"""

# element_dummies = pd.get_dummies(df['Element'], prefix='element', drop_first=True)
# df = pd.concat([df, element_dummies], axis=1)

"""## Dimensionality Reduction Summary
**Original Dataset:**
* 17 columns total
* 8 potential features (excluding target `value`)

**After Removal:**
* Removed: `country_code`, `Item_code`, `Element_code`, `Unit` (4 redundant / unnecessary)
* Removed : `min_temp_c`, `max_temp_c`(2 multicollinear)
* Removed: `snow_depth_mm`, `peak_wind_gust_kmh` (2 low quality)
* Total removed : 8 features

**After Feature Engineering:**
**Raw features kept: 8**
* `avg_tmep_c`, `precipitation_mm`, `avg_wind_speed_kmh`, `avg_sea_level_pres_hpa`
* `country_name`, `item`, `Element`, `year`

**Engineered features added:2**
* temp_squared, temp_precip_interaction

**After Encoding:**
* `country_encoded` (1 feature, replaces country_name)
* `item_encoded` (1 feature, replaces Item)
* `element_*` dummies (7 features, replaces Element)

**Final Feature Count: 15 features**
| Feature Type           | Count | Names                                                                 |
|------------------------|-------|------------------------------------------------------------------------|
| Continuous climate     | 4     | avg_temp_c, precipitation_mm, avg_wind_speed_kmh, avg_sea_level_pres_hpa |
| Engineered climate     | 2     | temp_squared, temp_precip_interaction                                 |
| Temporal               | 1     | year                                                                   |
| Encoded categorical    | 8     | country_encoded, item_encoded, element_* (7 dummies)                   |
| Target                 | 1     | log_value                                                              |
| **TOTAL**              | **16** | (15 features + 1 target)                                               |


## **Dimensionality Reduction Achieved**

- **Original potential features:** 16 (excluding target)  
- **Final features used:** 15  
- **Reduction:** 6.25% (modest reduction)

### **More importantly**
- Removed **8 problematic features**
- Added **2 meaningful engineered features**

## **Why Dimensionality Reduction Was Limited**

I achieved only modest dimensionality reduction (16 -> 15 features) for the following reasons:

1. **Started with clean, focused data:**  
   My dataset was already well-curated with mostly relevant variables.

2. **Critical confounders must be retained:**  
   Simpson's Paradox analysis showed that `country` and `item` are essential.  
   Removing them would significantly reduce model validity.

3. **Low-dimensional dataset to begin with:**  
   There were only 16 potential features.

4. **Tree-based algorithm planned:**  
   Models like Random Forest and XGBoost handle higher dimensionality well and  
   do not require aggressive feature reduction.

5. **Information preservation was the priority:**  
   It was more important to retain informative variables than to force  
   unnecessary dimensionality reduction.

6. **Quality > quantity:**  
   The focus was on removing **problematic features** (multicollinear, low-quality)  
   rather than maximizing the number of features removed.


## **Assumptions About the Resulting Dataset**

### **1. Independence of Observations - Violated**

**Assumption:**  
Each observation should be independent of others.

**Reality:**  
This assumption is **not satisfied** due to:

- **Temporal autocorrelation:**  
  Same country–crop combinations appear across adjacent years.

- **Spatial correlation:**  
  Neighboring countries often share similar climate patterns.

- **Grouped/hierarchical structure:**  
  Multiple observations per **country**, per **crop**, per **year**.



### **Mitigation Strategies**

- **Stratified train–test split (by country):**  
  Ensures each country is represented in both training and testing datasets.

- **Time-based split option:**  
  Train on **1961–2010**, test on **2011–2023** to evaluate temporal generalization.

- **Use tree-based models:**  
  Methods like Random Forest and XGBoost are more robust to violations of independence  
  compared to linear regression models.



## **2. Normality of Features - Not Required but addressed**

**Assumption:**  
Some algorithms assume that features are normally distributed.

**Reality:**

- **Temperature:** Approximately normal   
- **Precipitation:** Right-skewed  
- **Wind speed:** Approximately normal  
- **Pressure:** Nearly normal  
- **Target (log_value):** Much more normal after transformation  



### **Why Normality Does Not Matter for My Algorithm Choice**

- Tree-based models (Random Forest, XGBoost) **do not assume normality**  
- They split based on **thresholds**, not on absolute value distributions  
- Only classical linear models strictly require normality assumptions  


### **If I Used Linear Regression Instead, I Would Need To:**

- Transform precipitation (log or Box-Cox)  
- Standardize all numerical features  
- Verify that residuals are approximately normal  


## 3. No Multicollinearity - Addressed

**Assumption:**  
Features should not be highly correlated with each other.



### Actions Taken

- Removed:  
  - `min_temp_c` and `max_temp_c`  
    - Both had extremely high correlation with `avg_temp_c` (r > 0.94)  
    - Keeping them would inflate variance and reduce interpretability  

- Kept moderate correlations (r = 0.3–0.5):  
  These provide complementary information rather than redundancy.



### Examples of Acceptable Correlations Kept

- Temperature and Precipitation: r = 0.36  
- Temperature and Wind Speed: r = -0.44  

These values are not high enough to cause multicollinearity issues, especially for tree-based models.


## 4. Sufficient Sample Size  - Excellent

**Assumption:**  
There should be enough data for reliable pattern learning.



### Reality

- 564,291 total observations (after cleaning)
- 220,626 production observations (focus of the model)
- 30 countries × 276 crops × 63 years  
  * provides a rich, diverse, and balanced dataset



### Rule of Thumb

A common guideline is to have at least 10 times as many observations as features.

- Features: 15  
- Minimum required observations: ~150  
- Actual observations: 220,626  
- This is more than 1,400 times the minimum requirement.



### Implications

This dataset is large enough to support:

- Complex models (deep trees, large ensembles)
- Cross-validation without sacrificing too much data
- Detection of subtle interaction effects between climate and agricultural variables



##5. No Extreme Outliers in Features
**Assumption:** Outliers don't dominate patterns

**Actions taken:**

* Removed features with > 10% outliers (`snow_depth_mm`, `peak_wind_gust_kmh`)
* Kept features where outliers represent real phenomena (extreme rainfall, heat waves)
* Log-transformed target to reduce outlier influence

**Remaining Outliers:**
* Climate extremes: Represent real weather events (droughts, floods, heat waves)
* Production outliers: Reduced dramatically by log transformation
* Tree-based models handles outliers naturally (split-based, not distance-based)


##6. Stationarity

**Assumption:** Statistical properties don't change over time

**Reality:**
* Climate is changing: Temperatures rising, precipitation patterns shifting
* Technology is advancing: Yields increasing independent of climate
* Agricultural practies evolving: Irrigation, GMOs, precision agriculture

**Why this matters:**
* Model trained on 1961-200 may not generalize well to 2020+
* Relationship between temperature and yield may be non-stationary

## Final Feature List with Types

## Feature Summary Table

| #  | Feature Name              | Type                     | Range / Values                | Why Included                           |
|----|---------------------------|---------------------------|--------------------------------|----------------------------------------|
| 1  | avg_temp_c               | Continuous                | -4.7 to 30.3 C               | Primary climate driver                 |
| 2  | precipitation_mm         | Continuous                | 0 to 49.6 mm                  | Water availability                     |
| 3  | avg_wind_speed_kmh       | Continuous                | 0.8 to 27.2 km/h              | Evapotranspiration, physical stress    |
| 4  | avg_sea_level_pres_hpa   | Continuous                | 1,006.9 to 1,022.3 hPa        | Weather stability indicator            |
| 5  | temp_squared             | Continuous (engineered)   | 0 to 918                      | Non-linear temperature effects         |
| 6  | temp_precip_interaction  | Continuous (engineered)   | -233 to 1,502                 | Combined climate effects               |
| 7  | year                     | Continuous                | 1961 to 2023                  | Technological progress                 |
| 8  | country_encoded          | Discrete (categorical)    | 0 to 29                       | Geographic/infrastructure context      |
| 9  | item_encoded             | Discrete (categorical)    | 0 to 275                      | Crop-specific responses                |
| 10–16 | element_* (7 dummies) | Discrete (categorical)    | 0 or 1                        | Measurement type                       |

## Optional: Extra Credit (up to 10 points):

* Try out a couple of different techniques and compare them.
* Which one gave you the highest reduction in dimensions (number of features)?
* What do you think might be the reason for the difference? (Or if there was no difference, why might that be?)
* What is the benefit of reducing dimensionality?
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split

# Load your data
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DATA 602 Project/final_cleaned_data.csv')

# Create working copy


df_ml = df.copy()

# Remove low-quality features
features_to_remove = ['snow_depth_mm', 'peak_wind_gust_kmh']
df_ml = df_ml.drop(columns=features_to_remove, errors='ignore')


# Remove redundant columns
redundant_cols = ['country_code', 'Item Code', 'Element Code']
df_ml = df_ml.drop(columns=redundant_cols, errors='ignore')



# Remove multicollinear features


multicollinear_cols = ['min_temp_c', 'max_temp_c']

df_ml = df_ml.drop(columns=multicollinear_cols, errors='ignore')


# Remove Unit column

df_ml = df_ml.drop(columns=['Unit'], errors='ignore')

# Create engineered features

df_ml['temp_squared'] = df_ml['avg_temp_c'] ** 2


df_ml['temp_precip_interaction'] = df_ml['avg_temp_c'] * df_ml['precipitation_mm']

# Create log-transformed target
df_ml['log_value'] = np.log1p(df_ml['value'])


# Filter to Production records only


df_ml = df_ml[df_ml['Element'] == 'Production'].copy()

# Remove missing values

df_ml = df_ml.dropna(subset=['log_value'])

# Define features


numeric_features = [
    'avg_temp_c',
    'precipitation_mm',
    'avg_wind_speed_kmh',
    'avg_sea_level_pres_hpa',
    'temp_squared',
    'temp_precip_interaction',
    'year'
]

# Encode categorical features
le_country = LabelEncoder()


le_item = LabelEncoder()
df_ml['country_encoded'] = le_country.fit_transform(df_ml['country_name'])
df_ml['item_encoded'] = le_item.fit_transform(df_ml['Item'])



# Create final feature set


feature_columns = numeric_features + ['country_encoded', 'item_encoded']
target_column = 'log_value'


# Extract X and y


X = df_ml[feature_columns].copy()
y = df_ml[target_column].copy()

# EXTRA CREDIT: DIMENSIONALITY REDUCTION COMPARISON



import numpy as np
import pandas as pd
from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, mutual_info_regression
from sklearn.model_selection import train_test_split


from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# STEP 1: PREPARE DATA FOR DIMENSIONALITY REDUCTION


# Use the already prepared X and y from main analysis
print(f"Original feature set:")

print(f"   Total features: {X.shape[1]}")
print(f"   Total samples: {X.shape[0]:,}")

print(f"   Feature list: {list(X.columns)}")
print()



# Store original performance (baseline)
X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=df_ml['country_encoded']
)



rf_baseline = RandomForestRegressor(
    n_estimators=100,
    max_depth=20,
    min_samples_split=50,

    min_samples_leaf=20,

    random_state=42,
    n_jobs=-1
)
rf_baseline.fit(X_train_full, y_train_full)
y_pred_baseline = rf_baseline.predict(X_test_full)


r2_baseline = r2_score(y_test_full, y_pred_baseline)

print(f"Baseline Rsquare (with all {X.shape[1]} features): {r2_baseline:.4f}")
print()



# TECHNIQUE 1: PRINCIPAL COMPONENT ANALYSIS (PCA)


print("\n1.1: Scaling features for PCA")


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)




pca_full = PCA()
pca_full.fit(X_scaled)

# Calculate cumulative explained variance


explained_variance_cumsum = np.cumsum(pca_full.explained_variance_ratio_)

print(f"\n1.3: Explained variance by number of components:")


print(f"{'Components':<12} {'Cum. Variance':<15} {'Dimensionality Reduction'}")


for n_comp in [1, 2, 3, 5, 8, 10, 12, X.shape[1]]:

    if n_comp <= len(explained_variance_cumsum):
        var_explained = explained_variance_cumsum[n_comp-1]

        reduction_pct = (1 - n_comp/X.shape[1]) * 100

        print(f"{n_comp:<12} {var_explained*100:>6.2f}%         {reduction_pct:>6.1f}% reduction")

# Find components for 80%, 90%, 95% variance


n_comp_80 = np.argmax(explained_variance_cumsum >= 0.80) + 1

n_comp_90 = np.argmax(explained_variance_cumsum >= 0.90) + 1
n_comp_95 = np.argmax(explained_variance_cumsum >= 0.95) + 1

print(f"\n1.4: Components needed for variance thresholds:")
print(f"   80% variance: {n_comp_80} components (reduction: {(1-n_comp_80/X.shape[1])*100:.1f}%)")


print(f"   90% variance: {n_comp_90} components (reduction: {(1-n_comp_90/X.shape[1])*100:.1f}%)")
print(f"   95% variance: {n_comp_95} components (reduction: {(1-n_comp_95/X.shape[1])*100:.1f}%)")



# Test PCA with different numbers of components


pca_results = {}

for n_components in [5, 8, 10, n_comp_80, n_comp_90]:
    if n_components > X.shape[1]:
        continue



    # Apply PCA
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X_scaled)


    # Split data
    X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(
        X_pca, y, test_size=0.2, random_state=42
    )


    # Train model
    rf_pca = RandomForestRegressor(
        n_estimators=100,
        max_depth=20,
        min_samples_split=50,
        min_samples_leaf=20,
        random_state=42,
        n_jobs=-1
    )


    rf_pca.fit(X_train_pca, y_train_pca)

    # Evaluate

    y_pred_pca = rf_pca.predict(X_test_pca)
    r2_pca = r2_score(y_test_pca, y_pred_pca)

    pca_results[n_components] = r2_pca

    reduction = (1 - n_components/X.shape[1]) * 100
    var_explained = explained_variance_cumsum[n_components-1] * 100

    print(f"   {n_components} components: R²={r2_pca:.4f} | "
          f"Variance={var_explained:.1f}% | Reduction={reduction:.1f}%")

print()

# TECHNIQUE 2: SELECTKBEST (MUTUAL INFORMATION)




selector_full = SelectKBest(score_func=mutual_info_regression, k='all')
selector_full.fit(X, y)

# Create ranked DataFrame


mi_scores_df = pd.DataFrame({
    'Feature': X.columns,
    'MI_Score': selector_full.scores_


}).sort_values(by='MI_Score', ascending=False)

print(f"\n2.2: Mutual Information Rankings:")


print(f"{'Rank':<6} {'Feature':<30} {'MI Score':<12}")

for idx, row in mi_scores_df.iterrows():
    rank = mi_scores_df.index.get_loc(idx) + 1
    print(f"{rank:<6} {row['Feature']:<30} {row['MI_Score']:<12.4f}")



selectk_results = {}

for k in [5, 8, 10, 12]:
    if k > X.shape[1]:
        continue

    # Select top K features
    selector = SelectKBest(score_func=mutual_info_regression, k=k)
    X_selected = selector.fit_transform(X, y)



    # Get selected feature names
    selected_features = X.columns[selector.get_support()].tolist()

    # Split data
    X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(
        X_selected, y, test_size=0.2, random_state=42
    )

    # Train model

    rf_sel = RandomForestRegressor(
        n_estimators=100,
        max_depth=20,

        min_samples_split=50,
        min_samples_leaf=20,
        random_state=42,
        n_jobs=-1
    )
    rf_sel.fit(X_train_sel, y_train_sel)

    # Evaluate



    y_pred_sel = rf_sel.predict(X_test_sel)
    r2_sel = r2_score(y_test_sel, y_pred_sel)

    selectk_results[k] = {
        'r2': r2_sel,
        'features': selected_features
    }

    reduction = (1 - k/X.shape[1]) * 100

    print(f"\n   K={k} features: R²={r2_sel:.4f} | Reduction={reduction:.1f}%")

    print(f"   Selected: {', '.join(selected_features[:5])}...")

print()



# TECHNIQUE 3: RANDOM FOREST FEATURE IMPORTANCE
print("TECHNIQUE 3: Random Forest Feature Importance")




# Get feature importances from baseline model
feature_importance_df = pd.DataFrame({
    'Feature': X.columns,

    'Importance': rf_baseline.feature_importances_
}).sort_values('Importance', ascending=False)

print(f"\n3.2: Feature Importance Rankings:")


print(f"{'Rank':<6} {'Feature':<30} {'Importance':<12}")

for idx, row in feature_importance_df.head(15).iterrows():
    rank = feature_importance_df.index.get_loc(idx) + 1
    print(f"{rank:<6} {row['Feature']:<30} {row['Importance']:<12.4f}")



rf_importance_results = {}

for k in [5, 8, 10, 12]:
    if k > X.shape[1]:
        continue

    # Select top K features by importance

    top_features = feature_importance_df.head(k)['Feature'].tolist()
    X_top = X[top_features]

    # Split data
    X_train_top, X_test_top, y_train_top, y_test_top = train_test_split(
        X_top, y, test_size=0.2, random_state=42
    )



    # Train model
    rf_top = RandomForestRegressor(
        n_estimators=100,
        max_depth=20,
        min_samples_split=50,

        min_samples_leaf=20,
        random_state=42,
        n_jobs=-1
    )


    rf_top.fit(X_train_top, y_train_top)

    # Evaluate

    y_pred_top = rf_top.predict(X_test_top)
    r2_top = r2_score(y_test_top, y_pred_top)

    rf_importance_results[k] = {
        'r2': r2_top,
        'features': top_features
    }

    reduction = (1 - k/X.shape[1]) * 100


    print(f"\n   Top {k} features: Rsquare={r2_top:.4f} | Reduction={reduction:.1f}%")
    print(f"   Selected: {', '.join(top_features[:5])}...")

print()



# COMPARISON AND ANALYSIS
print("COMPARISON: Which Technique Gave Best Dimensionality Reduction?")


print(f"\n{'Technique':<30} {'K Features':<12} {'Rsquare Score':<12} {'Reduction':<12} {'Performance'}")


# Baseline
print(f"{'BASELINE (All Features)':<30} {X.shape[1]:<12} {r2_baseline:<12.4f} {0.0:<12.1f}% {'Reference'}")

# PCA results


for n_comp, r2 in pca_results.items():

    reduction = (1 - n_comp/X.shape[1]) * 100
    perf_loss = (r2_baseline - r2) / r2_baseline * 100

    status = " Good" if perf_loss < 5 else "Moderate" if perf_loss < 15 else " Poor"
    print(f"{'PCA':<30} {n_comp:<12} {r2:<12.4f} {reduction:<12.1f}% {status}")

# SelectKBest results
for k, result in selectk_results.items():
    reduction = (1 - k/X.shape[1]) * 100
    perf_loss = (r2_baseline - result['r2']) / r2_baseline * 100


    status = " Good" if perf_loss < 5 else " Moderate" if perf_loss < 15 else " Poor"
    print(f"{'SelectKBest (MI)':<30} {k:<12} {result['r2']:<12.4f} {reduction:<12.1f}% {status}")

# RF Importance results

for k, result in rf_importance_results.items():
    reduction = (1 - k/X.shape[1]) * 100
    perf_loss = (r2_baseline - result['r2']) / r2_baseline * 100


    status = " Good" if perf_loss < 5 else " Moderate" if perf_loss < 15 else " Poor"
    print(f"{'RF Feature Importance':<30} {k:<12} {result['r2']:<12.4f} {reduction:<12.1f}% {status}")

# Find best reduction




# Compare techniques at similar feature counts

comparison_k = 8  # Compare all at k=8 features

print(f"\nAt K={comparison_k} features ({(1-comparison_k/X.shape[1])*100:.1f}% reduction):")

if comparison_k in pca_results:


    print(f"   PCA:                    Rsquare = {pca_results[comparison_k]:.4f}")
if comparison_k in selectk_results:
    print(f"   SelectKBest (MI):       Rsquare = {selectk_results[comparison_k]['r2']:.4f}")
if comparison_k in rf_importance_results:
    print(f"   RF Feature Importance:  Rsquare = {rf_importance_results[comparison_k]['r2']:.4f}")

# Determine winner

best_technique = "RF Feature Importance"
best_k = max(rf_importance_results.keys(), key=lambda k: rf_importance_results[k]['r2'])
best_r2 = rf_importance_results[best_k]['r2']


best_reduction = (1 - best_k/X.shape[1]) * 100

print(f"\n BEST TECHNIQUE: {best_technique}")

print(f"   Features used: {best_k}/{X.shape[1]} ({best_reduction:.1f}% reduction)")
print(f"   Rsquare Score: {best_r2:.4f}")

print(f"   Performance drop: {(r2_baseline - best_r2)/r2_baseline*100:.2f}%")

# ANALYSIS: WHY DID THIS TECHNIQUE PERFORM BEST?



print("ANALYSIS: Why Did Different Techniques Perform Differently?")


print(f"""

1. WHY PCA MAY PERFORM WORSE:
   • PCA creates linear combinations of ALL original features

   • New components are harder to interpret (not real features)
   • Assumes linear relationships in variance structure

   • May lose important non-linear patterns that Random Forest exploits
   • Best for: Linear models, high multicollinearity, visualization

2. WHY SELECTKBEST (MUTUAL INFORMATION) PERFORMS WELL:
   • Directly measures dependency between each feature and target

   • Handles non-linear relationships (unlike correlation)
   • Selects ORIGINAL features (interpretable)
   • MI captures what matters for prediction
   • Best for: Non-linear models, feature interpretability



3. WHY RF FEATURE IMPORTANCE PERFORMS BEST:
   • Importance scores from the SAME algorithm we're using (Random Forest)
   • Captures what Random Forest actually finds useful during training
   • Accounts for  interactions between features automatically
   • Considers both direct effects and indirect effects through splits
   • Most aligned with final model's decision-making process
   • Best for: Same algorithm family (tree-based models)

KEY INSIGHT: Feature importance from your model > generic feature selection
""")

# BENEFITS OF DIMENSIONALITY REDUCTION

print("BENEFITS OF DIMENSIONALITY REDUCTION")


print(f"""
1. COMPUTATIONAL EFFICIENCY:
   • Training time: {X.shape[1]} features vs {best_k} features

   • Faster prediction times (critical for production systems)
   • Lower memory requirements
   • Enables larger datasets to fit in memory

2. REDUCED OVERFITTING:
   • Fewer features = less chance to memorize noise
   • Simpler model = better generalization
   • Particularly important with small training sets


3. IMPROVED INTERPRETABILITY:

   • Focus on most important drivers
   • Easier to explain to stakeholders
   • "Production depends on {best_k} key factors" vs "15 factors"

4. COST SAVINGS IN DATA COLLECTION:

   • If collecting new data, only need {best_k} features

   • Example: If wind speed doesn't matter, don't need weather stations
   • Reduced sensor/measurement costs

5. STATISTICAL BENEFITS:
   • Reduces  multicollinearity
   • More stable coefficient estimates
   • Better confidence intervals


IN OUR CASE:
   • Original features: {X.shape[1]}
   • Reduced features: {best_k}
   • Dimensionality  reduction: {best_reduction:.1f}%
   • Performance retained: {(best_r2/r2_baseline)*100:.1f}%

    We can eliminate {X.shape[1] - best_k} features with only {(r2_baseline-best_r2)/r2_baseline*100:.1f}% performance loss!
""")

"""## **Extra Credit Summary**
In this extra credit analysis, I evaluated PCA, SelectKBest (Mutual Information), and Random Forest Feature Importance to determine which method provides the most effective dimensionality reduction. **PCA consistently performed poorly, producing Rsquare scores near 0.30** even when preserving up to 95% of the variance, because PCA's linear components fail to retain the non-linear and interaction-based relationships that Random Forest relies on.

SelectKBest performed much better by identifying individual features with strong non-linear dependencies to the target, producing Rsquare values above 0.82 even with substantial feature reduction.

However, the Random Forest Feature Importance method produced the strongest balance of interpretability
and predictive power. **Using only 8 of the original 9 features, it achieved Rsquare of 0.8312**, nearly identical to the full model, demonstrating that only one feature can be safely removed without harming performance. This method works best because its feature rankings come directly from the same model used for prediction, making the selected feature subset optimally aligned with the underlying algorithm. Overall, dimensionality reduction provided computational benefits, reduced complexity, and increased interpretability while preserving nearly full predictive performance.

4. What algorithm will you use?
* ⭐ This can be an initial idea of which algorithm is appropriate → you may fi nd you want to come back and change it in the last assignment.
  * That is totally ok! If that happens, just document why it didn’t work or why something else would be better.
* What assumptions does this algorithm make?
  * Ex. Does it assume…
    * a specifi c distribution of the data?
    * that there are no outliers?
    * a particular sample size?
    * independence of features?
* Is this algorithm known to be prone to any issues that you should watch out for? Are there well-known mitigations you should use?
  * Ex. Overfi tting in Decision Trees or KNN, etc
* Explain why you think this algorithm is a good candidate for the ML task indicated in #2.
* Tammy recommendation: unless you are experienced or have ample extra time and motivation to dive deep on your own, stick to well-known supervised learning techniques for this course.

## 4. Algorithm Selection

### Primary Algorithm: Random Forest Regressor

I will use Random Forest Regression as my main algorithm for predicting agricultural production.

### What is Random Forest?

Random Forest is an ensemble learning method that works by building many decision trees and combining their predictions. The general idea is:

- It builds multiple decision trees (often between 100 and 1000 trees).
- Each tree is trained on a bootstrap sample. This means the dataset is sampled with replacement to create a slightly different training set for each tree.
- At every split inside a tree, only a random subset of features is considered. A common default is the square root of the total number of features.
- The final prediction is the average of the predictions from all trees.

A group of weak, noisy models can produce a strong and stable prediction when combined.

### Why Random Forest Fits This Project Well

#### 1. Handles Non-Linear Relationships Naturally

My dataset does not follow simple linear patterns. The exploratory analysis showed very weak linear correlations between climate variables and production (less than 0.10 in most cases).

Random Forest is well suited for this kind of data because:

- Decision trees rely on threshold-based splits rather than linear equations.
- They naturally capture non-linear patterns such as:
  - Optimal temperature zones (too cold gives low yield, moderate temperatures give high yield, too hot lowers yield again)
  - Sharp threshold effects (for example, yields dropping above a certain temperature)
  - Different piecewise behavior across crops and countries

This makes Random Forest a strong match for the diverse, complex, and context-dependent relationships present in agricultural production data.


### 2. Automatically Captures Interactions

One challenge in my data is that there are likely many interaction effects between variables. I created a temperature-precipitation interaction feature, but there are probably dozens of other meaningful interactions that are not obvious or easy to engineer manually.

Random Forest is strong in this area because:

- Decision trees naturally capture interactions through their sequence of splits.  
- There is no need to explicitly create interaction terms for every pair or combination of variables.

Because of this structure, the model can discover complex multi-way interactions on its own. For example, it can identify patterns that depend on a combination of country, crop type, temperature, and precipitation. Since the dataset includes 30 countries and 276 crops, manually building all these interactions would be unrealistic. Random Forest handles this automatically as part of the model-building process.

### 3. Handles Mixed Data Types Seamlessly

A major characteristic of my dataset is that it contains both continuous and categorical information. There are continuous features like temperature, precipitation, wind speed, and pressure, and there are categorical features such as country, crop type, and the element indicator variables.

Random Forest works well with this type of data because:

- It can split on both continuous and discrete variables without any special preprocessing.
- Continuous variables are handled through threshold-based splits such as "Is temperature greater than 20".
- Categorical variables that have been encoded as integers can be used directly, for example "Is country equal to 15" (where 15 might represent Argentina after encoding).
- The model does not require feature scaling or normalization.
- Differences in units, such as degrees Celsius, millimeters of precipitation, or kilometers per hour of wind speed, do not affect the algorithm.

In contrast, several alternative algorithms would require more preparation:

- Linear Regression would need standardized features and explicit dummy variables for categoricals.
- Neural Networks would require scaling and more careful handling of categorical inputs.
- Support Vector Machines are highly sensitive to feature scales and would need extensive preprocessing.

These differences make Random Forest a practical and efficient choice for a dataset with mixed feature types.


### 4. Robust to Outliers

A significant issue in my dataset is that production values contain many extreme observations. There are more than ninety thousand outliers, which is about seventeen percent of the data. Production values range from zero up to more than twenty-eight million. Even after applying a log transformation, some unusually large values remain.

Random Forest handles this situation well because of how trees make decisions:

- Trees rely on threshold-based splits, which depend on the ordering of values rather than their exact magnitude.
- A very large outlier is treated the same as any other value that falls above the split point.
- For example, if a split checks whether the value is greater than one hundred thousand, both one hundred fifty thousand and twenty-eight million follow the same path. The extreme magnitude does not distort the split.

The ensemble structure provides even more protection. One tree may overfit a few rare outliers, but averaging across many trees smooths this out.

In contrast, some other algorithms are much more sensitive to extreme values:

- Linear Regression is heavily affected because outliers can pull the regression line toward them.
- K Nearest Neighbors relies on distance calculations, so large values can dominate the neighborhood structure and mislead the model.

For these reasons, Random Forest is a good choice when working with data that contains many outliers.


### 5. Manages High-Dimensional Categorical Variables

The dataset includes several categorical variables with many unique values. There are thirty countries and two hundred seventy-six crop types. Taken together, this creates up to eight thousand two hundred eighty possible country-crop combinations.

Random Forest handles this kind of structure well:

- Label encoding is sufficient, because decision trees can split on integer codes without any trouble.
- There is no need to create hundreds of one-hot dummy variables for each country or crop.
- At each split, the tree considers only a small subset of features. With about fifteen features, the number of features considered at each split is roughly four. This helps prevent the model from overfitting to rare or unusual category combinations.
- The hierarchical nature of tree splits lets the model handle high-cardinality categories in a natural way.

If I used one-hot encoding instead, I would end up with more than three hundred dummy variables. This would make the model slower, inflate the dimensionality, and increase the risk of overfitting.



### 6. Provides Feature Importance Rankings

A key part of my research is understanding which climate variables influence production the most. Random Forest helps with this by producing feature importance scores. These scores reflect how much each feature contributes to reducing prediction error across all trees.

This allows me to answer questions such as:

- Which climate variable is most influential: temperature, precipitation, wind speed, or pressure.
- Whether country-level factors matter more than crop type.
- Whether engineered features, such as squared temperature or the temperature-precipitation interaction, meaningfully improve the model.

These rankings provide a direct way to interpret the model and connect the results back to the scientific questions in the project.


### 7. Large Sample Size is Well-Suited for Random Forest

One advantage of this project is that the dataset contains more than two hundred twenty thousand production observations. Random Forest models tend to perform better as the amount of data increases, because the trees become more stable and the ensemble becomes more reliable.

Random Forest uses bootstrap sampling during training. Each tree is trained on a random sample that contains about sixty-three percent of the total data. With two hundred twenty thousand observations, each tree in a one-hundred-tree forest trains on roughly one hundred thirty thousand samples. This is more than enough to build strong trees while still leaving a large number of out-of-bag samples available for validation.

Compared to other methods:

- K Nearest Neighbors becomes slow and memory-intensive as the dataset grows.
- Deep learning models usually require even more data and careful architecture tuning, which is unnecessary for this tabular dataset.

Overall, the size of the dataset aligns well with the strengths of Random Forest.



## Assumptions Made by Random Forest

### Minimal Assumptions

One of the reasons Random Forest is a strong choice for this project is that it makes far fewer assumptions than most statistical models. The algorithm is flexible and does not require the data to meet strict mathematical conditions.

### 1. No Distributional Assumptions

Random Forest does not assume that the features or the target follow any specific distribution, such as a normal distribution.  
This is important for my dataset because:

- Temperature is roughly normal.
- Precipitation is skewed.
- The log-transformed production values are closer to normal but still not perfect.

Decision trees rely on threshold-based splits, so the shape of the distribution does not affect the model.

### 2. No Linearity Assumption

Random Forest does not require linear relationships between the features and the target.  
This is helpful because my exploratory analysis showed that climate variables have very weak linear correlations with production. The model can still capture complex and non-linear patterns through the tree structure.

### 3. No Homoscedasticity Assumption

The algorithm does not assume that the variance of errors is constant across all levels of the target.  
Each tree predicts independently, and the averaging process stabilizes the overall predictions.  
This matters because production levels vary widely across countries and crop types, and the variance is not uniform.

### 4. No Feature Independence Assumption

Random Forest does not require the features to be uncorrelated.  
It can work effectively even when some features share moderate correlations.  
Feature subsampling also reduces the impact of correlations by ensuring that different trees consider different subsets of variables.  
In my data, temperature and precipitation have a correlation of about 0.36, which is not a problem for the model.

### 5. No Requirement for Outlier-Free Data

Random Forest does not rely on distance metrics or linear equations, so it is not sensitive to outliers.  
This is useful because production data contains many extreme values.  
The algorithm focuses on rank-based splits, meaning outliers do not distort decision boundaries the way they would in linear models.

## Implicit Assumptions to Be Aware Of

### 1. IID Assumption (Independent and Identically Distributed)

The IID assumption states that observations should be independent of each other and drawn from the same underlying distribution.  
This assumption is not fully satisfied in my dataset for several reasons:

- Temporal autocorrelation: Production in one year influences production in the following year for the same country and crop.
- Spatial correlation: Neighboring countries, such as Argentina and Uruguay, share similar climate conditions and agricultural practices.
- Non-identical distributions: Different countries and different crops have their own production scales and behaviors, so the data is not drawn from a single distribution.

### Impact on Random Forest

- Prediction uncertainty may be understated.
- Out-of-bag error estimates may look better than they truly are.
- Feature importance metrics may put too much weight on temporal or spatial patterns.

### Mitigation Strategies

- Stratified splitting: Make sure both the training and testing sets contain all countries.
- Time-based split: Train on data from 1961 to 2010 and test on data from 2011 to 2023 to assess temporal generalization.
- Clustered cross-validation: Group by country and crop to avoid data leakage.
- Interpretation caution: The results reflect historical patterns rather than causal relationships.


### 2. Sufficient Sample Size per Leaf

Random Forest assumes that each leaf node contains enough observations to make stable and reliable predictions.  
This can be an issue in my dataset because some combinations of country and crop occur only a few times.

For example:

- There are 30 countries and 276 crops.
- This creates up to 8,280 possible country-crop combinations.
- With 220,000 production observations, the average is about 27 observations per combination.
- Some combinations, such as Grenada with certain crops, appear even less often.

This means that if the trees are allowed to grow too deep, some leaf nodes may end up with very few samples. That would lead to unstable predictions and overfitting to rare patterns.

### Mitigation

To avoid this problem, I plan to set:

- min_samples_leaf = 20, which prevents leaf nodes from forming with fewer than twenty observations.
- min_samples_split = 50, which requires at least fifty observations before the algorithm attempts to split a node.

These settings help ensure that each leaf is supported by enough data and that the model does not overfit to rare or unusual country-crop combinations.


### 3. Feature Relevance

Random Forest assumes that the features provided to the model contain useful information for predicting the target.  
If irrelevant features are included, they may add noise or dilute the importance of meaningful variables.

In my dataset, this matters because some features, such as atmospheric pressure, may or may not contribute much to explaining production levels.  
If a feature does not influence the outcome, it can reduce the clarity of the feature importance rankings and make the model slightly less efficient.

### How Random Forest Handles Irrelevant Features

Random Forest is generally robust to irrelevant features because:

- Each split considers only a random subset of features.  
  Irrelevant variables are often ignored during splitting.
- Features that do not improve prediction accuracy naturally receive low importance scores.  
  This makes them easy to identify and potentially remove later.

However, the model still works best when most features are relevant.  
In this project, the chosen features are supported by domain knowledge, so the risk of including completely irrelevant variables is low.


## Known Issues with Random Forest and Mitigations

### Issue 1: Overfitting

A common problem with Random Forest is that it can overfit, especially when the individual trees are very deep.  
With a dataset that includes hundreds of crops and dozens of countries, the model might start memorizing specific historical values instead of learning general patterns.

For example, instead of learning the relationship between climate and sugarcane yields in Brazil, an overly deep tree might simply store:

- Brazil, Sugarcane, 2015 -> 5000000
- Brazil, Sugarcane, 2016 -> 5200000

This kind of memorization does not help with prediction on new data.

### How to Detect Overfitting

A clear sign is when the training accuracy is much higher than the test accuracy.  
For instance:

- Training R squared: 0.99  
- Testing R squared: 0.60  

A gap this large would indicate that the model has learned the noise instead of the underlying relationships.

### Mitigation Strategies

Several hyperparameter settings can reduce overfitting:

- Limit tree depth by setting max_depth = 20. This prevents trees from growing too complex.
- Use min_samples_split = 50 so that a node needs at least fifty observations before a split is allowed.
- Use min_samples_leaf = 20 to prevent leaves with very few samples.
- Leave bootstrap sampling enabled so each tree trains on a different random subset of the data.
- Use feature subsampling by setting max_features = "sqrt", which means each split uses only the square root of the total number of features. With about fifteen features, each split considers roughly four randomly chosen features.
- Apply five-fold cross-validation on the training data to tune the hyperparameters and check for overfitting.
- Monitor out-of-bag error, which provides an internal estimate of performance. If adding more trees does not improve this metric, additional trees are unnecessary.

These steps help prevent the model from memorizing the training data and improve its ability to generalize to new observations.


### Issue 2: Extrapolation Beyond the Training Range

Random Forest cannot extrapolate outside the range of values it sees during training.  
Decision trees can only make predictions based on the splits they have learned. As a result, if a value appears in new data that is outside the historical range, the model will simply return the prediction from the closest known value.

For example, if the training data contains temperatures between 5 and 30 degrees, the model cannot determine what should happen at 35 degrees. It will follow the branch for the closest temperature it has seen, which may be around 30 degrees.

### Why This Matters for This Project

My goal includes predicting agricultural production under future climate scenarios, such as a two degree temperature increase by 2040.  
If these future temperatures exceed the maximum values found in the historical data, the predictions will not reflect true extrapolated behavior. They will simply reuse patterns from the highest temperatures available in the training set.

A simple example illustrates this problem:

- Historical pattern: Brazil sugarcane shows a certain yield behavior at an average temperature of 28 degrees.
- Future scenario: The projected average temperature is 30 degrees.
- If the training data has no records at 30 degrees, the model will treat 30 as if it were 28, because it cannot infer the trend beyond what it has seen.

### Mitigation Strategies

1. **Check whether prediction inputs fall within the training range.**  
   The following code checks if the test data contains values outside the training range for each numeric feature:
"""

# # Verify test set is within training ranges
# for col in numeric_features:
#     train_min, train_max = X_train[col].min(), X_train[col].max()
#     test_min, test_max = X_test[col].min(), X_test[col].max()
#     if test_min < train_min or test_max > train_max:
#         print(f"WARNING: {col} in test set exceeds training range")

"""### Acknowledging the Limitation in Interpretation

Because Random Forest cannot extrapolate beyond the highest and lowest values seen during training, it is important to be clear about what the model can and cannot predict.

Examples:

- The model provides reliable predictions only within the historical climate range of approximately 5 to 30 degrees.
- Predictions for climate conditions that fall outside this range should be treated with caution, because the model will not infer new behavior beyond what it has already observed.

### Considering a Hybrid Approach for Long-Term Climate Projections

Random Forest remains a strong choice for short- and medium-term forecasting as long as future climate inputs fall within the historical range used for training.

For more extreme future scenarios, a hybrid strategy may be appropriate:

- Use Random Forest for predictions where the temperature and other climate variables are still within the bounds of the historical data.
- For scenarios that exceed those boundaries, combine Random Forest with process-based crop models, which are specifically designed to simulate plant behavior under conditions that have not occurred in the past.


### Issue 3: Memory and Computation Time

Random Forest can require substantial memory and computation time, especially when the dataset is large and the number of trees is high. Each decision tree stores its full structure, and training many trees can take time.

In this project, the dataset size is manageable:

- About 220,000 rows and 15 features
- Roughly 100 trees in the forest
- Each tree typically has around one thousand nodes
- This results in an estimated one hundred thousand nodes stored in memory across the entire model

This is not excessively large, but it is still something to be aware of when running the model on typical hardware.

### Mitigation Strategies

Several steps help keep training time and memory use under control:

- Use n_jobs = -1 to take advantage of all available CPU cores during training.
- Start with a smaller number of trees, such as fifty to one hundred, during the experimentation phase. Increase the number only when the model is stable.
- On a standard laptop, training with these settings is expected to take between two and five minutes, which is acceptable for this project.


### Issue 4: Black Box Interpretability

Random Forest models are harder to interpret than simple linear models.  
With linear regression, you can write an equation such as:

y = b0 + b1 * x1 + b2 * x2

and directly see how each feature influences the prediction.  
A Random Forest model, however, is made up of many decision trees, so there is no single equation or simple explanation. This makes it harder to answer questions like "Why did the model predict this value?"

### Why This Matters

For this project, interpretability is important because:

- Farmers and policymakers need to understand why climate factors affect production.
- Research requires clear explanations and insights, not just numerical predictions.
- Understanding which variables are most important helps validate the scientific reasoning behind the model.

### Mitigations

#### Feature Importance Plots

Feature importance scores show how much each variable contributes to the model's predictive accuracy.

Example code:





"""

# importances = model.feature_importances_
# plt.barh(feature_names, importances)
# plt.title("Feature Importance")
# plt.show()

"""#### Partial Dependence Plots (PDP)

Partial dependence plots help visualize how the model's prediction changes as one feature varies, while all other features are held constant.  
This is useful for understanding the relationship between climate variables and predicted production.

Example code:




"""

# from sklearn.inspection import PartialDependenceDisplay
# PartialDependenceDisplay.from_estimator(model, X_train, ['avg_temp_c'])
# plt.show()

"""This plot helps answer questions such as:

- As temperature increases, how does the predicted production change
- Does the model show an optimal temperature range for specific crops
- Are there points where production begins to decline when temperature becomes too high


### Issue 5: Imbalanced Data

The dataset has an uneven distribution of observations across countries and crops.  
For example:

- Mexico has more than thirty thousand observations.
- Several small Caribbean countries have fewer than one thousand observations each.

Because of this imbalance, the model may learn patterns for large countries very well but perform poorly on smaller countries that have limited data.

### Impact

- Predictions for underrepresented countries may be less accurate.
- Feature importance may be biased toward groups with more observations.

### Mitigation Strategies

#### 1. Stratified Train-Test Split

To ensure all countries appear in both the training and testing sets, a stratified split can be used:


"""

# from sklearn.model_selection import train_test_split

# X_train, X_test, y_train, y_test = train_test_split(
#     X, y, test_size=0.2, stratify=df["country_encoded"], random_state=42
# )

"""This prevents the model from being trained only on the majority groups.

#### 2. Sample Weights (Optional)

Sample weighting can give more influence to countries with fewer observations.  
This approach tells the model to pay more attention to underrepresented groups during training, so their patterns are not overwhelmed by countries with larger datasets.

"""

# from sklearn.utils.class_weight import compute_sample_weight

# weights = compute_sample_weight("balanced", y=df["country_encoded"])
# model.fit(X_train, y_train, sample_weight=weights)

"""3. Report Performance by Subgroup

To understand how well the model performs on each country, performance can be evaluated separately:
"""

# # Calculate R2 score separately for each country
# for country in countries:
#     subset = test_data[test_data["country_encoded"] == country]
#     country_r2 = r2_score(subset["y_true"], subset["y_pred"])
#     print(f"{country}: R2 = {country_r2:.3f}")

"""## Why Random Forest Is the Best Choice for This Project

### Compared to Linear Regression

Linear Regression assumes simple linear relationships between features and the target.  
My exploratory analysis showed that climate variables have almost no linear correlation with production, so a linear model would not capture the real patterns in the data.

Key differences:

- Linear Regression cannot model non-linear effects or interactions unless they are manually added.
- It is sensitive to outliers and requires extensive preprocessing.
- It handles categorical variables poorly without many dummy variables.

Random Forest, on the other hand:

- Captures non-linear relationships naturally.
- Models interactions automatically.
- Handles outliers and mixed data types well.
- Performs better when relationships are complex.

**Conclusion:** Linear Regression is not appropriate for this dataset.


### Compared to K-Nearest Neighbors (KNN)

KNN becomes very slow with large datasets because it must compute distances to all training points during prediction.  
With more than two hundred thousand observations, this is not practical.

Key differences:

- KNN scales poorly with large datasets.
- It does not handle high-dimensional or categorical data well.
- It offers no feature importance or interpretability tools.

Random Forest:

- Scales well to large datasets.
- Handles categorical and continuous variables easily.
- Provides feature importance and interpretation tools.

**Conclusion:** KNN is not suitable due to speed and data type limitations.



### Compared to Gradient Boosting (XGBoost or LightGBM)

Gradient boosting methods can achieve very high accuracy but require more careful tuning.  
They are more sensitive to hyperparameters and can overfit if not tuned properly.

Key differences:

- Gradient boosting tends to be slower to train because trees are built sequentially.
- Overfitting risk is higher without careful regularization.
- Requires more expertise and tuning to get peak performance.

Random Forest:

- Trains faster because trees are built in parallel.
- Has fewer hyperparameters and is more robust by default.
- Provides accuracy that is competitive for most tabular problems.

**Conclusion:** Gradient boosting is a good option later, but Random Forest is simpler, safer, and easier to use for the first version of the model.



### Compared to Neural Networks

Neural networks require much more data and more preprocessing.  
They are harder to tune and are less interpretable.

Key differences:

- Neural networks need normalized features and careful architecture choices.
- Training is slower and usually requires a GPU.
- Interpretability is limited without advanced tools.

Random Forest:

- Works well with the current dataset size.
- Needs minimal preprocessing.
- Provides interpretable outputs through feature importance and partial dependence plots.

**Conclusion:** Neural networks add unnecessary complexity for this type of tabular data.


## Final Justification: Why Random Forest Fits This Project

Random Forest aligns well with both the data and the research goals.

### Match with Data Characteristics

- Handles non-linear relationships that were confirmed by exploratory analysis.
- Naturally separates data by country and crop, matching the structure revealed in Simpson's paradox.
- Captures interaction effects such as temperature combined with precipitation.
- Works with mixed data types, including continuous climate variables and encoded categorical variables.
- Scales well to more than two hundred thousand observations.
- Is robust to outliers and high-cardinality categories.

### Match with Project Goals

- Predicts production accurately for climate scenarios.
- Provides feature importance for understanding which climate factors matter most.
- Supports interpretability through partial dependence and feature importance plots.
- Generalizes well across diverse countries and crops.



---

5. What are the hyperparameters for your algorithm and how will you choose/tune them? Choose them.
* Ex. k in KNN, maximum tree depth in DTs, etc

## Key Hyperparameters for Random Forest Regresssor

Random Forest has several important hyperparameters that control model complexity, training behaviour, and prediction quality. Here are the critical ones for my project:

##Primary Hyperparameters

1.`n_estimators` (number of trees)

What it controls: How many decision trees to build in the forest

Options:
* Too few (e.g., 10) : High Variance, unstable predictions
* Too many (e.g., 1000) :Diminishing returns, slower training/ predictions
* Typical range: 50-500

**My Choice `n_estimators`= 100**
**Resoning:**
* Industry standard starting point
* Sufficient for capturing diverse patterns in 220k observations
* Computational cost is managable (2-5 minutes training)
* Research shows performance plateaus around 100-200 trees for most datasets
* I can increase if out-of-bag error hasn't plateaued
"""

# # Plot OOB error vs number of trees


# oob_errors = []

# for n in range(10, 201, 10):
#     rf = RandomForestRegressor(n_estimators=n, oob_score=True, random_state=42)
#     rf.fit(X_train, y_train)

#     oob_errors.append(1 - rf.oob_score_)
# plt.plot(range(10, 201, 10), oob_errors)

# plt.xlabel('Number of Trees')
# plt.ylabel('OOB Error')

# plt.title('Optimal Number of Trees')

"""2`max_depth` (Maximum tree depth)
What it controls : How deep each tree can grow
OPtions:
* `none` : Tree grows untill pure leaves (high overfitting risk)
* Too shallow (e.g., 5) : Underfitting, can't capture complex patterns
* Too deep (e.g., 50): Overfitting, memorizing training data
* Typical Range: 10-30

**My Choice: `max_depth =20`**

Reasoning:

* Deep enough to capture complex interactions (country * crop * climate)
* shallow enough to prevent memorization of specific year-country-crop combinations
* With 15 features, depth of 20 allows exploring many feature combinations
* Research on agricultural dataset suggests 15-25 is optimal

Why not unlimited?

* My data has temporal structure (same country-crop over 63 years)
* Unlimited depth could memorize: "Brazil-sugar-2015 = exactly 5,234,567"
* I want to learn: "Brazil-sugar at 27 Degree C with 8mm rain = 5,200,000"


3.`min_samples_split` (Minimum Samples to Split)

What it controls: Minimum number of samples required to split an internal node

Options:

* Too low (e.g., 2): Every node splits, severe overfitting
* Too high (e.g., 500) : Tree can't specilize, underfitting
* Typical range: 20-100

**My Choice: `min_sample_split=50`**

Reasoning:
* With 220k training samples, requirung 50 for a split ensures statistical significance
* Prevents splitting on noise or rare combinations
* Rule of thumb: 0.1-0.5% of training data -> 200k * 0.023 % = 50

Balances:

* Too restrictive -> can't capture niche patterns
* Too permissive -> overfirs to outliers


4.`min_sample_leaf` (Minimum Samples per leaf)

**What it controls: Minimum number of samples required in a leaf node**

Options:

* Too low (e.g., 1 ) : Single-sample leaves, extreme overfitting
* Too high (e.g., 200) : Can't make specific prediction
* Typical range: 10-50

**My Choice `min_sample_leaf = 20`**

Reasoning:

* Each prediction should be based on at least 20 observations for reliability
* Prevents leaves like : "If country = Grenada AND crop = Wheat AND year =2015 -> Predict X"
* Ensures each leaf represents a generalizable pattern, not a single data point
* With 8,280 possible country-crop combinations, requiring 20/leaf prevents overspecialization

Statistical Justification:

* Central Limit Theorem suggests n=20-30 for relible mean estimation
* Smaller leaves have high variance, large leaves have high bias
* 20 is a good middle ground

5.`max_features`(Features per Split)

What it controls: Number of features to consider when looking for the best split

Options:
* `sqrt`: square root of (n_feature)
* `log2` : log to the base 2 of (n_feature)
* `int` : Specific number
* `auto`: Same as 'sqrt' for regression
* `None`: All features (no randomness, defeats ensemble purpose)

**My Choice: `max_feature='sqrt'`** (approx. 4 feature per split)

Reasoning:
* Decorrelates Trees: Each tree sees different feature subset -> diverse predictions -> better ensemble
* Reduces overfitting: Prevents dominant features (like `country_encoded`) from being used at every split
* Standard for regression : sklearn's default for regression is 'auto' = 'sqrt'
* Computational efficiency : Evaluating 4 features is 3.75 * fater than all 15

Why this matters:

* If I used all 15 features at each split, all trees would make similar splits -> ensemble adds little value

* Random feature subsampling is key innovation that makes Random Forest work


6.`random_state`(Random Seed)

What it controls: seed for random number generator (ensures reproducibility)

**My choice:`random_state = 42`**

Reasoning:
* Make results reproducible (same train-test split, same bootstrap samples, same feature subsamples)
* Standard practice in data science (42 is a common choice)
* Essential for debugging and comparing different models
* Doesnt affect model performance, only reproducibility

7. `n_jobs`(Parallel Processing)

What it controls: Number of CPU cores to use for training

OPtions:

* `1`: Single core (slow)
* `4` : Use 4 cores
* `-1` : Use all available cores (fastest)

**My Choice `n_jobs= -1`**

Reasoning:
* Random forest is embarrassingly parallel (each tree trains independently)
* Reduces training time from 5 minutes -> 1-2 minutes
* No downside (doesn't affect model quality)



8.`oob_score` (Out-of-bag Scoring)

What it controls: Whether to calculate out-of-bag R square score during training

Options:

* `True`: Calculate OOB score (free validation metric)
* `False:` Don't calculate (slightly faster)

My Choice : `oob_score = True`

Reasoning:

* Free validation Each tree is trained on ~ 63 % of data, tested on remaining 37 %
* Provides estimate of generalization performance withour needing seperate validation set
* Useful for detecting overfitting : If OOB score << train score -> overfir
* Helps decide if I need more trees: If OOB error still decreasing -> add more trees
* Minimal computational cost

##**My Final Hyperparameter Configuration**



"""

# from sklearn.ensemble import RandomForestRegressor

# model = RandomForestRegressor(

#     n_estimators=100,           # 100 trees in the forest
#     max_depth=20,               # Limit tree depth to prevent overfitting

#     min_samples_split=50,       # Need 50 samples to consider splitting
#     min_samples_leaf=20,        # Each leaf must have at least 20 samples

#     max_features='sqrt',        # Consider 15^-2 ≈ 4 features per split
#     random_state=42,             # Reproducibility
#     n_jobs=-1,                  # Use all CPU cores

#     oob_score=True,             # Calculate out-of-bag score
#     bootstrap=True,             # Use bootstrap sampling (default, but explicit)
#     verbose=1                    # Print progress during training
# )

"""## Hyperparameter Tuning Strategy

Approach : Start Simple, Then Optimize

Rather than immediately doing exhaustive grid search (which could take hours), I'll use progressive tuning strategy:

## Step 1: Baseline Model (Default Hyperparameters)


"""

#  # Train with my chosen parameters above
# model.fit(X_train, y_train)

# # Evaluate baseline
# train_score = model.score(X_train, y_train)
# test_score = model.score(X_test, y_test)
# oob_score = model.oob_score_

# print(f"Baseline Results:")
# print(f"  Train R²: {train_score:.4f}")
# print(f"  Test R²:  {test_score:.4f}")
# print(f"  OOB R²:   {oob_score:.4f}")
# print(f"  Overfit gap: {train_score - test_score:.4f}")

"""Success Criteria:

* Test R square > 0.70 (explains 70%+ variance)
* Overfit gap < 0.10 (train-test difference small)
* OOB ~ Test (validates that OOB is reliable)

 Step 2: Quick Manual Tuning (if needed)

 If baseline shows issues, I'll adjust:

 If OVERFITTING (train Rsquare >> test Rsquare):

 * Decrease `max_depth`: 20 -> 15 -> 10
 * Increase `min_sample_leaf`: 20 -> 30 -> 50
 * Increase `min_sample_split`: 50 - > 100 -> 200

 If UNDERFITTING (both train and test Rsquare low):
 * Increase `max_depth`: 20 -> 25 -> 30
 * Decrease `min_sample_leaf`: 20 -> 10 ->5
 * Increase `n_estimators`: 100 -> 200 -> 500

 If UNSTABLE (high variance in predictions):
 * Increase `n_estimators`: 100 -> 200 -> 500
 * Decrease `max_features`: 'sqrt' -> 'log2' -> 3

 Step 3: Randomized Search CV (if time permits)

 For more systematic tuning:


"""

#  from sklearn.model_selection import RandomizedSearchCV
# from scipy.stats import randint, uniform

# # Define hyperparameter distributions


# param_distributions = {
#     'n_estimators': [50, 100, 150, 200],

#     'max_depth': [10, 15, 20, 25, 30],
#     'min_samples_split': [20, 50, 100],

#     'min_samples_leaf': [10, 20, 30, 50],

#     'max_features': ['sqrt', 'log2', 0.5]
# }

# # Randomized search (faster than grid search)

# random_search = RandomizedSearchCV(

#     estimator=RandomForestRegressor(random_state=42, n_jobs=-1),
#     param_distributions=param_distributions,

#     n_iter=20,                  # Try 20 random combinations

#     cv=3,                         # 3-fold cross-validation

#     scoring='r2',               # Optimize for R²
#     verbose=2,
#     random_state=42,
#     n_jobs=-1
# )

# random_search.fit(X_train, y_train)

# print("Best hyperparameters found:")

# print(random_search.best_params_)
# print(f"Best CV R²: {random_search.best_score_:.4f}")

# # Use best model



# model = random_search.best_estimator_

"""## Why Randomized Search Instead of Grid Search

Randomized search is more efficient than grid search for a dataset of this size.

- Faster: tests only 20 random combinations instead of hundreds of grid combinations.
- Often better: research shows randomized search finds strong hyperparameters without testing every possibility.
- Practical: 220k samples, 3 folds, and 20 random trials is manageable on typical hardware.

I will only run randomized search if the baseline model performs poorly. This saves time.



## Justification for Each Hyperparameter Choice

### Summary Table

| Hyperparameter | Selected Value | Reasoning |
|----------------|----------------|-----------|
| n_estimators = 100 | Enough trees without excessive cost | Balances accuracy and speed |
| max_depth = 20 | Captures complex structure but avoids memorization | Prevents overfitting |
| min_samples_split = 50 | Requires meaningful evidence to split | Reduces noise-based splits |
| min_samples_leaf = 20 | Ensures stable predictions | Avoids tiny, unstable leaves |
| max_features = "sqrt" | Uses about 4 features per split | Creates diverse trees |
| random_state = 42 | Makes results reproducible | Standard practice |
| n_jobs = -1 | Uses all CPU cores | Faster training |
| oob_score = True | Free validation during training | Helps detect overfitting |



## Expected Impact of Hyperparameters

### Bias and Variance Balance

More restrictive settings (higher bias, lower variance):

- Lower max_depth -> simpler trees and less overfitting  
- Higher min_samples_split -> fewer splits and more generalization  
- Higher min_samples_leaf -> smoother predictions  
- Lower max_features -> more randomness and less tree correlation  

Less restrictive settings (lower bias, higher variance):

- Higher max_depth -> more complex trees that may overfit  
- Lower min_samples_split -> splits on weaker signals  
- Lower min_samples_leaf -> very specific predictions  
- Higher max_features -> trees become more similar  

My chosen values sit in the middle. They allow the model to learn detailed patterns while still avoiding overfitting.



## How These Hyperparameters Address the Dataset Challenges

### Challenge 1: Simpson's Paradox and Context Effects

Agricultural production depends heavily on country and crop context.  
A temperature of 15 degrees may mean one thing for Canada and something entirely different for Brazil.

The depth and split constraints allow this structure to emerge naturally:

- max_depth = 20 provides enough depth for country and crop specific branches.
- min_samples_split and min_samples_leaf prevent overly specific rules based on very small groups.

Example of what the model can learn:

If country = Canada  
 If crop = Wheat  
  If temperature is low: predict a lower value  
  Else: predict a higher value  

If country = Brazil  
 If crop = Wheat  
  If temperature is low: prediction differs from the Canada branch because the climate context is different.

The model separates these cases without memorizing specific years or exact values.

Challenge 2: High Cardinality Categoricals (276 crops, 30 countries)
Solution:
* `min_sample_leaf=20` prevents overfitting to rare combinations
* If only 15 observations of "Grenada-Papaya" exits, model won't create dedicated leaf
* Instead, Grenada- Papaya shares a leaf with similar Caribbean-tropical fruit combinations


Challenge 3: Temporal Autocorrelation

Solution:
* `bootstrap = True` with `oob_score=True` helps detect if model is just memorizing year-to-year patterns
* If OOB score is much lower than train score -> model is exploiting temporal structures rather than learning climate relationships
* This would signal need for time-based cross-validation

Challenge 4: Outliers (17.6 % of data)

Solution:
* Random Forest's rank-based splits naturally handle outliers
* `min_samples_leaf=20` ensures predictions aren't based on single extreme values
* Ensemble averaging (100 trees) smooths out individual tree predictions influenced by outliers


## Monitoring and Validation Plan

During Training:



"""

# # Monitor OOB error convergence
# print(f"OOB Rsquare score: {model.oob_score_:.4f}")

# # Check feature importance

# importances = model.feature_importances_
# for name, importance in zip(feature_names, importances):

#     print(f"{name:30s}: {importance:.4f}")

"""After Training:"""

# # Check for overfitting
# train_r2 = model.score(X_train, y_train)
# test_r2 = model.score(X_test, y_test)

# print(f"Train R²: {train_r2:.4f}")

# print(f"Test R²:  {test_r2:.4f}")
# print(f"Overfit gap: {train_r2 - test_r2:.4f}")



# # Acceptable: gap < 0.05 (minimal overfit)
# # Warning: gap 0.05-0.15 (moderate overfit)

# # Problem: gap > 0.15 (severe overfit, need to adjust)

"""##Contingency Plans

If Performance is Poor (R square < 0.70), I'll:
1. Check data leakage: Ensure test set has no counties/crops not in training
2. Try more trees: Increase to 200-500
3. Relax constraints: Increase `max_depth` to 30, decrease `min_samples_leaf` to 10
4. Try XGBoost: Switch Algorithm

If overfitting is severe (gap > 0.15), I'll :

1. Tighten constraints : Decrease `max_depth` to 15, increase `min_samples_leaf` to 50
2. Reduce features: Remove least important features based on feature_importance_
3. Use time-based split: Train 1961 - 2010, test on 2011-2023 (harder test)

If Training is Too slow (> 10 minutes), I'll:
1. Reduce trees: 100 -> 50 (small accuracy loss, 2x speedup)
2. Subsample data: Use 50% of training data (with `max_samples=0.5`)
3. Reduce depth: 20 -> 15 (faster, less overfitting risk)

---

##6. Are there any post-processing techniques you may need to apply?
* ex. Post-pruning a Decision Tree
* Why or why not?
* If so, what are they?

## **Yes, I will Need Post-Processing**

Based on my feature engineering choices and the nature of my prediction task, I need to apply two critical post-processing techniques:

## Post-Processing Technique #1: Inverse Log Transformation

What it is:

Converting log-scale predictions back to original scale

Why it's necessary:

In Step 3 : I log-transformed my target variable

The problem:
* My model will be trained to predict `log_value`
* But I need predictions in original units (tonnes, hectares, etc.)
* Predictions like "log_value=8.5" are meaningless to farmers and policymakers
* Need to convert: "log_value =8.5" -> "value =4.914 tonnes"

Mathematical relationship:

* Forward : `log_value = log (1+ value)`
* Inverse : `value = exp(log_value) -1`
* NumPy functions: `np.expm1(x) = exp (x) -1 (numerically stable)


Why `log1p` and `expm1` instead of plain `log` and `exp`?

Numerical stability for small values:

* My dataset has production values starting at 0
* `log(0)` = undefined
* `log (0.001)` = -6.9 (creates huge negative numbers)
* `log1p(0)` = `log(1+0)` = `log(1)` = 0


Why values differ:
* Log Transformation compresses large values, expands small values
* Good predictions in log-space is not equal to good predictions in original space
* Example :
  * Actual : 1,000,000 tonnes
  * Prediction A: 900,000 tonnes -> Error: 100,000 (10%)
  * Prediction B: 500,000 tonnes -> Error : 500,000 (50%)
  * In log-space: log(1M)= 13.82, log(900k) = 13.71, log(500k)=13.12
    * Error A: 0.11 log units
    * Error B: 0.70 log units

  
  * Log-space exaggerates errors on large values less -> can be misleading


## **Post-Processing Technique 2 : Prediction Clipping / Constraints**

What it is:

Ensuring predictions satisfy domain constraints

Why it's necessary:
Domain constraint: Agricultural production cannot be negative

The problem:
* After inverse log transform, numerical errors might produce small negative values
* Especially for very small production value (e.g., value = 0.01)
* Example: `expm1(-0.001) = -0.001(slightly negative)

Also: Production values have realistic upper bounds

* A single country can't produce 1 billion tonnes of wheat in one year

* Physically impossible given land area, growing season, technology


Should I set a maximum value?

Arguements for:
* Prevents absurd predictions (e.g., 100 billion tonnes)
* Could use historical maximum as ceiling

Arguements against:
* Future production might legitimately exceed historical max (due to technology)
* Clipping creates bias (all predictions above threshold become identical)
* Better to let model learn realistic ranges naturally

My decision: Minimum only, no maximum

Reasoning:

* Negative production is physically impossible -> must clip
* Maximum production is uncertain (techology advancing) -> don't clip
* If model predicts absurdly high values, that indicates a modeling problem (e.g., extrapolation) that should be investigated, not hidden by clipping

##Post-Processing Technique 3: Prediction Interval Estimation

What it is:

Providing uncertainty estimates for predictions (e.g., "1,000 +/- 200 tonnes")

Why it's useful:

Problem: Point predictions don't convey uncertainy
* Model predicts : "5,000 tonnes"
* But how confident are we? Could it be 4,500? 6,000? 1,000?

Value for stakeholders:

* Farmers: "Plant 100 hectare +/- 20 hectares depending on confidence"
* Policymakers : "Budget for 1M tonnes +/- 300k tonnes"
* Insurance  : Price based on risk (wide interval = high uncertainity = higher premium)

How Random FOrest Provides uncertainity:

Method 1: Standard Deviation Across Trees

Interpretation:
* Narrow interval (low std) : Model is confident (many trees agree)
  * Usually for common country-crop combination with lots of training data
* Wide interval (high std): Model is uncertain (trees disagree)
  * Usually for rare combination or extrapolation beyond training range

When uncertainity is high (red flag for caution):
* Rare country-crop combinations (e.g., "Canada-Papaya")
* Extreme climate values outside training range (e.g., 35 C where max training was 30 C)
* Years far in future (2040 predictions less reliable than 2025)


##Why No Other Post-Processing is Needed

**Post-Pruning (e.g., for decision Trees)-Not needed**

What it is: Removing branches from a decision tree after training to reduce overfitting

Why I don't need it:
* Random Forest doesn't require post-pruning because:
  1. Ensemble averaging already reduces overfitting (100 trees smooth out individual tree overfitting)
  2. I'm already doing pre-pruning during training via:
    * `max_depth=20` (depth limit)
    * `min_samples_split=50` (split threshold)
    * `min_samples_leaf=20` (leaf threshold)
  3. Bootstrap sampling means each tree is trained on different data (reduces overfitting)


**When post-pruning is used:**
* Single decision trees (not ensembles)
* Trees trained without depth/sample constraints
* After discovering that tree is too complex

**My case:** Pre-pruning during training is sufficient and more efficient




**Calibration- Not Needed**

What it is: Adjusting predicted probablities to match true probabilities

Why I don't need it:

* This is for classification problems, not regression
* Used when you need well-calibrated probability estimates (e.g., P(disease) = 0.7 means "70% of similar patients have disease")
* My problem: Predicting continuous production values, not probabilities


**Ensembling/Stacking-Not Needed(yet)**

What it is: Combining Multiple different models

Why I don't need it now:

* Random Forest is already an ensemble (100 Trees)
* Adding complexity without evidence it's needed
* Should first evaluate single model performance

When I might use it:

* If Random Forest alone gives Rsquare < 0.70
* Could train XGBoost and stack with Random Forest



**Feature Transformation After Training- Not Needed**

What it is: Creating new features based on model predictions

Examples:

* Using residuals as features
* Creating "difficulty" scores based on prediction confidence
* Feature engineering based on feature importance

Why I don't need it:

* This is iterative modelin (train model -> create features -> retrain)
* Appropriate for competition settings or production systems
* Overkill for Project
* My engineered features are already sufficient.


**Why These Post-Processing Steps Are Essential**

Scientific Intergrity:
* Predictions must be in meaningful units for interpretation
* "Log-value= 10.5" tells stakeholders nothing
* "Production = 36,315 tonnes" is actionable information

Domain Validity:

* Negative production is impossible -> must constrain
* Uncertainity quantification enables risk-based decision making


Methodological Rigor:

* Evaluating in log-scale would misrepresent model performance
* Must evaluate in scale that matters to stakeholders (original units )

Practical Utility:
* Farmers need: "Plant X hectares to produce Y +/- Z tonnes"
* Policymakers need: "Budget for imports if production < lower bound"
* Researchers need: "Model uncertainity increases for rare crops"

7. Run ML code
* Split your data into the appropriate training and test sets (assuming supervised ML)
* Run your algorithm
* Fit the model (if applicable)
* Perform any needed post-processing
* Make predictions on your test dataset
* Print the accuracy
  * Note: There is no need to analyze metrics yet (that will happen in the next project). Just print the accuracy for now.
"""

import pandas as pd

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# STEP 1: LOAD CLEANED DATA

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DATA 602 Project/final_cleaned_data.csv')

print(f"Dataset shape: {df.shape}")

print(f"Columns: {list(df.columns)}")

print()


# STEP 2: FEATURE ENGINEERING
# Create a copy to work with
df_ml = df.copy()

# 2.1: Remove low-quality features

features_to_remove = ['snow_depth_mm', 'peak_wind_gust_kmh']

df_ml = df_ml.drop(columns=features_to_remove, errors='ignore')

print(f"   Removed: {features_to_remove}")
print(f"   Reason: Data quality issues (suspicious constant values, excessive outliers)")

# 2.2: Remove redundant code columns

redundant_cols = ['country_code', 'Item Code', 'Element Code']
df_ml = df_ml.drop(columns=redundant_cols, errors='ignore')

print(f"   Removed: {redundant_cols}")
print(f"   Reason: Duplicate information (already in name columns)")

# 2.3: Remove multicollinear temperature features

multicollinear_cols = ['min_temp_c', 'max_temp_c']

df_ml = df_ml.drop(columns=multicollinear_cols, errors='ignore')
print(f"   Removed: {multicollinear_cols}")

print(f"   Reason: Extreme correlation with avg_temp_c (r > 0.94)")
print(f"   Keeping: avg_temp_c (most stable measure)")




# 2.4: Remove Unit column

df_ml = df_ml.drop(columns=['Unit'], errors='ignore')

print(f"   Removed: Unit")
print(f"   Reason: Redundant with Element column")

# 2.5 : Create engineered features
# Temperature squared (capture non-linearity)
df_ml['temp_squared'] = df_ml['avg_temp_c'] ** 2
print(f"   Created: temp_squared (avg_temp_c(squared))")

print(f"   Purpose: Capture optimal temperature ranges (parabolic relationships)")




# Temperature-precipitation interaction

df_ml['temp_precip_interaction'] = df_ml['avg_temp_c'] * df_ml['precipitation_mm']
print(f"   Created: temp_precip_interaction (avg_temp_c * precipitation_mm)")
print(f"   Purpose: Capture combined climate effects (drought vs humid heat)")


# 2.6: Create regional grouping

region_mapping = {

    'North America': ['Canada', 'United States Of America', 'Mexico'],
    'Central America': ['Guatemala', 'Belize', 'El Salvador', 'Honduras',
                        'Nicaragua', 'Costa Rica', 'Panama'],


    'Caribbean': ['Cuba', 'Haiti', 'Dominican Republic', 'Jamaica',
                  'Trinidad And Tobago', 'Puerto Rico', 'Bahamas', 'Barbados',
                  'Saint Lucia', 'Grenada', 'Saint Vincent And The Grenadines',

                  'Antigua And Barbuda', 'Dominica', 'Saint Kitts And Nevis'],
    'South America': ['Colombia', 'Venezuela', 'Ecuador', 'Peru', 'Brazil',
                      'Bolivia', 'Paraguay', 'Chile', 'Argentina', 'Uruguay',
                      'Guyana', 'Suriname']
}

# Create reverse mapping

country_to_region = {}

for region, countries in region_mapping.items():
    for country in countries:

        country_to_region[country] = region


df_ml['region'] = df_ml['country_name'].map(country_to_region)


# Check for unmapped countries

unmapped = df_ml[df_ml['region'].isna()]['country_name'].unique()

if len(unmapped) > 0:

    print(f"   Warning: {len(unmapped)} countries not mapped:")
    print(f"   {list(unmapped)}")

else:

    print(f"   All countries successfully mapped to regions")


# 2.7 : Log-transformed target variable


print(f"   Original 'value' statistics:")
print(f"      Mean: {df_ml['value'].mean():,.2f}")

print(f"      Median: {df_ml['value'].median():,.2f}")

print(f"      Std: {df_ml['value'].std():,.2f}")
print(f"      Min: {df_ml['value'].min():,.2f}")

print(f"      Max: {df_ml['value'].max():,.2f}")

df_ml['log_value'] = np.log1p(df_ml['value'])  # log(1 + value)


print(f"\n   Log-transformed 'log_value' statistics:")

print(f"      Mean: {df_ml['log_value'].mean():.4f}")
print(f"      Median: {df_ml['log_value'].median():.4f}")

print(f"      Std: {df_ml['log_value'].std():.4f}")

print(f"      Min: {df_ml['log_value'].min():.4f}")
print(f"      Max: {df_ml['log_value'].max():.4f}")



print(f"   Reason: Original target is extremely right-skewed")
print(f"   Method: Using log1p (log(1+x)) to handle zeros safely")

print()

# STEP 3: PREPARE FEATURES AND TARGET


# 3.1: Filter to Production records only

df_ml = df_ml[df_ml['Element'] == 'Production'].copy()

print(f"   Filtered dataset shape: {df_ml.shape}")

print(f"   Reason: Focus on production prediction (not yield, area, stocks, etc.)")


# 3.2: Remove rows with missin

initial_rows = len(df_ml)

df_ml = df_ml.dropna(subset=['log_value'])
final_rows = len(df_ml)


print(f"   Removed {initial_rows - final_rows} rows with missing log_value")
print(f"   Final dataset: {final_rows:,} observations")

#3.3: Define feature lists

# Continuous numeric features
numeric_features = [
    'avg_temp_c',

    'precipitation_mm',
    'avg_wind_speed_kmh',
    'avg_sea_level_pres_hpa',
    'temp_squared',

    'temp_precip_interaction',
    'year'
]


# Categorical features (need encoding)

categorical_features = [

    'country_name',
    'Item'
]


print(f"   Continuous features ({len(numeric_features)}): {numeric_features}")

print(f"   Categorical features ({len(categorical_features)}): {categorical_features}")

# 3.4: Encode categorical features


# Label Encoding for tree-based models


le_country = LabelEncoder()

le_item = LabelEncoder()

df_ml['country_encoded'] = le_country.fit_transform(df_ml['country_name'])
df_ml['item_encoded'] = le_item.fit_transform(df_ml['Item'])

print(f"   Encoded 'country_name' -> 'country_encoded'")


print(f"      Unique countries: {df_ml['country_encoded'].nunique()}")
print(f"      Range: 0 to {df_ml['country_encoded'].max()}")

print(f"   Encoded 'Item' -> 'item_encoded'")



print(f"      Unique items: {df_ml['item_encoded'].nunique()}")
print(f"      Range: 0 to {df_ml['item_encoded'].max()}")



# 3.5: Create final feature set

feature_columns = numeric_features + ['country_encoded', 'item_encoded']
target_column = 'log_value'

print(f"   Total features: {len(feature_columns)}")



print(f"   Feature list:")
for i, feat in enumerate(feature_columns, 1):
    feat_type = "Continuous" if feat in numeric_features else "Discrete (encoded)"

    print(f"      {i:2d}. {feat:30s} - {feat_type}")


print(f"\n   Target variable: {target_column} (log-transformed production)")


# 3.6: Extract X and y
X = df_ml[feature_columns].copy()
y = df_ml[target_column].copy()
print(f"   X shape: {X.shape}")
print(f"   y shape: {y.shape}")


# Check for any remaining missing values


missing_summary = X.isnull().sum()



if missing_summary.sum() == 0:
    print(f"       No missing values")
else:
    print(missing_summary[missing_summary > 0])

print()

# STEP 4: TRAIN-TEST SPLIT

# Stratified split by country to ensure all countries represented

print(f"   Strategy: Stratify by country to ensure geographic representation")


print(f"   Split ratio: 80% train / 20% test")



X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,

    stratify=df_ml['country_encoded'],
    random_state=42
)


print(f"\n   Training set: {X_train.shape[0]:,} observations ({X_train.shape[0]/len(X)*100:.1f}%)")


print(f"   Test set:     {X_test.shape[0]:,} observations ({X_test.shape[0]/len(X)*100:.1f}%)")


# Verify stratification worked





train_countries = df_ml.loc[X_train.index, 'country_name'].nunique()

test_countries = df_ml.loc[X_test.index, 'country_name'].nunique()

total_countries = df_ml['country_name'].nunique()


print(f"   Countries in training set: {train_countries}/{total_countries}")

print(f"   Countries in test set:     {test_countries}/{total_countries}")



if train_countries == test_countries == total_countries:
    print(f"    All countries present in both train and test sets")


else:
    print(f"    Warning: Some countries missing from train or test set")

print()

# STEP 5: TRAIN RANDOM FOREST MODEL


print(f"   Algorithm: Random Forest Regressor (Ensemble Method)")



print(f"   Hyperparameters:")

model_params = {
    'n_estimators': 100,
    'max_depth': 20,
    'min_samples_split': 50,
    'min_samples_leaf': 20,
    'max_features': 'sqrt',
    'random_state': 42,
    'n_jobs': -1,
    'oob_score': True,
    'verbose': 0
}



for param, value in model_params.items():
    if param == 'n_estimators':

        print(f"       n_estimators = {value} (number of trees)")
    elif param == 'max_depth':



        print(f"       max_depth = {value} (maximum tree depth)")
    elif param == 'min_samples_split':


        print(f"       min_samples_split = {value} (min samples to split)")
    elif param == 'min_samples_leaf':
        print(f"  min_samples_leaf = {value} (min samples per leaf)")
    elif param == 'max_features':

        print(f"       max_features = '{value}' (~{int(np.sqrt(len(feature_columns)))} features per split)")
    elif param == 'random_state':
        print(f"       random_state = {value} (reproducibility)")
    elif param == 'n_jobs':


        print(f"       n_jobs = {value} (use all CPU cores)")
    elif param == 'oob_score':
        print(f"       oob_score = {value} (calculate out-of-bag score)")


print(f"   Training on {X_train.shape[0]:,} observations...")

model = RandomForestRegressor(**model_params)
model.fit(X_train, y_train)

# Display OOB Score


print(f"   OOB Rsquare Score: {model.oob_score_:.4f}")

print(f"   Interpretation: Model explains {model.oob_score_*100:.2f}% of variance (in log-scale)")



print(f"   Note: OOB provides unbiased estimate using out-of-bag samples")

print()

#STEP 6: MAKE PREDICTIONS (LOG-SCALE)

y_train_pred_log = model.predict(X_train)
print(f"   Generated {len(y_train_pred_log):,} training predictions (log-scale)")



y_test_pred_log = model.predict(X_test)

print(f"   Generated {len(y_test_pred_log):,} test predictions (log-scale)")

print()

# STEP 7: POST-PROCESSING (INVERSE LOG TRANSFORM)


print(f"   Method: np.expm1() to reverse np.log1p()")

print(f"   Formula: value = exp(log_value) - 1")

# Inverse transform predictions


y_train_pred_original = np.expm1(y_train_pred_log)
y_test_pred_original = np.expm1(y_test_pred_log)

# Inverse transform true values

y_train_original = np.expm1(y_train)
y_test_original = np.expm1(y_test)

print(f"    Converted predictions to original scale")



print(f"   Constraint: Production cannot be negative")

# Apply non-negativity constraint




y_train_pred_final = np.maximum(y_train_pred_original, 0)
y_test_pred_final = np.maximum(y_test_pred_original, 0)

negative_train = (y_train_pred_original < 0).sum()

negative_test = (y_test_pred_original < 0).sum()

print(f"   Clipped {negative_train} negative training predictions to 0")
print(f"   Clipped {negative_test} negative test predictions to 0")

print()

# STEP 8: CALCULATE AND PRINT ACCURACY METRICS

# 8.1: Training set performance

train_r2_log = r2_score(y_train, y_train_pred_log)

train_rmse_log = np.sqrt(mean_squared_error(y_train, y_train_pred_log))
train_mae_log = mean_absolute_error(y_train, y_train_pred_log)




print(f"   Rsquare Score:  {train_r2_log:.4f}")
print(f"   RMSE:      {train_rmse_log:.4f}")

print(f"   MAE:       {train_mae_log:.4f}")

# 8.2: Test set performance (LOG-SCALE)


test_r2_log = r2_score(y_test, y_test_pred_log)

test_rmse_log = np.sqrt(mean_squared_error(y_test, y_test_pred_log))


test_mae_log = mean_absolute_error(y_test, y_test_pred_log)

print(f"   Rsquare Score:  {test_r2_log:.4f}")

print(f"   RMSE:      {test_rmse_log:.4f}")
print(f"   MAE:       {test_mae_log:.4f}")


# 8.3 Training set performance (ORIGINAL SCALE)

train_r2_original = r2_score(y_train_original, y_train_pred_final)
train_rmse_original = np.sqrt(mean_squared_error(y_train_original, y_train_pred_final))


train_mae_original = mean_absolute_error(y_train_original, y_train_pred_final)

print(f"   Rsquare Score:  {train_r2_original:.4f}")

print(f"   RMSE:      {train_rmse_original:,.2f}")

print(f"   MAE:       {train_mae_original:,.2f}")


# 8.4 Test set performance (ORIGINAL SCALE)


test_r2_original = r2_score(y_test_original, y_test_pred_final)


test_rmse_original = np.sqrt(mean_squared_error(y_test_original, y_test_pred_final))
test_mae_original = mean_absolute_error(y_test_original, y_test_pred_final)

print(f"   Rsquare Score:  {test_r2_original:.4f}")

print(f"   RMSE:      {test_rmse_original:,.2f}")
print(f"   MAE:       {test_mae_original:,.2f}")

print(f"\n   Interpretation:")


print(f"       Model explains {test_r2_original*100:.2f}% of variance in production")
print(f"       Average prediction error: {test_mae_original:,.0f} units")



print(f"       Root mean squared error: {test_rmse_original:,.0f} units")


# 8.5 Overfitting Check


overfit_gap_log = train_r2_log - test_r2_log
overfit_gap_original = train_r2_original - test_r2_original

print(f"   Log-scale Rsquare gap:      {overfit_gap_log:.4f}")


print(f"   Original-scale Rsquare gap: {overfit_gap_original:.4f}")


if overfit_gap_original < 0.05:
    print(f"    Minimal overfitting (gap < 0.05)")

elif overfit_gap_original < 0.15:
    print(f"    Moderate overfitting (gap 0.05-0.15)")


else:
    print(f"    Severe overfitting (gap > 0.15)")




# 8.6: OOB vs Test Comparison


print(f"   OOB Rsquare (log-scale):  {model.oob_score_:.4f}")
print(f"   Test Rsquare (log-scale): {test_r2_log:.4f}")


print(f"   Difference:          {abs(model.oob_score_ - test_r2_log):.4f}")




if abs(model.oob_score_ - test_r2_log) < 0.05:
    print(f"    OOB closely matches test set (validates stratification)")
else:


    print(f"    OOB differs from test set (possible distribution shift)")

print()

# STEP 9: FEATURE IMPORTANCE ANALYSIS

# Get feature importances

feature_importances = model.feature_importances_

feature_importance_df = pd.DataFrame({
    'Feature': feature_columns,
    'Importance': feature_importances
}).sort_values('Importance', ascending=False)




print("\n9.1: Top 10 Most Important Features:")

for idx, row in feature_importance_df.head(10).iterrows():

    bar_length = int(row['Importance'] * 100)
    bar = '|' * bar_length


    print(f"   {row['Feature']:30s} {row['Importance']:.4f} {bar}")

print(f"\n9.2: Feature Importance Insights:")



print(f"    Most important: {feature_importance_df.iloc[0]['Feature']}")
print(f"    Least important: {feature_importance_df.iloc[-1]['Feature']}")



# Check if engineered features are valuable
temp_sq_importance = feature_importance_df[feature_importance_df['Feature'] == 'temp_squared']['Importance'].values


temp_int_importance = feature_importance_df[feature_importance_df['Feature'] == 'temp_precip_interaction']['Importance'].values

if len(temp_sq_importance) > 0 and temp_sq_importance[0] > 0.01:

    print(f"    temp_squared is valuable (importance = {temp_sq_importance[0]:.4f})")
else:

    print(f"    temp_squared has low importance")



if len(temp_int_importance) > 0 and temp_int_importance[0] > 0.01:
    print(f"    temp_precip_interaction is valuable (importance = {temp_int_importance[0]:.4f})")


else:
    print(f"    temp_precip_interaction has low importance")


print()

# STEP 10 : EXAMPLE PREDICTION

# Get sample indices
sample_indices = np.random.choice(X_test.index, size=10, replace=False)

print(f"\n{'True Value':>15} | {'Predicted':>15} | {'Error':>15} | {'% Error':>10}")



for idx in sample_indices:

    true_val = y_test_original.loc[idx]

    pred_val = y_test_pred_final[y_test.index.get_loc(idx)]
    error = pred_val - true_val



    pct_error = (error / true_val * 100) if true_val > 0 else 0

    print(f"{true_val:>15,.0f} | {pred_val:>15,.0f} | {error:>+15,.0f} | {pct_error:>+9.1f}%")

print()

"""##Final Accuracy Summary:
The Random Forest model acheived an overall Rsquare accuracy of approximately 0.43 on the test set, indicating that the model explains around 43% of the variance in the log-transformed agricultural production values.

---

#**PROJECT 5**

2. Evaluate your machine learning
* What metrics will most effectively measure the performance of your model? Why?
  * Ex. Confusion matrix, precision vs recall for classifi cation
* Do you need to compare these metrics across models?
  *  Why or why not?
  * How will you do it?
  * Do it.
* Show/visualize the performance metric(s).
  * ex. Print metrics (individual metrics, confusion matrix, heatmap, etc), cross-validate, print errors, etc - whatever makes sense for your method
* Evaluate fit
  * Are you overfitting? Underfi tting? Fitting well? How do you know?

* Is there a different ML algorithm or tweak to the existing one that could be as good or better?
  * ex. Using a different algorithm, trying ensemble methods (such as Random Forest for Decision Tree), using regularization
  * Why or why not?
  * Optional: Extra Credit (up to 10 points each for trying up to 3 different algorithms/tweaks - 30 possible points): If yes, try it out. How does the other algorithm/tweak compare for the metrics you care about?
    * Be sure to show/visualize the new performance metric(s) and evaluate the new fit

**Choosing the Right Performance Metrics**

For this regression problem predicting crop production quantities, I need metrics that tell me different aspects of model performance. I've selected the following metrics, each serving a specific purpose:

Primary Metrics:

1. Rsquare (Coefficient of Determination)

* Tells me what proportion of variance in production my model explains
* Range: 0 to 1, where 1 is perfect prediction
* Why it matters: Give me an intutive sense of overall model quality - a 0.43 Rsquare means my model expalins 43% of production variation.

2. RMSE (Root Mean Squared Error)

* Measure average prediction error in the original units (tonnes)
* Why it matters: Heavily penalizes large errors, which is important for agricultural planning where big miscalculations could have serious consequences
* Easier to interpret than Rsquare because it's in actual production units
* Easier to interpret than Rsquare beacuse it's in actual production units

3. MAE (Mean Absolute Error)
* Average absolute differences between predictions and actual values
* Why it matters: More robust to outliers than RMSE, gives me the "typical" error size
* Helps me communicate prediction accuracy to non-technical stakeholders

4. MAPE (Mean Absolute Percentage Error)
* Shows error as a percentage of actual values
* Why it matters: Allows me to compare prediction quality across crops with vastly different production scales

Why Compare Metrics Across Different Context?

My Simpson's Paradox Analysis in Project 3 revealed a critical insight: aggregate patterns can hide important differences at the country and crop levels. What appears to be a positive relationship between temperature and production overall may actually mask negative relationships for specific countries or crops.

Therefore, I need to evaluate model performace not just overall, but also:

* Across production scales (small vs. large operations)
* Across countries
* Across crop types

This granular analysis ensures that my model doesn't work well on average while failing catastrophically for specific important subgroups. Agricultural policy is implemented at country and crop levels, so understanding these performance differences is essential for practical application.
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import cross_val_score, learning_curve
import warnings

warnings.filterwarnings('ignore')


# Set visualization style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)



print("CALCULATING PERFORMANCE METRICS")

# Generate predictions

y_train_pred_log = model.predict(X_train)
y_test_pred_log = model.predict(X_test)

# Convert to original scale

y_train_pred_original = np.expm1(y_train_pred_log)
y_test_pred_original = np.expm1(y_test_pred_log)
y_train_original = np.expm1(y_train)


y_test_original = np.expm1(y_test)



# Apply non-negativity constraint

y_train_pred_final = np.maximum(y_train_pred_original, 0)
y_test_pred_final = np.maximum(y_test_pred_original, 0)

# Metrics in LOG SCALE

train_r2_log = r2_score(y_train, y_train_pred_log)


train_rmse_log = np.sqrt(mean_squared_error(y_train, y_train_pred_log))
train_mae_log = mean_absolute_error(y_train, y_train_pred_log)


test_r2_log = r2_score(y_test, y_test_pred_log)
test_rmse_log = np.sqrt(mean_squared_error(y_test, y_test_pred_log))

test_mae_log = mean_absolute_error(y_test, y_test_pred_log)



# Metrics in ORIGINAL SCALE

train_r2_orig = r2_score(y_train_original, y_train_pred_final)
train_rmse_orig = np.sqrt(mean_squared_error(y_train_original, y_train_pred_final))

train_mae_orig = mean_absolute_error(y_train_original, y_train_pred_final)
train_mape_orig = np.mean(np.abs((y_train_original - y_train_pred_final) / (y_train_original + 1e-10))) * 100



test_r2_orig = r2_score(y_test_original, y_test_pred_final)

test_rmse_orig = np.sqrt(mean_squared_error(y_test_original, y_test_pred_final))
test_mae_orig = mean_absolute_error(y_test_original, y_test_pred_final)


test_mape_orig = np.mean(np.abs((y_test_original - y_test_pred_final) / (y_test_original + 1e-10))) * 100


print("\n METRICS IN LOG SCALE (model's native space):")


print(f"Training Set:")
print(f"  Rsquare Score:  {train_r2_log:.4f}")
print(f"  RMSE:      {train_rmse_log:.4f}")
print(f"  MAE:      {train_mae_log:.4f}")
print(f"\nTest Set:")

print(f"  Rsquare Score:  {test_r2_log:.4f}")
print(f"  RMSE:      {test_rmse_log:.4f}")
print(f"  MAE:       {test_mae_log:.4f}")



print("\n METRICS IN ORIGINAL SCALE (interpretable):")


print(f"Training Set:")

print(f"  Rsquare Score:  {train_r2_orig:.4f}")

print(f"  RMSE:      {train_rmse_orig:,.0f} tonnes")
print(f"  MAE:     {train_mae_orig:,.0f} tonnes")
print(f"  MAPE:      {train_mape_orig:.2f}%")
print(f"\nTest Set:")

print(f"  Rsquare Score:  {test_r2_orig:.4f}")

print(f"  RMSE:      {test_rmse_orig:,.0f} tonnes")
print(f"  MAE:     {test_mae_orig:,.0f} tonnes")
print(f"  MAPE:      {test_mape_orig:.2f}%")



# CROSS- VALIDATION


print("CROSS-VALIDATION ANALYSIS (5-Fold)")

cv_scores = cross_val_score(model, X_train, y_train,
                            cv=5, scoring='r2', n_jobs=-1)

print(f"\nCross-Validation Rsquare Scores:")

for i, score in enumerate(cv_scores, 1):

    print(f"  Fold {i}: {score:.4f}")

print(f"\n  Mean Rsquare:     {cv_scores.mean():.4f}")
print(f"  Std Dev:     {cv_scores.std():.4f}")

print(f"  Range:     {cv_scores.min():.4f} to {cv_scores.max():.4f}")


# OVERFITTING ANALYSIS

print("OVERFITTING/UNDERFITTING ANALYSIS")

r2_gap = train_r2_log - test_r2_log

oob_gap = abs(model.oob_score_ - test_r2_log)

print(f"\nTrain-Test Gap:")
print(f"  Training Rsquare (log):  {train_r2_log:.4f}")
print(f"  Test Rsquare (log):      {test_r2_log:.4f}")
print(f"  Gap:                {r2_gap:.4f}")

print(f"\nOut-of-Bag Validation:")
print(f"  OOB Rsquare:         {model.oob_score_:.4f}")
print(f"  Test Rsquare:            {test_r2_log:.4f}")
print(f"  Difference:         {oob_gap:.4f}")


# RESIDUAL ANALUSIS

print("RESIDUAL ANALYSIS")


residuals_test = y_test_original - y_test_pred_final

residual_mean = residuals_test.mean()


residual_std = residuals_test.std()


residual_median = np.median(residuals_test)



print(f"\nResidual Statistics:")

print(f"  Mean:    {residual_mean:,.0f} tonnes")

print(f"  Median:    {residual_median:,.0f} tonnes")
print(f"  Std Dev:    {residual_std:,.0f} tonnes")

print(f"  Min:      {residuals_test.min():,.0f} tonnes")
print(f"  Max:        {residuals_test.max():,.0f} tonnes")



# PERFORMANCE BY PRODUCTION SCALE


print("PERFORMANCE BREAKDOWN BY PRODUCTION SCALE")

try:
    production_quantiles = pd.qcut(y_test_original, q=4,


                                    labels=['Small (0-25%)',


                                           'Medium (25-50%)',
                                           'Large (50-75%)',
                                           'Very Large (75-100%)'],
                                    duplicates='drop')




    scale_performance = []
    for scale in production_quantiles.cat.categories:

        mask = production_quantiles == scale


        if mask.sum() > 0:


            scale_r2 = r2_score(y_test_original[mask], y_test_pred_final[mask])
            scale_mae = mean_absolute_error(y_test_original[mask], y_test_pred_final[mask])


            scale_mape = np.mean(np.abs((y_test_original[mask] - y_test_pred_final[mask]) /

                                        (y_test_original[mask] + 1e-10))) * 100

            scale_performance.append({

                'Production_Scale': scale,
                'Rsquare': scale_r2,

                'MAE': scale_mae,
                'MAPE': scale_mape,

                'N_samples': mask.sum(),

                'Avg_Production': y_test_original[mask].mean()
            })



    scale_perf_df = pd.DataFrame(scale_performance)


    print("\n" + scale_perf_df.to_string(index=False))


except Exception as e:

    print(f"\n Could not calculate scale breakdown: {e}")

    scale_perf_df = pd.DataFrame({
        'Production_Scale': ['All'],


        'Rsquare': [test_r2_orig],
        'MAE': [test_mae_orig],

        'MAPE': [test_mape_orig],
        'N_samples': [len(y_test_original)],

        'Avg_Production': [y_test_original.mean()]
    })


# FEATURE IMPORTANCE

print("TOP 15 MOST IMPORTANT FEATURES")

feature_importance = pd.DataFrame({

    'feature': X_train.columns,

    'importance': model.feature_importances_

}).sort_values('importance', ascending=False).head(15)



print("\n" + feature_importance.to_string(index=False))


# LEARNING CURVE DATA

print("LEARNING CURVE ANALYSIS")

train_sizes, train_scores, val_scores = learning_curve(

    model, X_train, y_train,
    cv=5,

    n_jobs=-1,


    train_sizes=np.linspace(0.1, 1.0, 10),
    scoring='r2',

    random_state=42
)

train_mean = np.mean(train_scores, axis=1)


val_mean = np.mean(val_scores, axis=1)


print("\nLearning Curve Data Points:")

print(f"{'Train Size':<15} {'Train Rsquare':<15} {'Validation Rsquare':<15}")

for size, tr_score, val_score in zip(train_sizes, train_mean, val_mean):


    print(f"{size:<15.0f} {tr_score:<15.4f} {val_score:<15.4f}")

print(f"\nFinal Gap: {train_mean[-1] - val_mean[-1]:.4f}")



# SUMMARY STATISTICS


print("COMPREHENSIVE SUMMARY")


print(f"""
Model Performance Metrics:
  • Test RSquare (original):     {test_r2_orig:.4f} ({test_r2_orig*100:.1f}% variance explained)
  • Test RMSE:                 {test_rmse_orig:,.0f} tonnes
  • Test MAE:               {test_mae_orig:,.0f} tonnes
  • Test MAPE:               {test_mape_orig:.1f}%


Fit Assessment:
  • Train-Test Gap:            {r2_gap:.4f}
  • OOB vs Test Diff:       {oob_gap:.4f}
  • CV Std Dev:              {cv_scores.std():.4f}

Scale Performance:
  • Best Scale:               {scale_perf_df.loc[scale_perf_df['Rsquare'].idxmax(), 'Production_Scale']}
  • Worst Scale:            {scale_perf_df.loc[scale_perf_df['Rsquare'].idxmin(), 'Production_Scale']}



Residual Analysis:
  • Mean Residual:            {residual_mean:,.0f} tonnes

  • Std Dev:                {residual_std:,.0f} tonnes
  • Mean/Std Ratio:           {abs(residual_mean/residual_std):.4f}
""")


#VISUALIZATIONS


print("GENERATING VISUALIZATIONS")

# Figure 1: Main Performance Grid


fig = plt.figure(figsize=(16, 12))

gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

# Plot 1: Actual vs Predicted
ax1 = fig.add_subplot(gs[0, 0])
ax1.scatter(y_test_original, y_test_pred_final, alpha=0.4, s=20, color='steelblue')
ax1.plot([y_test_original.min(), y_test_original.max()],


         [y_test_original.min(), y_test_original.max()],


         'r--', lw=2, label='Perfect Prediction')

ax1.set_xlabel('Actual Production (tonnes)', fontsize=10)

ax1.set_ylabel('Predicted Production (tonnes)', fontsize=10)
ax1.set_title('Actual vs Predicted Production\n(Test Set)', fontsize=11, fontweight='bold')
ax1.legend()

ax1.grid(True, alpha=0.3)
ax1.text(0.05, 0.95, f'R² = {test_r2_orig:.3f}',


         transform=ax1.transAxes, fontsize=10,
         verticalalignment='top',

         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))



# Plot 2: Log Scale

ax2 = fig.add_subplot(gs[0, 1])

ax2.scatter(np.log1p(y_test_original), np.log1p(y_test_pred_final),

            alpha=0.4, s=20, color='darkgreen')

ax2.plot([np.log1p(y_test_original).min(), np.log1p(y_test_original).max()],


         [np.log1p(y_test_original).min(), np.log1p(y_test_original).max()],
         'r--', lw=2, label='Perfect Prediction')


ax2.set_xlabel('Actual Production (log scale)', fontsize=10)
ax2.set_ylabel('Predicted Production (log scale)', fontsize=10)


ax2.set_title('Actual vs Predicted (Log Scale)\nBetter visibility',
              fontsize=11, fontweight='bold')
ax2.legend()

ax2.grid(True, alpha=0.3)


# Plot 3: Residuals vs Predicted

ax3 = fig.add_subplot(gs[0, 2])

ax3.scatter(y_test_pred_final, residuals_test, alpha=0.4, s=20, color='coral')
ax3.axhline(y=0, color='r', linestyle='--', lw=2)


ax3.set_xlabel('Predicted Production (tonnes)', fontsize=10)
ax3.set_ylabel('Residuals', fontsize=10)

ax3.set_title('Residual Plot\n(Random scatter = good)',
              fontsize=11, fontweight='bold')

ax3.grid(True, alpha=0.3)

# Plot 4: Residual Distribution

ax4 = fig.add_subplot(gs[1, 0])

ax4.hist(residuals_test, bins=60, edgecolor='black', alpha=0.7, color='skyblue')
ax4.axvline(x=0, color='r', linestyle='--', lw=2, label='Zero Error')
ax4.set_xlabel('Residuals (tonnes)', fontsize=10)


ax4.set_ylabel('Frequency', fontsize=10)

ax4.set_title('Distribution of Errors\n(Should be centered at 0)',
              fontsize=11, fontweight='bold')
ax4.legend()
ax4.grid(True, alpha=0.3)


# Plot 5: Percentage Errors

ax5 = fig.add_subplot(gs[1, 1])

percentage_errors = (residuals_test / (y_test_original + 1e-10)) * 100

percentage_errors_clipped = np.clip(percentage_errors, -200, 200)

ax5.hist(percentage_errors_clipped, bins=60, edgecolor='black', alpha=0.7, color='lightcoral')


ax5.axvline(x=0, color='r', linestyle='--', lw=2, label='Zero Error')

ax5.set_xlabel('Percentage Error (%)', fontsize=10)
ax5.set_ylabel('Frequency', fontsize=10)

ax5.set_title('Percentage Error Distribution',
              fontsize=11, fontweight='bold')
ax5.legend()

ax5.grid(True, alpha=0.3)

# Plot 6: CV Scores


ax6 = fig.add_subplot(gs[1, 2])

ax6.bar(range(1, 6), cv_scores, color='steelblue', edgecolor='black', alpha=0.7)
ax6.axhline(y=cv_scores.mean(), color='r', linestyle='--',


            lw=2, label=f'Mean: {cv_scores.mean():.3f}')
ax6.set_xlabel('Fold Number', fontsize=10)

ax6.set_ylabel('R² Score', fontsize=10)
ax6.set_title('Cross-Validation Scores',
              fontsize=11, fontweight='bold')


ax6.set_xticks(range(1, 6))
ax6.legend()

ax6.grid(True, alpha=0.3)

ax6.set_ylim([0, 1])


# Plot 7: Feature Importance

ax7 = fig.add_subplot(gs[2, :])


ax7.barh(range(len(feature_importance)), feature_importance['importance'],
         color='forestgreen', alpha=0.7, edgecolor='black')


ax7.set_yticks(range(len(feature_importance)))

ax7.set_yticklabels(feature_importance['feature'], fontsize=9)


ax7.set_xlabel('Importance Score', fontsize=10)

ax7.set_title('Top 15 Most Important Features',

              fontsize=11, fontweight='bold')
ax7.invert_yaxis()


ax7.grid(True, alpha=0.3, axis='x')

plt.suptitle('Model Performance Analysis',


             fontsize=14, fontweight='bold', y=0.995)
plt.show()


# Figure 2: Learning Curves


plt.figure(figsize=(10, 6))

plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score', linewidth=2)


plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score', linewidth=2)
plt.xlabel('Training Set Size', fontsize=11)
plt.ylabel('R² Score', fontsize=11)


plt.title('Learning Curves', fontsize=12, fontweight='bold')


plt.legend(loc='best', fontsize=10)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()



# Figure 3: Performance by Scale

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

ax1 = axes[0]


ax1.bar(scale_perf_df['Production_Scale'], scale_perf_df['Rsquare'],
        color='steelblue', edgecolor='black', alpha=0.7)

ax1.set_ylabel('R² Score', fontsize=11)
ax1.set_xlabel('Production Scale', fontsize=11)


ax1.set_title('R² by Scale', fontsize=12, fontweight='bold')
ax1.grid(True, alpha=0.3, axis='y')


ax1.set_ylim([0, 1])
ax1.tick_params(axis='x', rotation=45)

ax2 = axes[1]


ax2.bar(scale_perf_df['Production_Scale'], scale_perf_df['MAPE'],
        color='coral', edgecolor='black', alpha=0.7)
ax2.set_ylabel('MAPE (%)', fontsize=11)


ax2.set_xlabel('Production Scale', fontsize=11)

ax2.set_title('Error % by Scale', fontsize=12, fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

ax2.tick_params(axis='x', rotation=45)

ax3 = axes[2]

ax3.bar(scale_perf_df['Production_Scale'], scale_perf_df['Avg_Production']/1000,
        color='forestgreen', edgecolor='black', alpha=0.7)

ax3.set_ylabel('Avg Production (1000s tonnes)', fontsize=11)
ax3.set_xlabel('Production Scale', fontsize=11)

ax3.set_title('Avg Production by Scale', fontsize=12, fontweight='bold')
ax3.grid(True, alpha=0.3, axis='y')

ax3.tick_params(axis='x', rotation=45)

plt.tight_layout()


plt.show()

"""**Performance Metrics Results**

**Metrics in Log Scale (Model's Native Training Space)**

Training Set Performance:

* Rsquare score: 0.4718
* RMSE : 2.7347
* MAE : 2.1189

Test Set Performace:

* Rsquare score: 0.4132
* RMSE : 2.8296
* MAE : 2.1950

Interpretation: In log-transformed space, the model achieves solid performance with test Rsquare of 0.4312, meaning it explains 43.12 % of the variance in crop production. The training and test metrics are reasonably close, suggesting the model generalizes well to unseen data.

Why Log Scale Performance Matters Most

The original scale metrics show negative Rsquare values and astronomically high MAPE percentages. This does not mean the model is terrible, it's a know mathematical artifact that occour when:
1. Log tansforamtion compresses large values: The model trains on log-transformed data where very large and very small produciton values are brought closer together

2. Exponential back-transformation amplifies errors: When converting predictions from log space back to original tonnes using `expm1()`, small prediciton errors in log space became massive errors in original space for large production values

3. Division by near-zero causes MAPE explosion: When calculating percentage errors, dividing by very small actual production values creates high percentages


The log-scale metrics are the reliable and appropriate measures of my model's true performance. This is standard practice in regression problems with highly skewed target variables.


**Cross-Validation Analysis: Model Stability**

I conducted 5-fold cross-validation to assess whether my model's performance is consistent across different subsets of training data.


Cross-Validation Rsquare Scores:
* Fold 1: 0.4251
* Fold 2: 0.4360
* Fold 3: 0.4214
* Fold 4: 0.4301
* Fold 5: 0.4279

Mean Rsquare: 0.4281
Standard Deviation: 0.0049
Range:0.4214 to 0.4360


Assesment: Model is HIGHLY STABLE

The standard deviation of only 0.0049 is exceptionally low, indicating that:

* The model performs consistently regardless of how the data is split
* It's not overfitting to pecularities of specific data subsets.
* The results are reproducible and reliable
* The 80/20 train-test split is representative of the overall data distribution

The narrow range (0.0146 spread) between the best and the worst folds further confirms stability.
This consistency gives my confidence that the model's test performance of 0.4312 is a trustworthy estimate of real-world performance.



**Overfitting/Underfitting Analysis: Evaluating Model Fit**

Train-Test Gap Analysis:

Training Rsquare (log scale): 0.4718
Test Rsquare (log scale) : 0.4312
Gap: 0.0405 (4.05%)

Diagnosis : GOOD FIT- Minimal Overfitting

The gap of 0.0405 falls well below the 0.05 threshold typically used to identify problematic overfitting. This tells me:

1. The model generalizes effectively- It performs nearly as well on unseen data as on training data

2. It's not memorizing training patterns: It it were, we'd see much higher training score and lower test scores

3. Regularization parameters are well-calibrated- My hyperparameter choices (max_depth=20, min_samples_leaf =20, min_samples_split= 50) successfully prevented overfitting



**Out-of-Bag (OOB) Validation**

OOB Rsquare: 0.4284
Test Rsquare: 0.4312
Difference: 0.0028

Assesment: Excellent Agreement

The tiny differnce of  0.0028 between OOB and test Rsquare provides additional confirmation that:

* My train-test split was  representative and fair
* There's no distribution shift between training and test sets
* The stratified splitting by country worked as intended

Random Forest's out-of-bag scoring provides an internal validation mechanism by testing each tres on the ~37% of training data it didn't see during bootstrap sampling. The fact that this internal estimate amtched my external test set so closely validates my entire evaluation approach


**Absolute Performace Assesment**

Test Rsquare = 0.4312 -> MODERATE - Resonable fit for agricultural data

This performace level is appropriate and expected for several reasons:
1. Agricultural systems are inherently complex: Crop production depends on countless interacting factors including weather, soil quality, water availability, pest pressure, farming practices, technology adoption, market conditions, and policy interventions.

2. Natural biological variability: Even under identical conditions, crop yields vary due to random factors, disease outbreaks, and genetic variation.

3. Data Aggregation loses information: My analysis works at the country-year level, which masks important sub-national variation in climate, farming practices, and yields.

Given these constraints, explaining 43% of production variance using only weather variables and basic features represnts solid predictive performance. Many published agricultural studies report Rsquare values in the range of 0.30-0.60 for similar prediciton tasks.



**Residual Analysis: Understanding Model Errors**

Residual Statistics:
* Mean : 420,252 tonnes
* Median : 415 tonnes
* Standard Deviation: 2,056,417 tonnes

Key Findings:

1. Massive Mean-Median Discrepancy
This 1,000 fold difference reeals that:
* The median error is near zero- For most predictions the model is quite accurate
* A few enormous positive errors are pulling the mean upward dramatically
* The model systematically underpredicts for some very large production operattions
* The error distribution is heavly right-skewed

2. Mean/ Standard Devation Ratio: 0.204


The mean residual is about 20% of one standard deviation. While this indicates slight systematic bias toward underprediction, it's not severe enough to be a major concern. The bias is driven by the model's struggle with extreme values rather than a fundemental flaw in the approach.

3. Extreme Maximum Error: 28,062,047 tonnes

This enormous error almost certainly corresponds to:
* Large-scale grain production in major agricultural countries (USA, Brazil, Argentina)
* Crops like corn, wheat, or soybeans where single countries produce tens of millions of tonnes.
* Years with exceptionally favourable conditions that pushed production far above historical norms

The model, trained on average patterns, struggles to anticipate these exceptional cases.

4. What the Residuals Tell Us about Model Limitations

The residual pattern indicates heteroscedasticity- prediction error increase with production scale. This makes sense because:

* Small farms (producing hundreds of tonnes) are predicted within +/- 1,000 tonnes
* Large operations (producing millions of tonnes) have errors in the hundreds of thousands of tonnes
* The model maintains roughly constant percentage error rather than constant absolute error.

This is actually desirable property for agricultural forecasting, when stakeholders typically care more about relative accuracy (percentage error) than absolute accuracy.

**Performance Breakdown by Production Scale**

Critical Insight: The Scale Paradox

The production scale breakdown reveals a suprising pattern that initially seems contradictory:

Best Rsquare Performance: Very Large (75-100%)
Worst Rsquare Performance: Small (0-25%)

However, looking at MAPE (percentage error) tells the real story:

Best MAPE: Very Large (75-100%)
Worst MAPE: Small (0-25%)

What's Really Happening:

These negatice Rsquare values original scale occur because the model is working in log space, where it performs well (Rsquare= 0.43), but the back-transformation to original scale recreates mathematical artifacts.

For Small Operations (0-25% quantile, avg121 tonnes):

* Small absolute errors in log space become proportionally huge in original scale
* A 100-tonnes prediction error on 121- tonne farm is 82% MAPE
* The exponential transformation exaggerates these relative errors catastrophically

For Large Operations (75-100% quantile, avg 1.77 million tonnes):
* Large absolute errors ( 1.7 Million tonnes MAE) seem massive
* But represent only 86.5% MAPE relative to production scale
* The model maintains consistent percentage accuracy across scales


The Actual Performance Pattern:

MAOE decreases as production scale increases:

* Small farms: Unable to calculate reliably
* Very Large Farms: 87% error

This progression makes intutive sense: larger operations have more stable, predictable production pattern beacuse they:

* Averaage out local weather variations across larger ares
* Use more consistent, mechanized farming practics
* Have better resources for adapting to conditions
* Produce commodity crops with established growing patterns


**Feature Importance: What Drives Predictions?**

Analysis of Top Features:

1. Item Encoded (30.2% importance)- Most Critical Factor

The specific crop type dominates prediction accuracy. This makes complete sense because:

* Different crops have fundamentally different production scales
* Each crop has unique climate requirements, growing seasons, and yield potentials
* Crop Choice determines baseline production expectations before weather even matters

2. Country Encoded (17.5 % Importance)- Second Most Important

Country identity captures many unmeasured factors:

* Agricultual infrastructure and technology levels
* Soil quality and land availability
* Farming expertise and education systems
* Government policies, subsidies, and regulations
* Economic conditions affecting input availability

This high importance confirms my Simpson's Paradox findings, country-specific context matters enormously.

3. Average Temperature (13.1 %) and Temperature Square (11.2%)

Combined, temperature effects account for 24.3 % of importance, showing:

* Temperature is the single most important weather  variable
* The squared term captures non-linear relationships (optimal temperature ranges, heat stress)
* Both linear and quadratic effects matter for crop growth

4. Atmospheric Pressure (8.9%) and Wind Speed (8%)

These capture additional climate patterns:

* Pressure relates to weather systems and storm likelihood
* wind affects evapotranspiration rates and moisture stress

5. Temperature- Precipitation Interaction (5.1%)

This engineered feature captures the synergistic effect of combined climate conditions:

* High temperature + high predicitation = tropical/ humid conditions
* High temperature + low precipitation = drought stress
* The interaction matters beyond individual effects


6. Year (3.1%) and Precipitation (2.8)

* Year captures technological progress and agriultural improvements over timie
* Suprisingly low precipitation importance suggests temperature dominates direct effects, while precipitation works mainly through interactions


What's missing:

The relatively modest individual importance score (nothing over 30%) indicate that no single factor dominates production. Instead, production results from complex interactions among multiple factors. The features I'm missing (irrigation, fertilizer, soil quality) likely explains much of the remaining 57% unexplained variance.


Learning Curve Analysis: Would More Data Help?

Final Train- Validation Gap : 0.0425

Key Observaions:

1. Steady Improvement with Data Size

Both training and validation scores improve consistently as training set size increases. This is the expected pattern for a well-behaved model.

2. Gap Remains Small and Stable

The train-validation gap starts at 0.08 with minimal data and converges to 0.04 with full data. This stability indicates good regularization, the model isn't overfitting even with more capacity.

3. Validation Curve Still Rising

The validation Rsquare continues to imporve from 0.3138 (10% data) to 0.4281 (100% data), gaining 0.1143 in performance. However, the rate of improvement is slowing:

* First 50% of data: +0.008 Rsquare gain
* Last 50% of data: +0.05 Rsquare gain


4. Diminishing Returns

Extrapolating the trend suggests that:

* Doubling the dataset (to~ 280,000 samples) might gain another 0.02~ 0.03 Rsquare)
* The model is approaching its representative capacity limit


Conclusion: More Data Would Provide Marginal Benefirs

The learning curve show we've reached the point where more data alone won't dramatically improve performace. To achieve substantial improvements ( moving from Rsquare = 0.43 to 0.60+), I would need:
1. Crop specific models instead of one model for all crops
2. Regional/ Climate zone models instead of one global model
3. More complex feature interaction or different modelling approaches

#**Extra Credit: Testing Alternative ML Approaches**

Is there a different ML algorithm or tweak that could be as good or better?

Yes, absolutely. After systematically testing three alternative approaches, I discovered that Crop-Specific Models dramatically outperform the baseline Random Forest, acheiving a remarkable 91% improvement in predictive accuracy. Additionally, Gradiient Boosting also provides substantial gains over the baseline approach.

**Motivation: Why Test Alternative?**

My baseline Random Forest Model acheived respectable but not exceptional performance (rsquare = 0.43), explaining only 43% of agricultural production variance. This left substantial room for improvement. Several findings from my earlier analysis sugested that alternative approaches might unlock better performance:

1. Simpson's Paradox revealed context-dependency : The aggregate climate-production relationship masked dramatically different patterns at the country and crop levels, suggesting specialized models might unlock better performance:

2. Feature Importance showed crop dominates: Item encodeing (crop type) accounted for 30% of features importance- more than any other variable-indicating crops have fundamentally different production patterns.

3. Heteroscedastic errors: My residual analysis showed prediction errors increased with produciton scale, suggesting sequential error-correction algorithms might help.

4. String baseline stability: Low overfitting and excellent cross-validation stability indicated the model wasn't maxed out on complexity.

Given these insights, I tested three different strategies to improve predicion accuracy.



#**Three Approached Tested**

**Approach 1: Gradient Boosting might outperform Random Forest:**

Why Gradient Boosting might outperform Random Forest:

Gradient Boosting represents a fundamentally different ensemble philosophy than Random Forest. While Random Forest buuilds independent trees in paralle and averages their predictions, Gradient Boosting builds trees sequentially, with each new tree specifically trained to correct the errors made by all previous trees.

Key Advantages:

1. Iterative error correction: Each tree focuses on the observation that previous trees predicted poorly, progressively refining the ensemble's accuracy on different cases.

2. Adaptive learning: Later trees become increasingly specialized, capturing subtle patterns that early trees missed.

3. Better handling of heteroscedasticity: can adapt tree complexity to different error patterns, shallow trees for easy cases, deeper trees for complex patterns.

$. Stochastic gradient boosting: Using only 80% of data per tree (subsample = 0.8) adds regularization while maintaining accuracy.

Potential drawbacks:

1. Sequential training: Cannot parallelize like Random Forest, resulting in longer training times

2. Overfitting risk: Sequential building can memorize training noise if not properly regularized with shallow trees and learning rate control

3. Hyperparameter sensitivity: Requires careful tuning of learning rate, max depth, and number of estimators.

Comparision to Baseline:

* Improvement : +0.1902 Rsquare (+44.1% Relative gain)
* RSME Reduction : -0.5211 (18.4 % better predictions)
* Overfitting : Better than baseline (0.0027 vs 0.0405)
* Stability : Equivalent to baseline (CV std both ~ 0.005)

Analysis: Why Did Gradient Boosting work so well?

Gradient Boosting Acheived dramatic improvement, raising Rsquare from 0.43 to 0.62. This means it explains 62 % of agricultual production variance compared to Random Forest's 43 %, an absolute gain of 19 percentage points.

Three key factors explain this success:

1. Sequential Error correction

Random Forest's 100 independent trees make similar mistakes on different cases. Gradient Boosting's sequential approach means tree 50 specifically targets the errors remaining after trees 1-49, and tree 100 focuses on the hardest remaining cases. This iterative refinement is particularly valuable for agricultural prediction where some country-crop-climate combinations are inherently difficult to predict.


2. Near-Perfect Generalization

The overfitting gap of only 0.0027 is remarkable, training and test performance are nearly identical. Despite being more complex than Random Forest, the conservative regularization prevents overfitting. This proves that with proper tuning, sophisticated models can improve both accuracy and generaliation.


3. Better Handling of Extreme Values

My earlier residual analysis showed Random Forest Systematically underpredicted very large production values. Gradient Boosting's ability to build ireaslingly specialized trees for difficult cases helps address these extreme predictions more effectively.

Tradeoffs:

The main cost is training time. 46 secends vs Random Forest's ~ 20 seconds. However, for batch prediction tasks (like annual agricultural forecasts), this 26-second difference is negligible compared to the 44% accuracy gain.


#**Approach 2: Crop-Specific Models (Specialized Ensembles)**

Why Crop-Specific Model might outperform a unifed model:

This approach represents a fundamental departure from one-size-fits-all modelling. Instead of training a single model for all 276 crops, I train seperate Random Forest models for each major crop type.


Key Advantages:

1. Crop-specific climate responses: Wheat thrives in cool weather; rice needs tropical heat. A unified model average these contradictory patterns; specialized models capture each crop's requirements.

2. Relevant feature importance: Temperature matters enormously for tomatoes but minimally for root vegetables. Crop-specific models learn which features actually matter for each crop.

3. Avoid negative transfer: A unified model trained heavily on corn data might mispredict cassava. Specialized models prevent patterns from one crop from distorting predictions for unrelated crops.

4. Simpler decision boundaries: Each specialized model has an easier learning task (one crop's patterns) than the unified model (all crop's patterns).


Potential Drawbacks:

1. Less Training data per model: Instead of 140,000 samples, each model sees only 1,500- 3,000 samples.

2. Cannot share information: Similar crops (wheat and barley) cannot share learned patterns.

3. Increased complexity: Must maintain and deploy 5+ models instead of one.

4. Overfitting Risk: Smaller dataset increase overfitting vulnerability.


Implementation Details

I identified the top 5 crops by sample count and trained seperate Random Forest Models for each:

Top 5 by Sample Size:

1. Hen Eggs in Shell, Fresh: 3,018 samples
2. Fat of Pigs: 1,528 samples
3. Eggs Primary: 1,582 samples
4. Edible offal of Pigs, fresh, Chilled or Frozen : 1,524 samples
5. Meat of Sheep, Fresh or Chilled: 1,524 smpales


Each crop received its own Random Forest with the same hyperparameters as baseline but adjusted for smaller sample sizes (min_samples_leaf = 10 instead of 20)

Comparision to Baseline:

* Improvement : +0.3934 Rsquare (+91,2% relative gain)
* Overfitting: Slightly higher (0.049 vs 0.041) but still acceptable
* Training Time: Much faster (3.2s vs ~20s)

Analysis: Why Did Crop-Specific Models Excel?

Crop-Specific Models acheived spectacular performance, with a weighted average Rsquare of 0.82 nearly double the baseline's 0.43. This 91% improvement is the largest gain among all approached tested.

Four factors explained this remarkable success:

1. Homogeneous Learning Tasks

Each specialized model faces a much simpler problem than the unified model. Instead of learning "how do 276 different crops respond to climate across 30 countries" each model only learns " How does this one crop respond to climate across relevant countries". This focused learning task allows each model to capture subtle patterns specific to that crop.

2. Elimination of Conflicting Patterns

The unified model must reconcile contradictory relationships. For example:

* Eggs/ poultry: Temperature has minimal direct effect (climate-controlled facilities)
* Field crops: Temperature has massive direct effect (growth rates, photosynthesis)

A unified model averages these patterns, performing poorly on both. Specialized models captures each crop's true relationship without interference from unrelated crops.

3. Relevant Features Selection

Each model automatically emphasizes the features that actually matter for its crop, rather tahn using a one-size-fits-all features importance.

4. Validation of Simpson's Paradox Findings

This result powerful confirms my Project# Simpson's Paradox Analyysis. I discovered that aggregate climate-production relationship hide contradictory patterns at the crop level. The 91% improvement from crop-specific models proves this wan't just a stability curiosity, it has massive practical implications for prediction accuracy.

Why Overfitting Remained Acceptable:

Despite smaller sample sizes, overfitting increased only slightly. This is beacuse:

1. Each learning task is simpler
2. Feature relevance is cleaner
3. The regularization parameters were adjusted for smaller datasets


Computational Effecienct Bonus:

Training five seperate models took only 3.2 seconds, faster than training one unified model. This is because each model trains on a smaller dataset, and modern computers can train multiple models in parallel efficiently.


#**Approach 3: Polynomial Feature Interaction (Complex Feature Engineering)**


Theoretical Rationale

Why Polynomial Features might improve performance:

Agricultural systems involve complex synergistic effects that simple linear features cannot capture. For example:
* Drought stress: high temperature * low precipitation (interaction effect)

* Optimal growing conditions = moderate temperature^2 * adequate precipitation

* Wind-temperature interaction = high wind + low temperature = wind chill stress

Polynomial features create all pairwise interactions and squared terms from continuous variables, allowing models to capture these complex relationships.

Key Advantages:

1. Captures synergistic effects: Models can learn that high temp+ low rain is worse than either factor alone.

2. Approximation non-linear curves: Temperature^2 allows capturing optimal temperature ranges with diminishing returns.

3. Richer features space: More features mean more patterns the model can potentially discover.

4. Works with any algorithm: Even linear models can fit curves with polynomial features.


Potential Drawbacks:

1. Curse of dimensionality: Features explode from 9 to 30, increasing overfitting risk.

2. Multicollinearity: Temp and temp^2 are highly correlated, potentially confusing the model.

3. Diminishing returns:Random Forest already captures interaction through recursive splits.

4. Longer training time: More features = more computation per split.

Implementation Details

I applied polynomial feature expansion to the six continuous climate variables:

* avg_temp_c
* precipitation_mm
* avg_wind_speed_kmh
* avg_sea_level_pres_hpa
* temp_squared
* temp_precip_interaction

Using sklearn's Polynomial Features with degree = 2 created all pairwise interaction plus squared terms:

* Original: 6 continuous + 3 categorical = 9 features
* After Polynomial expansion: 27 continuos + 3 categorical = 30 features
* New interactions: temp * wind, precip * pressure, wind^2, etc.


Results:

Comparision to Baseline:

* Decline : -0.1019 (Rsquare)
* MSME: Worse by 0.243
* Overfitting: similar to baseline (0.042 vs 0.041)
* Stability: Equivalent  (CV std both ~ 0.005)

Analysis: why did Polynomial Features Fail?

Polynomial features significantly hurt performance, dropping Rsquare from 0.43 to 0.33, a 24% decline. This surprisingly result teaches several important lessons about feature engineering:

Three explain this failure:

1. Random Forest Already Captures  Interactions

Random Forest's recursive splitting naturally discovers interactions. This automatically captures the temp*precip interactions without explicitly creating that feature. Adding polynomial features gives the model redundant information that doesn't help and potentially confuses feature selection.

2. Curse of Dimensionality Without Benefit

Increasing features from 9 to 30 has costs:

* More features to evaluate at each split
* Higher risk of spurious correlations
* harder to find truly useful features among many candidates

These costs would be wothwhile if polynomial features provided new information. But since Random Forest already captures these patters, we pay the cost withoout getting the benefit.

3. Multicollinearity Confuses Feature Selection

Adding polynomial expansions creates redundant features that compete during splits. Instead of consistently using the most informative features, the model randomly chooses among correlated alternatives, redcing prediction stability and accuracy.

4. Validation of Random Forest's Capability

This failure actually validates Random Forest's strength. Tree-based models are specifically designed to find non-linear patterns and interactions through recursive partioning. Unlike linear models, trees models don't benefit from explicit interaction terms.

When would Polynomial Features Help?

This approach would likely succed with:

* Linear models (Ridge, Lasso, linear regression) where polynomial terms are necessary for capturing curves
* Simple models that can't naturally find interactions
* Neural networls (depending on architecture) where explicit features engineering can help

But for tree-based models like Random Forest, polynomial features are redundant at best and harmfuul at worst.


#**Conclusion**

Testing three alternative approaches revealed that substantial improvements over baseline Random Forest are acheivable through appropriate algorithmic choice and model specialization:

1. Crop-specific models (Rsquare =0.82): Best Performance through specialization, validating Simpson's Paradox findings

2. Gradient Bossting (Rsquare = 0.62) Strong improvement through sequential error correction

3. Polynomial Features (Rsquare = 0.33) : Performance decline due to redundancy with tree-based modeling
"""

# EXTRA CREDIT


import numpy as np

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import cross_val_score

from sklearn.preprocessing import PolynomialFeatures
import warnings

warnings.filterwarnings('ignore')


# Baseline

# Get Baseline Predictions

baseline_train_pred = model.predict(X_train)

baseline_test_pred = model.predict(X_test)

# Calculate baseline metrics
baseline_train_r2 = r2_score(y_train, baseline_train_pred)

baseline_test_r2 = r2_score(y_test, baseline_test_pred)

baseline_train_rmse = np.sqrt(mean_squared_error(y_train, baseline_train_pred))
baseline_test_rmse = np.sqrt(mean_squared_error(y_test, baseline_test_pred))

print(f"\nBaseline Performance:")

print(f"  Training Rsquare:  {baseline_train_r2:.4f}")
print(f"  Test Rsquare:      {baseline_test_r2:.4f}")
print(f"  Training RMSE: {baseline_train_rmse:.4f}")

print(f"  Test RMSE:     {baseline_test_rmse:.4f}")

print(f"  Overfit Gap:   {baseline_train_r2 - baseline_test_r2:.4f}")

# Store baseline results

results_comparison = [{
    'Algorithm': 'Random Forest (Baseline)',
    'Train_R2': baseline_train_r2,
    'Test_R2': baseline_test_r2,

    'Train_RMSE': baseline_train_rmse,
    'Test_RMSE': baseline_test_rmse,

    'Overfit_Gap': baseline_train_r2 - baseline_test_r2,
    'Training_Time': 'Already trained'


}]


# APPROACH 1: GRADIENT BOOSTING

import time
start_time = time.time()


# Create and train Gradient Boosting model
gb_model = GradientBoostingRegressor(

    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,

    min_samples_split=50,
    min_samples_leaf=20,

    subsample=0.8,
    random_state=42,
    verbose=0
)


gb_model.fit(X_train, y_train)

gb_training_time = time.time() - start_time

# Make predictions
gb_train_pred = gb_model.predict(X_train)

gb_test_pred = gb_model.predict(X_test)

# Calculate metrics
gb_train_r2 = r2_score(y_train, gb_train_pred)
gb_test_r2 = r2_score(y_test, gb_test_pred)

gb_train_rmse = np.sqrt(mean_squared_error(y_train, gb_train_pred))
gb_test_rmse = np.sqrt(mean_squared_error(y_test, gb_test_pred))

# Cross-Validation

gb_cv_scores = cross_val_score(gb_model, X_train, y_train, cv=5, scoring='r2', n_jobs=-1)

print(f"\nGradient Boosting Performance:")

print(f"  Training Rsquare:    {gb_train_r2:.4f}")
print(f"  Test Rsquare:         {gb_test_r2:.4f}")
print(f"  Training RMSE:  {gb_train_rmse:.4f}")
print(f"  Test RMSE:       {gb_test_rmse:.4f}")
print(f"  Overfit Gap:    {gb_train_r2 - gb_test_r2:.4f}")
print(f"  CV Mean Rsquare:     {gb_cv_scores.mean():.4f} (±{gb_cv_scores.std():.4f})")
print(f"  Training Time:  {gb_training_time:.1f} seconds")

# STORE RESULTS

results_comparison.append({

    'Algorithm': 'Gradient Boosting',


    'Train_R2': gb_train_r2,
    'Test_R2': gb_test_r2,


    'Train_RMSE': gb_train_rmse,
    'Test_RMSE': gb_test_rmse,

    'Overfit_Gap': gb_train_r2 - gb_test_r2,
    'Training_Time': f'{gb_training_time:.1f}s'
})

improvement = gb_test_r2 - baseline_test_r2

print(f"\n{'IMPROVEMENT' if improvement > 0 else 'DECLINE'}: {improvement:+.4f} R² vs baseline")


# APPROACH 2: CROP-SPECIFIC MODELS

try:

    # access original data with crop names

    crop_counts = df_ml.loc[X_train.index, 'Item'].value_counts()


    top_5_crops = crop_counts.head(5).index.tolist()

    print(f"Top 5 crops by sample count:")

    for i, crop in enumerate(top_5_crops, 1):


        count = crop_counts[crop]
        print(f"  {i}. {crop}: {count:,} samples")


    # Train separate model for each crop


    crop_models = {}
    crop_results = []

    start_time = time.time()


    for crop in top_5_crops:
        # Get indices for this crop

        crop_mask_train = df_ml.loc[X_train.index, 'Item'] == crop



        crop_mask_test = df_ml.loc[X_test.index, 'Item'] == crop



        if crop_mask_train.sum() < 100 or crop_mask_test.sum() < 20:
            print(f"\n  Skipping {crop}: insufficient samples")
            continue



        # Get crop-specific data
        X_train_crop = X_train[crop_mask_train]


        y_train_crop = y_train[crop_mask_train]

        X_test_crop = X_test[crop_mask_test]
        y_test_crop = y_test[crop_mask_test]

        # Train Random Forest for this crop

        crop_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=20,
            min_samples_split=20,


            min_samples_leaf=10,
            max_features='sqrt',
            random_state=42,
            n_jobs=-1
        )



        crop_model.fit(X_train_crop, y_train_crop)

        # Evaluate



        train_pred = crop_model.predict(X_train_crop)
        test_pred = crop_model.predict(X_test_crop)

        train_r2 = r2_score(y_train_crop, train_pred)



        test_r2 = r2_score(y_test_crop, test_pred)

        crop_models[crop] = crop_model
        crop_results.append({
            'Crop': crop,


            'Train_Samples': len(X_train_crop),
            'Test_Samples': len(X_test_crop),
            'Train_R2': train_r2,
            'Test_R2': test_r2,

            'Overfit_Gap': train_r2 - test_r2
        })


    crop_training_time = time.time() - start_time


    # Calculate weighted average performance
    crop_results_df = pd.DataFrame(crop_results)
    total_test_samples = crop_results_df['Test_Samples'].sum()


    weighted_test_r2 = (crop_results_df['Test_R2'] * crop_results_df['Test_Samples']).sum() / total_test_samples
    weighted_train_r2 = (crop_results_df['Train_R2'] * crop_results_df['Train_Samples']).sum() / crop_results_df['Train_Samples'].sum()

    print(f"\nCrop-Specific Model Results:")


    print(crop_results_df.to_string(index=False))

    print(f"\nWeighted Average Performance (Top 5 Crops):")
    print(f"  Training Rsquare:    {weighted_train_r2:.4f}")
    print(f"  Test Rsquare:        {weighted_test_r2:.4f}")


    print(f"  Overfit Gap:    {weighted_train_r2 - weighted_test_r2:.4f}")
    print(f"  Training Time:  {crop_training_time:.1f} seconds")

    # Store results



    results_comparison.append({
        'Algorithm': 'Crop-Specific Models',
        'Train_R2': weighted_train_r2,
        'Test_R2': weighted_test_r2,
        'Train_RMSE': np.nan,


        'Test_RMSE': np.nan,
        'Overfit_Gap': weighted_train_r2 - weighted_test_r2,
        'Training_Time': f'{crop_training_time:.1f}s'
    })



    improvement = weighted_test_r2 - baseline_test_r2
    print(f"\n{'IMPROVEMENT' if improvement > 0 else 'DECLINE'}: {improvement:+.4f} R² vs baseline")


except Exception as e:

    print(f"\n Could not create crop-specific models: {e}")


    print("Skipping this approach")



# APPROACH 3: POLYNOMIAL FEATURE INTERACTIONS

# Select features for expansion

continuous_features = ['avg_temp_c', 'precipitation_mm', 'avg_wind_speed_kmh',
                       'avg_sea_level_pres_hpa', 'temp_squared', 'temp_precip_interaction']


# Get indices of continuous features

continuous_indices = [i for i, col in enumerate(X_train.columns) if col in continuous_features]


# Extract continuous features

X_train_continuous = X_train.iloc[:, continuous_indices]

X_test_continuous = X_test.iloc[:, continuous_indices]

# Create polynomial features

poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)

start_time = time.time()

X_train_poly = poly.fit_transform(X_train_continuous)


X_test_poly = poly.transform(X_test_continuous)


# Combine with categorical features


categorical_indices = [i for i, col in enumerate(X_train.columns) if col not in continuous_features]
X_train_categorical = X_train.iloc[:, categorical_indices]


X_test_categorical = X_test.iloc[:, categorical_indices]

X_train_combined = np.hstack([X_train_poly, X_train_categorical])


X_test_combined = np.hstack([X_test_poly, X_test_categorical])

print(f"  Original features: {X_train.shape[1]}")


print(f"  Polynomial features: {X_train_poly.shape[1]}")
print(f"  Total features: {X_train_combined.shape[1]}")


# Train Random Forest with polynomial features

poly_model = RandomForestRegressor(

    n_estimators=100,
    max_depth=20,
    min_samples_split=50,
    min_samples_leaf=20,
    max_features='sqrt',
    random_state=42,
    n_jobs=-1
)

poly_model.fit(X_train_combined, y_train)

poly_training_time = time.time() - start_time


# Make predictions

poly_train_pred = poly_model.predict(X_train_combined)

poly_test_pred = poly_model.predict(X_test_combined)


# Calculated metrics

poly_train_r2 = r2_score(y_train, poly_train_pred)

poly_test_r2 = r2_score(y_test, poly_test_pred)
poly_train_rmse = np.sqrt(mean_squared_error(y_train, poly_train_pred))


poly_test_rmse = np.sqrt(mean_squared_error(y_test, poly_test_pred))



# Cross-validation

poly_cv_scores = cross_val_score(poly_model, X_train_combined, y_train,
                                 cv=5, scoring='r2', n_jobs=-1)


print(f"\nPolynomial Features Performance:")

print(f"  Training Rsquare:    {poly_train_r2:.4f}")
print(f"  Test Rsquare:        {poly_test_r2:.4f}")


print(f"  Training RMSE:  {poly_train_rmse:.4f}")
print(f"  Test RMSE:      {poly_test_rmse:.4f}")


print(f"  Overfit Gap:    {poly_train_r2 - poly_test_r2:.4f}")
print(f"  CV Mean Rsquare:     {poly_cv_scores.mean():.4f} (±{poly_cv_scores.std():.4f})")

print(f"  Training Time:  {poly_training_time:.1f} seconds")


# Store Results

results_comparison.append({
    'Algorithm': 'Polynomial Features',
    'Train_R2': poly_train_r2,


    'Test_R2': poly_test_r2,
    'Train_RMSE': poly_train_rmse,
    'Test_RMSE': poly_test_rmse,

    'Overfit_Gap': poly_train_r2 - poly_test_r2,

    'Training_Time': f'{poly_training_time:.1f}s'
})

improvement = poly_test_r2 - baseline_test_r2


print(f"\n{'IMPROVEMENT' if improvement > 0 else 'DECLINE'}: {improvement:+.4f} R² vs baseline")


# COMPREHENSIVE COMPARISION

#CREATE comparision dataframe

comparison_df = pd.DataFrame(results_comparison)


print("\n" + comparison_df.to_string(index=False))


# Find best model
best_model_idx = comparison_df['Test_R2'].idxmax()
best_model_name = comparison_df.loc[best_model_idx, 'Algorithm']
best_test_r2 = comparison_df.loc[best_model_idx, 'Test_R2']

print(f"\n BEST APPROACH: {best_model_name}")
print(f"   Test Rsquare: {best_test_r2:.4f}")


# Calculate improvement over baseline
if best_model_idx != 0:


    improvement = best_test_r2 - baseline_test_r2

    improvement_pct = (improvement / baseline_test_r2) * 100
    print(f"   Improvement: +{improvement:.4f} R² ({improvement_pct:+.2f}%)")



# VISUALIZATION : Performance Comparision



fig, axes = plt.subplots(2, 2, figsize=(14, 10))


# Plot 1: Test Rsquare Comparision
ax1 = axes[0, 0]


colors = ['steelblue', 'coral', 'lightgreen', 'gold'][:len(comparison_df)]
bars = ax1.bar(comparison_df['Algorithm'], comparison_df['Test_R2'],
               color=colors, edgecolor='black', alpha=0.7)


ax1.axhline(y=baseline_test_r2, color='red', linestyle='--',
            lw=2, label=f'Baseline: {baseline_test_r2:.4f}')
ax1.set_ylabel('Test Rsquare Score', fontsize=11)


ax1.set_title('Test Rsquare Comparison Across Approaches', fontsize=12, fontweight='bold')
ax1.legend()


ax1.grid(True, alpha=0.3, axis='y')
ax1.tick_params(axis='x', rotation=45)

for bar in bars:

    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height,
             f'{height:.4f}', ha='center', va='bottom', fontsize=9)

# Plot 2: Overfitting Comparision
ax2 = axes[0, 1]

bars = ax2.bar(comparison_df['Algorithm'], comparison_df['Overfit_Gap'],
               color=colors, edgecolor='black', alpha=0.7)

ax2.axhline(y=0.05, color='orange', linestyle='--', lw=2, label='Acceptable Threshold')
ax2.set_ylabel('Train-Test Gap', fontsize=11)


ax2.set_title('Overfitting Analysis (Lower is Better)', fontsize=12, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3, axis='y')
ax2.tick_params(axis='x', rotation=45)


for bar in bars:


    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height,


             f'{height:.4f}', ha='center', va='bottom', fontsize=9)

# Plot 3: Train vs Test Rsquare


ax3 = axes[1, 0]


for i, row in comparison_df.iterrows():
    ax3.scatter(row['Train_R2'], row['Test_R2'],

                s=200, c=colors[i], edgecolor='black',
                alpha=0.7, label=row['Algorithm'])
ax3.plot([0.3, 0.7], [0.3, 0.7], 'r--', lw=2, label='Perfect Fit')
ax3.set_xlabel('Training Rsquare', fontsize=11)
ax3.set_ylabel('Test Rsquare', fontsize=11)


ax3.set_title('Training vs Test Performance', fontsize=12, fontweight='bold')
ax3.legend(fontsize=8)
ax3.grid(True, alpha=0.3)

# PLOT 4: Improvement vs Baseline


ax4 = axes[1, 1]
improvements = comparison_df['Test_R2'] - baseline_test_r2

bar_colors = ['green' if x > 0 else 'red' for x in improvements]
bars = ax4.bar(comparison_df['Algorithm'], improvements,
               color=bar_colors, edgecolor='black', alpha=0.7)

ax4.axhline(y=0, color='black', linestyle='-', lw=1)
ax4.set_ylabel('Rsquare Change vs Baseline', fontsize=11)


ax4.set_title('Performance Improvement/Decline', fontsize=12, fontweight='bold')
ax4.grid(True, alpha=0.3, axis='y')

ax4.tick_params(axis='x', rotation=45)

for bar in bars:


    height = bar.get_height()
    ax4.text(bar.get_x() + bar.get_width()/2., height,


             f'{height:+.4f}', ha='center',
             va='bottom' if height > 0 else 'top', fontsize=9)

plt.tight_layout()
plt.show()



# FINAL RECOMMENDATIONS

print(f"\n Approach Performance Summary:")


for i, row in comparison_df.iterrows():
    print(f"   {i+1}. {row['Algorithm']}: Rsquare = {row['Test_R2']:.4f}")

print(f"\n Key Insights:")
print(f"   Best performing: {best_model_name}")
print(f"    Performance range: {comparison_df['Test_R2'].min():.4f} to {comparison_df['Test_R2'].max():.4f}")
print(f"    Max improvement: {(comparison_df['Test_R2'].max() - baseline_test_r2):.4f} R²")

"""#**SECTION 3 : INSIGHTS & REFLECTIONS**

What did you find in terms of your hypothesis?

Original Hypothesis

My initial hypothesis stated : "Regions with the greatest increase in temperature will be more favourable for crops requiring warmer conditions"

This hypothesis was based on an intutive but overly simplistic understanding of climate-agricultural relationships. I assumed that warmer temperature would universally benefit heat loving crops and that regional warming patterns would strightforwardly predict agricultural outomes.

**What I actually Found**

My findings revealed that reality is far more complex and context-dependent than this hypothesis suggests. The data tells nuanced story that fundamentally challenges the assumption of simple, universal climate-production relationship.


**Finding 1: Simpson's Paradox Reveals Hidden COmplexity**

The most striking discovery came from my Simpson's Paradox analysis in Project 3. When examining the realtionship between average temperature and agricultural production:

Aggregate Pattern (All Countries Combined):

* Slope: -24,254.9
* Direction: NEGATIVE
* Interpretation: Pooling all data suggests warmer temperature associate with lower production

Individual Country Patterns:

* 67% of country showed the OPPOSITE trend
* Individual country slopes ranged from -136,426 to +68,354
* The paradox magnitude reached up to 204,780 units difference

What This Means: The aggregate negative relationship completely masks the fact that most individual countries show positive relationships. This occurs because countries with naturally warmer climate have lower baseline productivity due to infrastructure and economic factors, not because heat is inherently bad. Countries with cooler climate show production increasing with their modest warming trends. The aggregate mixes these incomparable context, producing misleading conclusions.


**Finding 2: Regional Heterogeneity Dominates**

The paradox exists in 75% of regions, three out of four regions show the opposite pattern from the aggregate. This definitively proves you cannot draw regional conclusions from aggregate analysis.

**Finding 3:Crop Type Dominates Climate Effects**

My Machine learning feature importance analysis revealed:

1. Item : 30.2% importance
2. Country: 17.5% importance
3. Average temperature: 13.1 % importance
4. Temperature: 11.2% importance


**Key-Insight**: What you grow matters more than what the climate is doing. The fact that crop type dominates by 2.3:1 over temperature shows that different crops have entirely different climate requirements and responses. Adaptations through crop selection is possible and prehaps already happening.

The extra Credit crop specific models reinforced this dramatically. The 91% improvement demonstrates that treating all crops identically scrafices massive predictive accuracy.

**Finding 4: Non-Linear Relationship Dominate**

The importance of temp_squared (11.2%) and temp_precip_interaction (5.1%) reveals that:

**Temperature has optimal ranges:**
* Below optimal: warmer is better
* At optimal: peak production
* Above optimal: warmer is worse

This explains why my hypothesis was wrong. "Warmer is better" only applies below the optimum. For crops/ regions already near or above their thermal optimum, additional warning reduces production.

**Temperature doesn't act alone:**

* High temp + high precip = productive tropical agriculture
* High temp + low precip = drought stress and crop failure
* The combination matters more than either factor individually


**Revised Understanding**

What I now understand: Climate cahge impacts on agriculture are heterogeneous, context-dependent, and mediated by multiple interacting factors. Rather than a simple "warming helps warm-loving crops" story, the reality is:

* Cold regions may beneft from modest warming by extending growing seasons, but extreme heat events and precipitation changes could offset gains

* Temperate regions show mixed effects-some crops benefit from longer seasons, others suffer from heat stress

* Tropical regions may already be near or past thermal optima for many crops. Additional warming likely reduces productivity

* Crop Selection emerges as a critical adaptation mechanism, farmers can shift to better- adapted varieties or different crops entirely

* Context determines outcomes, infrastructure, technology, water availability, soil quality, and economic resources interact with climate to shape production


**The hypothesis should be reframed as:** "Climate change impacts on agricultural production vary dramatically by region, crop type, baseline climate, and adaptation capacity, with outcomes determined more by context-specific responses and crop selection than by temperature change magnitude alone."

**Any previous assumptions that you had to adjust or proved wrong?**

Assumption 1:"Aggregate patterns tell the story"

What I assumed: I could analyse all countries together and draw general conclusions about climate-production relationships.

What I learned: Simpson's Paradox showed this is dangerously misleading. The aggeregate negative slope masked the fact that 67% of individual countries show positive slopes. The aggregate slope was -24,255, but individual countries ranged from -136,426 to +68,354. The aggregate doesn't represent any indiividual country.

Adjustment Made:

* Implemented stratified analysis by country and region
* Used country_encoded as a feature for country-specific effects
* Deveoped crop-specific models for major crops
* Learned to always ask: "Does this pattern hold within subgroups?"

Assumption 2: "Simple correlation analysis is sufficient"- WRONG

What I assumed: If temperature correlates weakly with production, temperature doesn't matter much.

What I learned: Weak aggregate correlations hide strong relationship within subgroups. The correlation between temperature and production overall was nearly zero, yet individual countries showed correlations ranging from -0.30 to +0.45. The weak aggregate resulted from averaging across opposite-signed relationships that cancelled out.

Adjustment Made:

* Moved beyonf correlation to examine subgroup patterns
* Used interaction terms
* Employed Random Forest because it discovers complex interactions that correlation analysis misses.

Assumption3: "More features always improve models" - WRONG

What I assumed: Adding polynomial features would help capture complex relationships.

What I learned: Polynomial features hurt performance by 24%.
Random Forest already discovers interactions through recursive splitting. Explicitly creating polynomial terms created redundancy, multicollinearity and curse of dimensionality without information gain.

Adjustments made:

* Removed polynomial feature expansion
* Kept only theoretically justifies engineered features
* Learned that feature engineering must match the algorithm

Assumption 4: "Climate is the primary driver of production"-INCOMPLETE

What I Assumed: Weather variables would dominate feature importance.

What I learned: Crop type (30.2%) and country (17.5%) together account for 47.7% of importance, while climate variables combined account for only ~30%. What you grow and where you grow it matters more than weaher.

Adjustment Made:
* Recognized that agricultural production is a socio-ecological system, not just climate-driven
* Reframed question from "how does climate affect production?" to "how do climate impact depend on socio-economic context?"


Assumption 5: "A unified model is always better"- WRONG

What I assumed: Training one model on all crops would leverage more data and produce better generalization.

What I learned: Crop-specific models achieved Rsquare= 0.82 vs. unified model's Rsquare = 0.43- a 91% improvement. Different crops have fundamentally different production functions. Forcing them into one model averages across incompactible patterns.

Adjustment made:

* Recommended hybrid approach: Crop specific models for top 10-20 crops unified Gradient Boosting for rare crops.


Assumption 6: "Tree-  based models prevent overfitting automatically" -INCOMPLETE

What I assumed: Random Forest's bootstrap aggregating would automatically prevent overfitting.

What I learned: While Random Forest generalizes reasonably, Gradient Boosting achieved even better generalization while also improving accuracy. With proper regularization, more sophisticated models can simultaneously improve both accuracy and generalization.

Adjustment Made:

* Recognized that overfitting is about model capacity relative to data complexity
* Learned that complexity isn't inherently bad if properly controlled
* Switched to Gradient Boosting for improved performance



**Is the problem different from what you had originally thought?**

Yes, fundamentally different in four critical ways:

1. It's Context-Dependent Problem, Not a Universal One

Original conception: "How does temperature affect agricultural production?" implies a single, generalizable answer.

Actual problem: "How do temperature effects vary by crop, country, region, baseline climate, and agricultural infrastructure?" requires crop-specific, region-specific, context-specific answers. There is no single coefficient, there are thousands, one for each context.

The problem isn't finding one relationship between climate and produciton. It's understanding many different relationships and the factors that determine which relationship applies in each context.

2. It's a Socio-Ecological Problem, not just a climate problem

Original Conception: Viewed this primarily as climate science

Actual Problem: Agricultural production is determined by multiple factors. Climate matters, but it's embedded in a complex socio-ecological system. Two countries experienced +2 C warming can have opposite outcomes depending on adaptation capacity.


3. It's a heterogeneous Treatment Effects Problem

Original Conception: "Does Warimng Increase Production" as if there's a single causal effect.

Actual Problem: Warming has different causal effects depending on starting temperature, crop type, water availability, and adaptation capacity. This iis heterogeneous treatment effects from casual inference. The average teatment effect doesn't represent individual treatment effects.


Simpson's Paradox is evidence of this heterogeneity. 67% of countries have positive effects while the aggreagte shows negative.


4. It required extrapolation, Not just interpolation

Original conception: Train model on historical data, make predictions within that range.

Actual Problem: Need to predict production under future climate conditions that haven't occureed yet. This extrapolation beyond training data, which machine learning models handle poorly. Historical relationships may not hold in novel climates.


**Anything you would do differently if you were to do it again?**

Major Stratergic Change

1. Start with crop-specific Analysis from Day 1

Begin with exploratory analysis stratified by crop type. Examine top 10 crops individually from Project 2 onward. The 91% improvement from crop-specific models suggests I wasted effort optimizing a unified approach that was fundamentally wrong.


2. Use Time-Based Train-Test Split

What i did: 80/20 split stratified by country

What I'd Do: Train on 1961-2010, test on 2011-2023.

Why: Random splits make the problem atrifically easy. Time-based splits test whether the model can actually forecast, which is the real use case, Would also reveal whether relationships are stable over time or changing due to adaptation.

3. Implement Comprehensive Uncertainity Quantification.

What I did: Point predictions only (single Rsquare values)

What I'd Do: Implement prediction intervals showing uncertainity ranges using quantile regression or bootstrap confidence interval.

4. Implement Comprehensive Uncertainity Quantification

5. Develop Interpretable Model Alongside Black Box

6. Implement Cross-Model Validation

7. Conduct Systematic Scenario Analysis

8. Use temporal Cross-validation




**Policy And Decision Implication**

1. Climate Adaptation Investment
Crop-specific model performance identifies regions and crops most vulnearble to climate variability.

2. Crop Diversication Strategies

Different crops respond differently to temperature and precipitation. Crop-specific results guide shifts toward more climate- resilient crops.
Impact: Reduced production risk, smoother long-term transition, and protected farmer livelihoods.

3. International Trade Planning

Identifying complementary climate risk profiles helps design trade agreements that buffer regional production shocks.
 Impact: More stable global food markets and reduced price volatality.


 4. Agricultual R&B Prioritization

 High climate sensitivity highlight crops and regions needing targeted research.
 Impact: Faster development of climate-adapted variety and better returns on R&D investment.

 Indirect Impacts

 * Food price forecasting and reserve planning
 * Climate- informed mitigation policy
 * Insurance and financial products



**Ethical Considerations**

1. Data Sovereignty & Power Imblances

This study relies on FAO data from developing countries, often analyzed by researchers in developed regions. This raises concers about extractive knowledge production, limited local participation, and unequal benefit distribution. Ethical practice requires local collaboration, accessible finding, and community validation.

2. Equity & Climate Justice

Climate chage may disproportionately harm poorer tropical countries while benefiting wealthier temperate ones, despite unequal historical responsibility for emissions. Ethical analysis should empasize equity, prioritize vulnerable population, and inform climate finance and adaptaion support.

3. Self-Fulfilling Predictions

Labeling regions as "climate-vulnerable" risk triggering disinvestment, abandonment, or fatalism. Predictions should be framed conditionally, emphasizing adaptation potential rather than inevitability.

4. False Precision & Overconfidence

Model output appear precise but explains only 43-62% of variance. Policymakers may over-trust point estimates. Responsible use requires uncertainity communication, prediction intervals, and adaptive decision- making.

5. Algorithmic Bias

Historical data embeds existing inequalities. Underrepresented crops and countries may recieve poorer predictions, reinforcing underinvestment. Mitigation includes subgroup performance checks and fair representation across farming systems.


6. Environmental Ethics

Maximizing production may conflict with sustainability, biodiversity, and climate goals. A more ethical farming prioritizes resilient nutrition and environmental stewardship over yield over.


7. Consent & Secondary Data use
Although FAO data is public and aggregated, it was not collected for prediction ML use. Secodary use is defensible for public benefit but still requires ethical caution.



#**Final Thoughts : Summarize your experience across 5 projects. What did you learn ?**

The Journey : From  Hypothesis to Humility

When I began this proejct, I had a straightforward question: " How will climate change affect agriculture?" I expected to find a clear answer, perhaps a simple equation relating temperature to production. Instead, I discovered that nearly every assumption I made was wrong, and the real world is far more interesting than my initial hypothesis.

Technical Lessons: The craft of Data Science

1. Exploratory Analysis is Discovery, Not Just Description

I initially viewed EDA as a checkbox: make histogram, check missing value, move on. But the Simpson's Paradox analysis became the most important finding of the entire project. It fundamentally changed my understanding.

Lesson: Spend more time exploring. The most valuable insight come from asking "what's happening in the data?"


2. Context Matterns More Than Algorithms

I spent considerable effort optimizing algorithms: Random Forest, Gradient Boosting, hyperparameter tuning. These improved performance modestly (Rsquare 0.43 -> 0.62). But recognizing that different crops need different models, a problem framing insigh, produced the largest gain (Rsquare 0.43-> 0.82)

Lesson: Better problem framing beats better algorithms. Understand the structure of the problem before optimizing technical details.

3. Features Engineering Requires Domain Knowledge

My polynomial feature expansion failed because I applied a generic technique without thinking about whether it made sense for tree-based models. Meanwhile, simple domain-informed features succeeded.

Lesson: Effective features engineering comes from understanding the phenomenon, not just the math. Talk to domain experts. Read the literature. Think about the mechanisms driving outcomes.

4. Model Performance Has Multiple Dimensions

I initially obsesses over Rsquare as "the" performace metric. But I learned that generalization (train-test gap), stability (cross-validation std), interpretability and computational efficiency all matter.


Lessons: "Best Model" depends on the use case. For policy recommendations, interpretability, might outwigh 5 % accuracy.

5. Failure Is informative

The polynomial feature failure (Rsquare dropped 24%). But this failure taught me that Random Forest already captures interactions. Extra Trees failed (Rsquare dropped 26%), but this vlaidated that agricultural patterns are structured, not random.


Lesson: Document failure. Failure experiments that teach you something about the problem are more valuable than successful experiments that just confirm what you already know.


6. Heterogeneity Is The Rule, Not the Exception

Agricultural systems are heterogeneous across crops, regions, countries and time.

Lessons: Beware of generalizations. "Climate change impacts agriculture" is useless without specifying which crops, where, under what conditions.

7. Simpson's Paradox Is Everywhere

Aggregate analysis is almost always misleading when subgroups have different characteristics. This isn't just a statistical curiosity.

Lesson: Always disaggregate. Always check whether patterns hols within subgroups. Never trust aggregate correlations withut examining components.


**Personal Growth: How This Changed Me as a Researcher**

8. Embrace Uncertainity and Complexity

I wanted simple answers. "Does warming help or hurt?" But the worls doesn't offer simple asnwers. It offers conditional relationships, heterogeneous effects, and context- dependence. Learning to embrace this complexity rather than fight it was transformative.


9. Interdisciplinary Thinking is Essential

This project required climate sciecne, agronomy, economics, statistics, machine learning and policy analysis. No single discipline was sufficient. The most valuable insights came at disciplinary intersections.

10. Research Has Real-World Consequences

My analysis could inform policies affecting millions of farmers and billions of consumers. This responsibility weighs heavily. It demands rigor, honesty about uncertainity, and consideration of ethical implications.


**The Bigger Picture: Why This Matters**

This wasn't just an academic exercise. Climate change and food security are among the most pressing challenges humanity faces. This analysis, despite its limitations, contributes a small piece to understanding these challenges:

* It demonstrates that climate impacts are heterogeneous and context dependent
* It shows that adaptation through crop selection is possible and necessary
* It identifies vulnerable systems that need policy support
* It provides quantitative evidence for climate






"""